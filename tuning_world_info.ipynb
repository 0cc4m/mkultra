{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# World Info Tuning\n",
    "mkultra provides a simplified trainer for generating soft World Info*.\n",
    "\n",
    "This trainer takes datapoints in a \"call and response\" format. The \"call\" is a fixed prompt (e.g. \"A detailed description of the Spider Tank:\") that should elicit the \"response\" (a few paragraphs of information about the Spider Tank). Loss is only calculated against the responses.\n",
    "\n",
    "The trainer will optimize a soft prompt such that the call is followed by the response. The combined call and response is moved randomly within the context window to prevent overfitting, and the gap is filled with random tokens.\n",
    "\n",
    "This setup may work for other tasks, but if your needs are significantly different then it is recommended to see tuning_finetune.ipynb for tips on rolling your own.\n",
    "\n",
    "*Short prompt infixes for AI text adventures and writing.\n",
    "Describes a character, subject, artistic direction, etc. with the intent of steering the output to consistently make use of the details in question."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+https://github.com/corolla-johnson/mkultra.git#egg=mkultra --log PIP_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "from mkultra.trainers import WorldInfoTrainer\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the length of the soft prompt.\n",
    "# The paper does not recomment going over 100 tokens.\n",
    "model.initialize_soft_prompt(n_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SoftPrompt.from_file(\"sample_sps/world_info/emma_gpt2.json\")\n",
    "model.set_soft_prompt(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'call': 'Detailed description of Emma Violence:\\n', 'response': 'Emma Violence is a British cybernetic assassin who is known for her elegant style and high-profile targets. She is described to be a perfectionist in her work, often going above and beyond the call of duty. She has two guns implanted to her forearms and can utilize them both with deadly accuracy. Despite her cold, ruthless, and calculating nature, she has a warm and motherly side that she only shows to a select few people.'}, {'call': 'Detailed description of Emma Violence:\\n', 'response': 'Emma Violence is a British cybernetic assassin who is known for her elegant style and high-profile targets. She is described to be a perfectionist in her work, often going above and beyond the call of duty. She has two guns implanted to her forearms and can utilize them both with deadly accuracy. Despite her cold, ruthless, and calculating nature, she has a warm and motherly side that she only shows to a select few people.'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/emma.json\") as file:\n",
    "    blocks = json.load(file)\n",
    "\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldInfoTrainer will help prevent overfitting by moving the target\n",
    "# up and down the context, filling the rest with random tokens.\n",
    "# Also, only the 'response' string is used to calculate loss.\n",
    "trainer = WorldInfoTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=Adafactor(params=[model.get_soft_params()]),\n",
    "    blocks=blocks,\n",
    "    max_spacing=100,\n",
    "    min_loss=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0/2: Loss: 0.7642713189125061\n",
      "1/2: Loss: 0.5425863265991211\n",
      "C:\\Users\\STARSTRUCK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:562: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A detailed dscription of Emma Violence:\nEmma Violence is a British cybernetic assassin who is known for her elegant style and high-profile targets. She is described to be a perfectionist in her work, often going above and beyond the call of duty. Despite her cold, ruthless, and calculating nature, she has a warm and motherly side that she often goes above and beyond the call of duty. Despite her cold, ruthless, and calculating nature, she has a warm and motherly side that she often goes above and beyond the\n"
     ]
    }
   ],
   "source": [
    "call = tokenizer(\"A detailed dscription of Emma Violence:\\n\", return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=False,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = { \"name\" : \"Emma Violence\",\n",
    "             \"description\" : \"A British cybernetic assassin\" }\n",
    "\n",
    "torch.save(model.get_soft_params(), \"emma.pt\")\n",
    "\n",
    "sp = SoftPrompt.from_tuning_model(model, metadata)\n",
    "sp.to_file(\"sample_sps/world_info/emma_gpt2.json\")"
   ]
  },
  {
   "source": [
    "## Best Practices for World Info\n",
    "This is fresh territory and the example here is likely far from optimal.\n",
    "Here are potential techniques to explore:\n",
    "- Define bad_words_ids (e.g. square brackets) and surround the response in them to keep the output from repeating the training data verbatim.\n",
    "- Rather than describing the subject in grammatically correct sentences, try a word cloud approach.\n",
    "- Rather than describing the subject, provide implicit examples of to write about it (\"John doffed his black top hat and cleaned it with his white handkerchief\")\n",
    "- Consider the use of the subject's proper noun vs its pronouns. (Should you start every sentence with \"John Doe\", or a mix of \"John\" and \"He\"?)\n",
    "- Write your own trainer?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}