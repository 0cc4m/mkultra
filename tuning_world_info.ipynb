{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# World Info Tuning\n",
    "mkultra provides a specialized trainer for generating soft World Info*.\n",
    "\n",
    "This trainer takes datapoints in a \"call and response\" format. The \"call\" is a fixed prompt (e.g. \"A detailed description of the Spider Tank:\") that should elicit the \"response\" (a few paragraphs of information about the Spider Tank). Multiple responses can be specified for each call, but loss is only calculated against the responses.\n",
    "\n",
    "The trainer will optimize a soft prompt such that the call is followed by the response. The combined call and response is moved randomly within the context window to prevent overfitting, and the gap is filled with random tokens.\n",
    "\n",
    "This setup may work for other tasks, but if your needs are significantly different then it is recommended to see tuning_finetune.ipynb for tips on rolling your own.\n",
    "\n",
    "*Short prompt infixes for AI text adventures and writing.\n",
    "Describes a character, subject, artistic direction, etc. with the intent of steering the output to consistently make use of the details in question."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 97])\ntorch.Size([1, 97])\n"
     ]
    }
   ],
   "source": [
    "# Set the length of the soft prompt.\n",
    "# The paper does not recomment going over 100 tokens.\n",
    "model.initialize_soft_prompt(n_tokens=20)\n",
    "\n",
    "call_str = \"Detailed description of Emma Violence:\\n\"\n",
    "\n",
    "response_str = (\"Emma Violence is a British cybernetic assassin who is known for her \"\n",
    "                \"elegant style and high-profile targets. She is described to be a perfectionist \"\n",
    "                \"in her work, often going above and beyond the call of duty. \"\n",
    "                \"Despite her cold, ruthless, and calculating nature, she has a warm and \"\n",
    "                \"motherly side that she only shows to a select few people. She has two guns \"\n",
    "                \"implanted to her forearms and can utilize them both with deadly accuracy.\")\n",
    "\n",
    "call = tokenizer(call_str, return_tensors=\"pt\").input_ids.cuda()\n",
    "response = tokenizer(response_str, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "input_ids = torch.cat( [call, response], dim=1 )\n",
    "\n",
    "ignore_call = torch.full((1,call.shape[-1]),-100).cuda()\n",
    "\n",
    "labels = torch.cat( [ ignore_call, response ], dim=1 )\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Multiple answers can be written for a single question.\n",
    "# answers = [[]]\n",
    "\n",
    "# The components are assembled like this:\n",
    "# |soft prompt|random tokens|question|answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = Adafactor(params=model.get_soft_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: Loss: 0.6359873414039612\n",
      "5: Loss: 0.6187944412231445\n",
      "10: Loss: 0.7337299585342407\n",
      "15: Loss: 0.5554460287094116\n",
      "20: Loss: 0.7459617853164673\n",
      "25: Loss: 0.6622592210769653\n",
      "30: Loss: 0.6346929669380188\n",
      "35: Loss: 0.5845086574554443\n",
      "40: Loss: 0.4881477355957031\n",
      "45: Loss: 0.736186146736145\n",
      "50: Loss: 0.6162692904472351\n",
      "55: Loss: 0.5107298493385315\n",
      "60: Loss: 0.5365223288536072\n",
      "65: Loss: 0.6170713901519775\n",
      "70: Loss: 0.679900050163269\n",
      "75: Loss: 0.5693861246109009\n",
      "80: Loss: 0.49865373969078064\n",
      "85: Loss: 0.3966021239757538\n",
      "90: Loss: 0.5641685724258423\n",
      "95: Loss: 0.544960081577301\n",
      "100: Loss: 1.4968494176864624\n",
      "105: Loss: 0.607020914554596\n",
      "110: Loss: 0.4740757942199707\n",
      "115: Loss: 0.5228590965270996\n",
      "120: Loss: 0.5585851669311523\n",
      "125: Loss: 0.604408323764801\n",
      "130: Loss: 0.4761580526828766\n",
      "135: Loss: 0.6136701703071594\n",
      "140: Loss: 0.4362785220146179\n",
      "145: Loss: 0.5728248357772827\n",
      "150: Loss: 0.4585609436035156\n",
      "155: Loss: 0.5957559943199158\n",
      "160: Loss: 0.4781897962093353\n",
      "165: Loss: 0.6148995161056519\n",
      "170: Loss: 0.4572176933288574\n",
      "175: Loss: 0.5807996392250061\n",
      "180: Loss: 0.7966114282608032\n",
      "185: Loss: 0.46111953258514404\n",
      "190: Loss: 0.5630686283111572\n",
      "195: Loss: 0.5084531307220459\n",
      "200: Loss: 0.6298947930335999\n",
      "205: Loss: 0.6209437251091003\n",
      "210: Loss: 0.5219013690948486\n",
      "215: Loss: 0.57744961977005\n",
      "220: Loss: 0.6602721214294434\n",
      "225: Loss: 0.5841668844223022\n",
      "230: Loss: 0.4186428487300873\n",
      "235: Loss: 0.4104469418525696\n",
      "240: Loss: 0.6393155455589294\n",
      "245: Loss: 0.5648429989814758\n",
      "250: Loss: 0.44054901599884033\n",
      "255: Loss: 0.5906956791877747\n",
      "260: Loss: 0.41373616456985474\n",
      "265: Loss: 0.6945628523826599\n",
      "270: Loss: 0.4986288249492645\n",
      "275: Loss: 0.6207769513130188\n",
      "280: Loss: 0.9183221459388733\n",
      "285: Loss: 0.7401600480079651\n",
      "290: Loss: 0.536108672618866\n",
      "295: Loss: 0.4291750192642212\n",
      "300: Loss: 0.5710659623146057\n",
      "305: Loss: 0.6139786839485168\n",
      "310: Loss: 0.570438027381897\n",
      "315: Loss: 0.5891115069389343\n",
      "320: Loss: 0.6594356298446655\n",
      "325: Loss: 0.4049615263938904\n",
      "330: Loss: 0.5701367259025574\n",
      "335: Loss: 0.5366660356521606\n",
      "340: Loss: 0.5274413824081421\n",
      "345: Loss: 0.3586221933364868\n",
      "350: Loss: 0.5035576224327087\n",
      "355: Loss: 0.5400426387786865\n",
      "360: Loss: 0.43280962109565735\n",
      "365: Loss: 0.481569766998291\n",
      "370: Loss: 0.4420213997364044\n",
      "375: Loss: 0.31864216923713684\n",
      "380: Loss: 0.4857698678970337\n",
      "385: Loss: 0.4317479729652405\n",
      "390: Loss: 0.5685060024261475\n",
      "395: Loss: 0.5886033177375793\n",
      "400: Loss: 0.6767187714576721\n",
      "405: Loss: 0.44310808181762695\n",
      "410: Loss: 0.40928027033805847\n",
      "415: Loss: 0.6028981804847717\n",
      "420: Loss: 0.3691615164279938\n",
      "425: Loss: 0.5478492379188538\n",
      "430: Loss: 0.5321123600006104\n",
      "435: Loss: 0.6498546004295349\n",
      "440: Loss: 0.5074960589408875\n",
      "445: Loss: 0.5323868989944458\n",
      "450: Loss: 0.4142388105392456\n",
      "455: Loss: 0.5532550811767578\n",
      "460: Loss: 0.5179426670074463\n",
      "465: Loss: 0.5817359685897827\n",
      "470: Loss: 0.3955170810222626\n",
      "475: Loss: 0.5458870530128479\n",
      "480: Loss: 0.4868210256099701\n",
      "485: Loss: 0.4711190462112427\n",
      "490: Loss: 0.47418326139450073\n",
      "495: Loss: 0.5227727890014648\n",
      "500: Loss: 0.5169346332550049\n",
      "505: Loss: 0.5187628269195557\n",
      "510: Loss: 0.5036608576774597\n",
      "515: Loss: 0.5663185119628906\n",
      "520: Loss: 0.590307354927063\n",
      "525: Loss: 0.4971991777420044\n",
      "530: Loss: 0.4285773038864136\n",
      "535: Loss: 0.41617342829704285\n",
      "540: Loss: 0.5745148658752441\n",
      "545: Loss: 0.5168178677558899\n",
      "550: Loss: 0.8464939594268799\n",
      "555: Loss: 0.5979862213134766\n",
      "560: Loss: 0.6440207958221436\n",
      "565: Loss: 0.3807031810283661\n",
      "570: Loss: 0.3597535490989685\n",
      "575: Loss: 0.3320988416671753\n",
      "580: Loss: 0.47183388471603394\n",
      "585: Loss: 0.618763267993927\n",
      "590: Loss: 0.4128320813179016\n",
      "595: Loss: 0.3665243685245514\n",
      "600: Loss: 0.38022470474243164\n",
      "605: Loss: 0.4878231883049011\n",
      "610: Loss: 0.4120370149612427\n",
      "615: Loss: 0.41100451350212097\n",
      "620: Loss: 0.463986337184906\n",
      "625: Loss: 0.47740665078163147\n",
      "630: Loss: 0.4992114305496216\n",
      "635: Loss: 0.47858548164367676\n",
      "640: Loss: 0.35026663541793823\n",
      "645: Loss: 0.5154545307159424\n",
      "650: Loss: 0.5850874781608582\n",
      "655: Loss: 0.8014999628067017\n",
      "660: Loss: 0.5165509581565857\n",
      "665: Loss: 0.5068142414093018\n",
      "670: Loss: 0.5934543013572693\n",
      "675: Loss: 0.38288649916648865\n",
      "680: Loss: 0.5894755721092224\n",
      "685: Loss: 0.5478021502494812\n",
      "690: Loss: 0.3165791928768158\n",
      "695: Loss: 0.7783167362213135\n",
      "700: Loss: 0.45099711418151855\n",
      "705: Loss: 0.43163609504699707\n",
      "710: Loss: 0.4545482099056244\n",
      "715: Loss: 0.49958673119544983\n",
      "720: Loss: 0.6395845413208008\n",
      "725: Loss: 0.44719386100769043\n",
      "730: Loss: 0.3494466245174408\n",
      "735: Loss: 0.517274796962738\n",
      "740: Loss: 0.5740607976913452\n",
      "745: Loss: 0.36059609055519104\n",
      "750: Loss: 0.5274238586425781\n",
      "755: Loss: 0.5411306023597717\n",
      "760: Loss: 0.3731454610824585\n",
      "765: Loss: 0.40917035937309265\n",
      "770: Loss: 0.5832014083862305\n",
      "775: Loss: 0.5182331800460815\n",
      "780: Loss: 0.43902158737182617\n",
      "785: Loss: 0.6337186098098755\n",
      "790: Loss: 0.40848976373672485\n",
      "795: Loss: 0.34102240204811096\n",
      "800: Loss: 0.5566389560699463\n",
      "805: Loss: 0.3720838725566864\n",
      "810: Loss: 0.6162918210029602\n",
      "815: Loss: 0.41441619396209717\n",
      "820: Loss: 0.34798485040664673\n",
      "825: Loss: 0.37921804189682007\n",
      "830: Loss: 0.4923200011253357\n",
      "835: Loss: 0.3501097857952118\n",
      "840: Loss: 0.4470275342464447\n",
      "845: Loss: 0.38303864002227783\n",
      "850: Loss: 0.5019475817680359\n",
      "855: Loss: 0.3905975818634033\n",
      "860: Loss: 0.3939957022666931\n",
      "865: Loss: 0.42774853110313416\n",
      "870: Loss: 0.5167403817176819\n",
      "875: Loss: 0.4996834099292755\n",
      "880: Loss: 0.5184770226478577\n",
      "885: Loss: 0.38611793518066406\n",
      "890: Loss: 0.39934849739074707\n",
      "895: Loss: 0.4664268493652344\n",
      "900: Loss: 0.3552396595478058\n",
      "905: Loss: 0.5256217122077942\n",
      "910: Loss: 0.6555043458938599\n",
      "915: Loss: 0.464555561542511\n",
      "920: Loss: 0.45502743124961853\n",
      "925: Loss: 0.41662338376045227\n",
      "930: Loss: 0.36562666296958923\n",
      "935: Loss: 0.3453464210033417\n",
      "940: Loss: 0.4107022285461426\n",
      "945: Loss: 0.41913849115371704\n",
      "950: Loss: 0.45483505725860596\n",
      "955: Loss: 0.6164480447769165\n",
      "960: Loss: 0.40041854977607727\n",
      "965: Loss: 0.4649423360824585\n",
      "970: Loss: 0.49673935770988464\n",
      "975: Loss: 0.4928792417049408\n",
      "980: Loss: 0.4949074983596802\n",
      "985: Loss: 0.56182861328125\n",
      "990: Loss: 0.47914934158325195\n",
      "995: Loss: 0.4184281527996063\n"
     ]
    }
   ],
   "source": [
    "#@title Training\n",
    "#@markdown 4000+ for \"gpt2\"\n",
    "#@markdown\n",
    "#@markdown 200+ for \"GPT-Neo-2.7B\"\n",
    "\n",
    "iterations = 1000#@param{type:\"number\"}\n",
    "\n",
    "for i in range(iterations):\n",
    "  optimizer.zero_grad()\n",
    "  output = model(input_ids=input_ids, labels=labels)\n",
    "  loss = output.loss\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if i%5 == 0:\n",
    "    print(f\"{i}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Detailed description of Emma Violence:\nEmma Violence is a British cybernetic assassin who is known for her elegant and high-profile targets. She is portrayed to be a perfectionist in her work, often taking down targets in a manner that is almost certain to mock execution. Despite her cold temperate personality, she is a dedicated a strict and dedicated killer. She has two guns implanted to her forearms and moles, and nce, and is able to utilize them both with deadly precision and accuracy. She is known for her\n"
     ]
    }
   ],
   "source": [
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    temperature=0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntrainer = WorldInfoTrainer(\\n    model=model\\n    optimizer=Adafactor(params=model.get_soft_params())\\n    blocks = [block],\\n    max_spacing=0, # Setting this to 0 for illustrative purposes\\n    min_loss=0.4\\n)\\n\\ntrainer.train()\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# WorldInfoTrainer will help prevent overfitting by moving the target\n",
    "# up and down the context, filling the rest with random tokens.\n",
    "# Also, only the 'target' string is used to calculate loss.\n",
    "# Note that very low loss may not be acceptable for the description task.\n",
    "\"\"\"\n",
    "trainer = WorldInfoTrainer(\n",
    "    model=model\n",
    "    optimizer=Adafactor(params=model.get_soft_params())\n",
    "    blocks = [block],\n",
    "    max_spacing=0, # Setting this to 0 for illustrative purposes\n",
    "    min_loss=0.4\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "## Best Practices for World Info\n",
    "Unlike the finetuning example, this is fresh territory.\n",
    "Here are potential techniques to explore:\n",
    "- Define bad_words_ids (e.g. square brackets) and surround the response in them to keep the output from repeating the training data verbatim.\n",
    "- Rather than describing the subject in grammatically correct sentences, try a word cloud approach.\n",
    "- Rather than describing the subject, provide implicit examples of to write about it (\"John doffed his black top hat and cleaned it with his white handkerchief\")\n",
    "- Consider the use of the subject's proper noun vs its pronouns. (Should you start every sentence with \"John Doe\", or a mix of \"John\" and \"He\"?)\n",
    "- Definitely use the min_loss parameter to arrest the tuning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}