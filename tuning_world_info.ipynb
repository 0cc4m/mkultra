{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# World Info Tuning\n",
    "mkultra provides a specialized trainer for generating soft World Info*.\n",
    "\n",
    "This trainer takes datapoints in a \"call and response\" format. The \"call\" is a fixed prompt (e.g. \"A detailed description of the Spider Tank:\") that should elicit the \"response\" (a few paragraphs of information about the Spider Tank). Multiple responses can be specified for each call, but loss is only calculated against the responses.\n",
    "\n",
    "The trainer will optimize a soft prompt such that the call is followed by the response. The combined call and response is moved randomly within the context window to prevent overfitting, and the gap is filled with random tokens.\n",
    "\n",
    "This setup may work for other tasks, but if your needs are significantly different then it is recommended to see tuning_finetune.ipynb for tips on rolling your own.\n",
    "\n",
    "*Short prompt infixes for AI text adventures and writing.\n",
    "Describes a character, subject, artistic direction, etc. with the intent of steering the output to consistently make use of the details in question."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import torch\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 97])\ntorch.Size([1, 97])\n"
     ]
    }
   ],
   "source": [
    "# Set the length of the soft prompt.\n",
    "# The paper does not recomment going over 100 tokens.\n",
    "model.initialize_soft_prompt(n_tokens=20)\n",
    "\n",
    "call_str = \"Detailed description of Emma Violence:\\n\"\n",
    "\n",
    "response_str = (\"Emma Violence is a British cybernetic assassin who is known for her \"\n",
    "                \"elegant style and high-profile targets. She is described to be a perfectionist \"\n",
    "                \"in her work, often going above and beyond the call of duty. \"\n",
    "                \"Despite her cold, ruthless, and calculating nature, she has a warm and \"\n",
    "                \"motherly side that she only shows to a select few people. She has two guns \"\n",
    "                \"implanted to her forearms and can utilize them both with deadly accuracy.\")\n",
    "\n",
    "call = tokenizer(call_str, return_tensors=\"pt\").input_ids.cuda()\n",
    "response = tokenizer(response_str, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "input_ids = torch.cat( [call, response], dim=1 )\n",
    "\n",
    "ignore_call = torch.full((1,call.shape[-1]),-100).cuda()\n",
    "\n",
    "labels = torch.cat( [ ignore_call, response ], dim=1 )\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Multiple answers can be written for a single question.\n",
    "# answers = [[]]\n",
    "\n",
    "# The components are assembled like this:\n",
    "# |soft prompt|random tokens|question|answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeds = torch.load(\"sample_sps/world_info/emma_gpt2.pt\")[0]\n",
    "#print(embeds)\n",
    "#model.set_soft_prompt_embeds(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(params=[model.get_soft_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 768])\ntorch.Size([1, 97, 768])\ntorch.Size([1, 99, 768])\ntorch.Size([1, 97])\ntorch.Size([1, 99])\n0: Loss: 3.186060667037964\n"
     ]
    }
   ],
   "source": [
    "#@title Training\n",
    "#@markdown 4000+ for \"gpt2\"\n",
    "#@markdown\n",
    "#@markdown 200+ for \"GPT-Neo-2.7B\"\n",
    "model.train()\n",
    "iterations = 1#@param{type:\"number\"}\n",
    "\n",
    "for i in range(iterations):\n",
    "  optimizer.zero_grad()\n",
    "  output = model(input_ids=input_ids, labels=labels)\n",
    "  loss = output.loss\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if i%5 == 0:\n",
    "    print(f\"{i}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    temperature=0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldInfoTrainer will help prevent overfitting by moving the target\n",
    "# up and down the context, filling the rest with random tokens.\n",
    "# Also, only the 'target' string is used to calculate loss.\n",
    "# Note that very low loss may not be acceptable for the description task.\n",
    "\"\"\"\n",
    "trainer = WorldInfoTrainer(\n",
    "    model=model\n",
    "    optimizer=Adafactor(params=model.get_soft_params())\n",
    "    blocks = [block],\n",
    "    max_spacing=0, # Setting this to 0 for illustrative purposes\n",
    "    min_loss=0.4\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "## Best Practices for World Info\n",
    "Unlike the finetuning example, this is fresh territory.\n",
    "Here are potential techniques to explore:\n",
    "- Define bad_words_ids (e.g. square brackets) and surround the response in them to keep the output from repeating the training data verbatim.\n",
    "- Rather than describing the subject in grammatically correct sentences, try a word cloud approach.\n",
    "- Rather than describing the subject, provide implicit examples of to write about it (\"John doffed his black top hat and cleaned it with his white handkerchief\")\n",
    "- Consider the use of the subject's proper noun vs its pronouns. (Should you start every sentence with \"John Doe\", or a mix of \"John\" and \"He\"?)\n",
    "- Definitely use the min_loss parameter to arrest the tuning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}