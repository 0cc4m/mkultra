{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Generation\n",
    "mkultra is set up for convenient text generation with transformers generation and sampling tools.\n",
    "\n",
    "You can load a SoftPrompt object and simply concatenate it into your context string, where it shows up as a human-readable tag that tokenizes to the underlying number of tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+https://github.com/corolla-johnson/mkultra.git#egg=mkultra --log PIP_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.inference import GPT2SoftPromptLM\n",
    "from mkultra.tokenizers import GPT2SPTokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to instantiate mkultra's model and tokenizer classes.\n",
    "model = GPT2SoftPromptLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2SPTokenizerFast.from_pretrained(\"gpt2\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftPrompts may be loaded in several ways.\n",
    "# sp = SoftPrompt.from_json(json_str)\n",
    "# sp = SoftPrompt.from_inputs_embeds(inputs_embeds)\n",
    "# sp = SoftPrompt.from_tuning_model(model)\n",
    "# sp = SoftPrompt.from_string(\"With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.\",\n",
    "#                             model=model, tokenizer=tokenizer)\n",
    "\n",
    "sp = SoftPrompt.from_file(\"sample_sps/finetune/neuromancer_gpt2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "neuromancer-gpt2 (2021-06-10 02:46:17.708191)\nLength: 100\nUUID:   299ce582-5a2e-427d-a8e8-8c17aef3b458\nDescription:\nTrained for 30 epochs on the full text of William Gibson's 'Neuromancer'\n"
     ]
    }
   ],
   "source": [
    "# Information about a SoftPrompt can be printed with\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'get_tag_str()' to get a tag string that you can add to your context.\n",
    "prompt = sp.get_tag_str() + \"Armitage\"\n",
    "\n",
    "# The addition operator also works:\n",
    "# prompt = sp + \"Armitage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<neuromancer-gpt2-299ce582-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>Armitage\n"
     ]
    }
   ],
   "source": [
    "# The tag string conveniently contains the SP's name, part of its GUID,\n",
    "# and a series of '@'s which represent individual soft tokens.\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of soft prompt: 100\nLength of full prompt: 103\n"
     ]
    }
   ],
   "source": [
    "# The tag string tokenizes to the correct length budget your context.\n",
    "prompt_len = len(tokenizer.encode(prompt))\n",
    "print(f\"Length of soft prompt: {len(tokenizer.encode(sp.get_tag_str()))}\")\n",
    "print(f\"Length of full prompt: {prompt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "<neuromancer-gpt2-299ce582-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>Armitage was a sort of black man with long, white hair and the sharpest bangs that made him look like he'd been in jail. The only thing missing from his visor were eyes: His face had faded into grey as if it hadn't grown old enough to be recognizable by its coloration; there's no trace on any other hand.\"\"I've seen you before,\" Case said softly over her shoulder, \"but this is where I saw your first death â€” what do we call my\n"
     ]
    }
   ],
   "source": [
    "# Generation is as usual.\n",
    "output = generator( prompt,\n",
    "                    do_sample=True,\n",
    "                    min_length=prompt_len+100,\n",
    "                    max_length=prompt_len+100,\n",
    "                    repetition_penalty=1.7,\n",
    "                    top_p=0.8,\n",
    "                    temperature=0.7,\n",
    "                    use_cache=True,\n",
    "                    return_full_text=True)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "<neuromancer-gpt2-299ce582-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>Armitage's the best he'd ever seen. \"I don't know how it got here, but I've been doing this for five years.\"\"You mean like a hundred? You just started getting into trouble with them?\" The man was talking about his grandfather in front of him; they were both standing next to each other at one end and their eyes met on Case as if two people had stared down from above: an old woman wearing jeans that she held tightly against her back while leaning forward\n"
     ]
    }
   ],
   "source": [
    "# It is also fine to use generate() instead of a pipeline.\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids,\n",
    "                        do_sample=True,\n",
    "                        min_length=prompt_len+100,\n",
    "                        max_length=prompt_len+100,\n",
    "                        repetition_penalty=1.7,\n",
    "                        top_p=0.8,\n",
    "                        temperature=0.7,\n",
    "                        use_cache=True,\n",
    "                        return_full_text=True)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decide to write your own sampler instead of a pipeline,\n",
    "# be aware that the special token logits will be very high.\n",
    "# This is accounted for when using model.generate() or pipelines,\n",
    "# but you will need to exclude special tokens when sampling.\n",
    "\n",
    "output = model(input_ids)"
   ]
  }
 ]
}