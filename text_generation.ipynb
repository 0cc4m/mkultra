{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Generation\n",
    "mkultra is set up for convenient text generation with transformers generation and sampling tools.\n",
    "\n",
    "You can load a SoftPrompt object and simply concatenate it into your context string, where it shows up as a human-readable tag that tokenizes to the underlying number of tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.inference import GPT2SoftPromptLM\n",
    "from mkultra.tokenizers import GPT2SPTokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to instantiate mkultra's model and tokenizer classes.\n",
    "model = GPT2SoftPromptLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2SPTokenizerFast.from_pretrained(\"gpt2\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftPrompts may be loaded in one of several ways.\n",
    "# sp = SoftPrompt.from_file(\"soft_prompts/neuromancer.pt\")\n",
    "# sp = SoftPrompt.from_inputs_embeds(inputs_embeds)\n",
    "# sp = SoftPrompt.from_tuning_model(model)\n",
    "# We will instantiate an SP from a string for testing. This should behave identically to the text.\n",
    "sp = SoftPrompt.from_string(\"With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.\",\n",
    "                            model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FromString (2021-06-08 20:47:03.855460)\nLength: 22\nUUID:   453df955-c0a4-497c-bc58-3049250ff059\nDescription:\nCreated from string 'With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.'\n"
     ]
    }
   ],
   "source": [
    "# Information about an sp can be printed with\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'get_tag_str()' to get a tag string that you can add to your context.\n",
    "prompt = sp.get_tag_str() + \" She\"\n",
    "\n",
    "# The addition operator also works:\n",
    "# prompt = sp + \" She\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<FromString-453df955-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@> She\n"
     ]
    }
   ],
   "source": [
    "# The tag string conveniently contains the SP's name, part of its GUID,\n",
    "# and a series of '@'s which represent individual soft tokens.\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of soft prompt: 22\nLength of full prompt: 23\n"
     ]
    }
   ],
   "source": [
    "# These tokens help you budget your context.\n",
    "prompt_len = len(tokenizer.encode(prompt))\n",
    "print(f\"Length of soft prompt: {len(tokenizer.encode(sp.get_tag_str()))}\")\n",
    "print(f\"Length of full prompt: {prompt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[{'generated_text': \"<FromString-453df955-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@> She could only rely on the aid of her sister, who was still young and inexperienced.\\nThe court had to decide if she should go with their leader or stay behind in order for him to find a suitable partner within this new world as well? The two women were very close friends; they even dated when Charlotte wanted them together more than once! They couldn't afford such an awkward relationship now that it seemed so much easier since there wasn' no way he'd be able help outâ€¦ yet somehow both\"}]\n"
     ]
    }
   ],
   "source": [
    "# Generation is as usual.\n",
    "output = generator( prompt,\n",
    "                    do_sample=True,\n",
    "                    min_length=prompt_len+100,\n",
    "                    max_length=prompt_len+100,\n",
    "                    repetition_penalty=1.7,\n",
    "                    top_p=0.8,\n",
    "                    temperature=0.7,\n",
    "                    use_cache=True,\n",
    "                    return_full_text=True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also fine to use generate() instead of a pipeline\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decide to write your own sampler instead of a pipeline,\n",
    "# be aware that the special token logits will be very high.\n",
    "# This is accounted for when using model.generate() or pipelines,\n",
    "# but you will need to exclude special tokens when sampling.\n",
    "\n",
    "output = model(input_ids)"
   ]
  }
 ]
}