{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "source": [
    "# Tuning on Datasets\n",
    "This sheet is adapted from the language modeling example at\n",
    "https://github.com/huggingface/transformers/tree/master/examples/pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "n_tokens = 100\n",
    "\n",
    "model.initialize_soft_prompt(n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-812ce7cd67147d37\n",
      "Reusing dataset text (C:\\Users\\STARSTRUCK\\.cache\\huggingface\\datasets\\text\\default-812ce7cd67147d37\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"datasets/neuromancer_reformatted.txt\"\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": path, \"validation\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.44ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 28.57ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1], 'input_ids': [1525, 3977, 20400]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ", 13, 1400, 2128, 475, 262, 38952, 1308, 1806, 286, 262, 698, 3618, 290, 262, 37479, 12704, 286, 262, 8486, 13, 8134, 12609, 7577, 35456, 1973, 30236, 338, 18405, 355, 262, 1450, 48149, 13, 383, 31912, 9474, 547, 3478, 12, 6477, 7842, 6637, 26, 379, 3478, 11, 262, 23110, 484, 2714, 547, 655, 739, 257, 16430, 890, 13, 383, 638, 361, 891, 4799, 338, 11762, 318, 262, 277, 12137, 338, 11762, 11, 8913, 12086, 11, 262, 9353, 41108, 11, 15683, 19874, 351, 11865, 13, 383, 23110, 3947, 284, 1445, 286, 511, 220, 898, 8178, 11, 1278, 2530, 351, 257, 12146, 3092, 286, 25615, 832, 262, 44606, 290, 8318, 286, 511, 9280, 11, 966, 6427, 966, 11, 355, 262, 1450, 13488, 329, 281, 4756, 13, 30236, 338, 18529, 44866, 1986, 373, 7209, 290, 991, 11, 4964, 13, 1, 40, 1183, 467, 1064, 514, 617, 2057, 553, 8913, 531, 13, 1375, 14464, 11, 2626, 287, 50152, 286, 262, 9280, 13, 1544, 1422, 470, 588, 428, 1295, 13, 1544, 2900, 290, 6807, 736, 656, 262, 16187, 13, 14190, 3223, 13, 14190, 5897, 13, 464, 4315, 11, 339, 2497, 11, 373, 4632, 4960, 13, 1892, 1107, 257, 5265, 2254, 4315, 13, 9634, 82, 866, 422, 262, 10389, 5823, 13, 679, 4385, 326, 4001, 262, 13478, 550, 262, 7546, 286, 617, 6355, 18136, 5583, 13, 679, 14028, 11589, 644, 340, 561, 307, 588, 11, 1762, 477, 534, 1204, 329, 530, 1976, 64, 571, 19231, 13, 5834, 5627, 11, 1664, 2537, 10295, 11, 1664, 14825, 13, 1544, 1549, 925, 3016, 257, 1336, 10349, 286, 262, 29500, 878, 339, 1043, 262, 2057, 40308, 13, 679, 5839, 46251, 2072, 72, 319, 43370, 364, 392, 734, 7331, 266, 6969, 6383, 684, 286, 6099, 13, 2671, 5077, 510, 379, 262, 31912, 9474, 11, 339, 2497, 326, 2910, 49699, 530, 3785, 338, 7721, 13, 45816, 7586, 10746, 6908, 992, 866, 262, 43370, 364, 290, 625, 465, 638, 34083, 13, 31334, 1528, 290, 339, 1549, 14509, 287, 13, 1002, 339, 4838, 465, 2951, 783, 11, 339, 1549, 766, 262, 17593, 13, 2484, 9797, 19074, 355, 262, 31912, 9474, 28507, 832, 511, 9280, 13, 6423, 262, 3252, 2540, 284, 29654, 1022, 465, 12450, 13, 317, 4692], 'labels': [1175, 460, 466, 329, 530, 338, 5939, 526, 1, 9690, 11, 21946, 13, 314, 19059, 345, 530, 526, 1, 17309, 306, 11, 8913, 13, 843, 24829, 526, 1870, 1568, 339, 1549, 1560, 2241, 326, 262, 6180, 379, 3409, 11632, 338, 550, 2936, 2642, 422, 262, 923, 11, 326, 772, 355, 339, 1549, 3940, 30236, 1863, 326, 20749, 11, 32299, 1359, 832, 257, 29957, 10137, 35971, 354, 286, 7846, 17071, 82, 290, 8944, 305, 6513, 321, 14180, 11, 339, 1549, 39243, 340, 13, 21776, 338, 1918, 11, 4953, 2644, 2990, 1549, 3750, 284, 262, 17871, 3820, 11, 706, 339, 1549, 1775, 1024, 1531, 11, 290, 3432, 572, 465, 5057, 284, 45874, 351, 257, 4836, 286, 943, 2781, 496, 338, 968, 47028, 13, 45874, 550, 8288, 326, 11, 465, 6510, 550, 8288, 340, 1342, 11, 290, 30236, 550, 36268, 379, 8913, 338, 1735, 351, 257, 1611, 286, 45782, 43057, 12245, 11, 6189, 40314, 329, 530, 286, 606, 284, 787, 257, 1445, 13, 3244, 339, 1549, 2077, 607, 736, 284, 262, 24101, 329, 257, 4144, 13, 1, 54, 9222, 534, 640, 11, 42966, 553, 30236, 531, 11, 618, 8913, 1718, 281, 19318, 1840, 422, 262, 10000, 286, 465, 15224, 13, 1, 2437, 338, 326, 30, 921, 765, 530, 1701, 679, 2714, 262, 9582, 503, 284, 607, 13, 1, 7120, 649, 22154, 260, 292, 11, 8913, 11, 290, 883, 37008, 287, 534, 14383, 13, 943, 2781, 496, 550, 606, 3562, 284, 17286, 326, 7510, 526, 1375, 22499, 262, 19318, 1840, 351, 530, 15601, 45459, 17864, 13, 366, 1639, 821, 3182, 18958, 1146, 23402, 286, 1972, 572, 319, 20766, 25385, 393, 15144, 526, 1, 2484, 270, 553, 339, 531, 13, 679, 3114, 379, 262, 19318, 1840, 11, 788, 379, 607, 13, 1, 47659, 340, 13, 27574, 257, 8667, 13, 10528, 1183, 1645, 526, 1544, 750, 13, 10528, 750, 13, 12510, 16800, 1568, 11, 673, 373, 4737, 11738, 89, 546, 262, 11418, 13, 1, 50, 6475, 72, 338, 553, 11738, 89, 531, 13, 1, 40, 1183, 1208, 553, 8913, 531, 11, 366, 40, 3285, 484, 1494, 1123, 584, 866, 612, 526, 2025, 1711, 1568, 11, 673, 373, 7067, 8587, 422, 257, 29494, 18933, 287, 257, 2330, 256, 12, 15600, 290, 6131, 1360, 22948, 22078, 13, 50, 6475, 72, 338, 373, 281, 32387, 29500, 2157, 257, 2493, 1589, 20933, 11, 256, 2306, 12768, 9664, 23738, 351, 257, 2010, 286, 7888, 7771, 18018, 13, 383, 20749, 11, 351, 257, 3420, 379, 2035, 886, 11, 373, 257, 14897, 1633, 5354, 23934, 262, 3833, 22577, 326, 4855, 262, 29500, 13, 34070, 35414, 13917, 547, 27527, 284, 262, 35960, 3822, 13387, 379, 20016, 11, 475, 749, 286, 606, 550, 587, 5445, 13, 383, 1633, 373, 21151, 290, 1969, 351, 262, 8508, 286, 15488, 290, 10017, 13, 14202, 286, 326, 5597, 683, 329, 262, 13478, 11, 262, 4315, 11, 262, 20170, 289, 1530, 11, 262, 38879, 13595, 1039, 286, 1657, 11061, 262, 29500, 13, 1482, 38669, 1017, 19458, 1497, 287, 33355, 284, 257, 1611, 286, 4318, 3800, 11, 257, 4376, 9197, 5858, 276, 351, 257, 31133, 278, 6546, 316, 286, 20128, 7733, 13, 1400, 1657, 475, 262, 31912, 9474, 326, 14869, 290, 277, 677, 28970, 2029, 262, 5858, 11, 8186, 2259, 262, 8650, 286, 262, 734, 1450, 2174, 13, 4285, 1045, 286, 17779, 7523, 8278, 422, 262, 33355, 11, 38193, 1566, 340, 7425, 28629, 900, 510, 416, 262, 698, 3618, 326, 4855, 262, 29500, 13, 1400, 2128, 475, 262, 38952, 1308, 1806, 286, 262, 698, 3618, 290, 262, 37479, 12704, 286, 262, 8486, 13, 8134, 12609, 7577, 35456, 1973, 30236, 338, 18405, 355, 262, 1450, 48149, 13, 383, 31912, 9474, 547, 3478, 12, 6477, 7842, 6637, 26, 379, 3478, 11, 262, 23110, 484, 2714, 547, 655, 739, 257, 16430, 890, 13, 383, 638, 361, 891, 4799, 338, 11762, 318, 262, 277, 12137, 338, 11762, 11, 8913, 12086, 11, 262, 9353, 41108, 11, 15683, 19874, 351, 11865, 13, 383, 23110, 3947, 284, 1445, 286, 511, 220, 898, 8178, 11, 1278, 2530, 351, 257, 12146, 3092, 286, 25615, 832, 262, 44606, 290, 8318, 286, 511, 9280, 11, 966, 6427, 966, 11, 355, 262, 1450, 13488, 329, 281, 4756, 13, 30236, 338, 18529, 44866, 1986, 373, 7209, 290, 991, 11, 4964, 13, 1, 40, 1183, 467, 1064, 514, 617, 2057, 553, 8913, 531, 13, 1375, 14464, 11, 2626, 287, 50152, 286, 262, 9280, 13, 1544, 1422, 470, 588, 428, 1295, 13, 1544, 2900, 290, 6807, 736, 656, 262, 16187, 13, 14190, 3223, 13, 14190, 5897, 13, 464, 4315, 11, 339, 2497, 11, 373, 4632, 4960, 13, 1892, 1107, 257, 5265, 2254, 4315, 13, 9634, 82, 866, 422, 262, 10389, 5823, 13, 679, 4385, 326, 4001, 262, 13478, 550, 262, 7546, 286, 617, 6355, 18136, 5583, 13, 679, 14028, 11589, 644, 340, 561, 307, 588, 11, 1762, 477, 534, 1204, 329, 530, 1976, 64, 571, 19231, 13, 5834, 5627, 11, 1664, 2537, 10295, 11, 1664, 14825, 13, 1544, 1549, 925, 3016, 257, 1336, 10349, 286, 262, 29500, 878, 339, 1043, 262, 2057, 40308, 13, 679, 5839, 46251, 2072, 72, 319, 43370, 364, 392, 734, 7331, 266, 6969, 6383, 684, 286, 6099, 13, 2671, 5077, 510, 379, 262, 31912, 9474, 11, 339, 2497, 326, 2910, 49699, 530, 3785, 338, 7721, 13, 45816, 7586, 10746, 6908, 992, 866, 262, 43370, 364, 290, 625, 465, 638, 34083, 13, 31334, 1528, 290, 339, 1549, 14509, 287, 13, 1002, 339, 4838, 465, 2951, 783, 11, 339, 1549, 766, 262, 17593, 13, 2484, 9797, 19074, 355, 262, 31912, 9474, 28507, 832, 511, 9280, 13, 6423, 262, 3252, 2540, 284, 29654, 1022, 465, 12450, 13, 317, 4692]}.\n",
      "Sample 11 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [1701, 3347, 3332, 351, 607, 736, 284, 262, 3355, 11, 379, 262, 1290, 886, 286, 262, 30834, 13, 1375, 550, 607, 14475, 510, 11, 19186, 607, 38163, 319, 606, 26, 262, 13385, 3524, 31746, 286, 257, 5104, 20043, 660, 16790, 9349, 422, 607, 2832, 13, 1, 2504, 345, 287, 262, 27210, 1701, 679, 5954, 262, 25834, 866, 13, 366, 8496, 338, 21776, 1701, 1, 17889, 326, 43140, 5078, 526, 1544, 750, 13, 1, 2504, 534, 2576, 30, 21776, 1701, 1544, 14464, 13, 1, 3347, 338, 3750, 13, 309, 566, 534, 7286, 14299, 13, 6416, 10927, 5141, 13, 1867, 546, 262, 2485, 11, 582, 1701, 1375, 12408, 40070, 15232, 13, 2332, 8242, 547, 2042, 11, 262, 18728, 286, 2042, 14412, 2769, 287, 262, 4124, 6513, 321, 13, 1, 40, 1718, 340, 736, 284, 11466, 11, 1392, 616, 14667, 13, 12324, 465, 16043, 736, 284, 683, 329, 2063, 644, 314, 3432, 13, 921, 765, 262, 1637, 1701, 1, 2949, 526, 1, 19633, 617, 5894, 4771, 30, 1439, 314, 1392, 11, 826, 783, 526, 1, 2061, 1392, 656, 345, 9975, 30, 4162, 1549, 345, 2834, 326, 3715, 379, 262, 27210, 30, 314, 550, 284, 2085, 510, 428, 5602, 330, 404, 1625, 706, 502, 351, 299, 3316, 6238, 526, 1, 43, 22261, 531, 345, 547, 8066, 1494, 502, 526, 1, 43, 22261, 531, 30, 314, 1239, 2497, 607, 878, 314, 1625, 510, 994, 526, 1, 1639, 3588, 470, 351, 45874, 1701, 3347, 14682, 607, 1182, 13, 679, 6939, 326, 262, 15232, 547, 19797, 1146, 1035, 316, 11, 38689, 607, 37037, 13, 383, 8465, 18405, 3947, 284, 1663, 422, 7209, 14005, 4168, 2029, 607, 19353, 35095, 11, 23665, 416, 3223, 4190, 2005, 287, 257, 5210, 427, 363, 13, 464, 9353, 41108, 1088, 262, 277, 29257, 547, 36808, 11, 2330, 28395, 351, 23895, 15601, 45459, 13, 383, 23361, 3114, 11666, 13, 366, 40, 892, 345, 27527, 510, 11, 8913, 13, 314, 3751, 510, 290, 345, 655, 4197, 502, 826, 656, 534, 3950, 4286, 526, 1, 2396, 644, 466, 345, 765, 11, 10846, 1701, 679, 264, 14655, 736, 1028, 262, 25834, 13, 1, 1639, 13, 1881, 2107, 1767, 11, 14290, 991, 6454, 16572, 13, 30236, 11, 8913, 13, 2011, 1438, 338, 30236, 13, 314, 1101, 13157, 345, 329, 262, 582, 314, 670, 329, 13, 2329, 3382, 284, 1561, 11, 318, 477, 13, 15658, 3382, 284, 5938, 345, 526, 1, 2504, 338, 922, 526, 1, 705, 34, 19598, 314, 466, 5938, 661, 3360, 11, 8913, 13, 314, 4724, 340, 338, 655, 262, 835, 314, 1101, 28217, 526, 1375, 12408, 5381, 2042, 29144, 293, 1032, 21029, 290, 257, 39392, 2042, 15224, 2005, 422, 617, 36908, 9664, 326, 3947, 284, 17565, 1657, 13, 366, 1532, 314, 1234, 428, 35970, 7145, 1497, 11, 481, 345, 307, 2562, 11, 8913, 30, 921, 804, 588, 345, 588, 284, 1011, 8531, 8395, 526, 1, 10814, 11, 314, 1101, 845, 2562, 13, 314, 1101, 257, 4574, 2502, 11, 645, 1917, 526, 1, 2504, 338, 3734, 11, 582, 526, 383, 277, 29257, 23717, 656, 262, 2042, 15224, 13, 366, 8128, 345, 1949, 284, 5089, 1088, 351, 502, 11, 345, 1183, 307, 2263, 530, 286, 262, 8531, 395, 8395, 286, 534, 2187, 1204, 526, 3347, 2714, 503, 607, 2832, 11, 39513, 510, 11, 262, 2330, 9353, 4622, 4104, 11, 290, 351, 257, 8523, 35282, 3904, 11, 3478, 4274, 12, 48916, 11, 1440, 12, 1087, 16912, 16578, 30242, 20784, 27803, 422, 511, 3821, 654, 11061, 262, 15601, 45459, 23361, 13, 3347, 13541, 13, 383, 20784, 6364, 25518, 13, 14126, 362, 1925, 23718, 2254, 15261, 3260, 257, 614, 286, 19540, 1040, 11, 262, 2119, 319, 262, 8208, 12, 43556, 4314, 286, 262, 609, 23718, 31248, 3947, 9812, 13, 632, 373, 3478, 10700, 416, 3624, 11, 2063, 286, 257, 18389, 13, 317, 2330, 34492, 19540, 13761, 3110, 2876, 2434, 319, 257, 1877, 3084, 416, 262, 22292, 5405, 13043, 326, 4721, 4291, 257, 7135, 29780, 13, 1, 3855, 617, 6891, 287, 345, 13, 6803, 588, 345, 761, 340, 526, 1375, 1718, 572, 607, 2042, 15224, 26, 262, 277, 29257, 9174, 11061, 607, 3211, 287, 257, 2042, 36104, 8163, 7805, 13, 1375, 12408, 257, 14368, 626, 408, 12768, 2834, 2502, 351, 8631, 7771, 1976, 2419, 1973, 1123, 8163, 13, 18003, 13288, 11, 8913, 3066, 11, 1017, 33307, 6891, 656, 257, 6016, 2266, 25152, 13, 2399, 5101, 290, 7405, 2936, 588, 484, 547, 925, 503, 286, 4898, 13, 1, 20448, 526, 679, 3114, 510, 11, 4379, 262, 582, 329, 262, 717, 640, 13, 366, 3666, 1438, 318, 943, 2781, 496, 526, 383, 3223, 33192, 373, 1280, 284, 262, 16139, 11, 262, 3154, 7721, 4190, 1203, 290, 27014, 11, 262, 11384, 6228, 290, 1327, 13, 4518, 2951, 523, 14005, 484, 925, 8913, 892, 286, 49024, 13, 366, 16012, 338, 510, 11, 8913, 13, 770, 318, 534, 9670, 1110, 11, 2933, 526, 20448, 30712, 465, 3211, 35303, 290, 262, 582, 3538, 288, 17758, 262, 629, 1940, 278, 6891, 13, 4373, 18746, 2491, 866, 262, 40260, 11464, 20189, 3355, 13, 679, 2497, 262, 32558, 3869, 5858, 832, 262, 1364, 49918, 13, 6093, 12700, 13, 383, 582, 13541, 13, 1, 3855, 534, 6891, 11, 8913, 553, 30236, 531, 13, 366, 1639, 821, 8788, 11, 475, 345, 821, 407, 1016, 6609, 705, 47163, 943, 2781, 496, 468, 465, 910, 526, 1375, 3332, 3272, 40898, 319, 257, 23938, 13294, 261, 290, 2540, 284, 2214, 36311, 262, 277, 29257, 1231, 41656, 284, 804, 379, 340, 13, 14968, 22353, 9646, 355, 339, 12606, 284, 262, 3084, 290, 1006, 2967, 465, 6508, 13, 1, 23307, 1862, 284, 3505, 262, 1175, 11, 3588, 470, 345, 11, 8913], 'labels': [1701, 3347, 3332, 351, 607, 736, 284, 262, 3355, 11, 379, 262, 1290, 886, 286, 262, 30834, 13, 1375, 550, 607, 14475, 510, 11, 19186, 607, 38163, 319, 606, 26, 262, 13385, 3524, 31746, 286, 257, 5104, 20043, 660, 16790, 9349, 422, 607, 2832, 13, 1, 2504, 345, 287, 262, 27210, 1701, 679, 5954, 262, 25834, 866, 13, 366, 8496, 338, 21776, 1701, 1, 17889, 326, 43140, 5078, 526, 1544, 750, 13, 1, 2504, 534, 2576, 30, 21776, 1701, 1544, 14464, 13, 1, 3347, 338, 3750, 13, 309, 566, 534, 7286, 14299, 13, 6416, 10927, 5141, 13, 1867, 546, 262, 2485, 11, 582, 1701, 1375, 12408, 40070, 15232, 13, 2332, 8242, 547, 2042, 11, 262, 18728, 286, 2042, 14412, 2769, 287, 262, 4124, 6513, 321, 13, 1, 40, 1718, 340, 736, 284, 11466, 11, 1392, 616, 14667, 13, 12324, 465, 16043, 736, 284, 683, 329, 2063, 644, 314, 3432, 13, 921, 765, 262, 1637, 1701, 1, 2949, 526, 1, 19633, 617, 5894, 4771, 30, 1439, 314, 1392, 11, 826, 783, 526, 1, 2061, 1392, 656, 345, 9975, 30, 4162, 1549, 345, 2834, 326, 3715, 379, 262, 27210, 30, 314, 550, 284, 2085, 510, 428, 5602, 330, 404, 1625, 706, 502, 351, 299, 3316, 6238, 526, 1, 43, 22261, 531, 345, 547, 8066, 1494, 502, 526, 1, 43, 22261, 531, 30, 314, 1239, 2497, 607, 878, 314, 1625, 510, 994, 526, 1, 1639, 3588, 470, 351, 45874, 1701, 3347, 14682, 607, 1182, 13, 679, 6939, 326, 262, 15232, 547, 19797, 1146, 1035, 316, 11, 38689, 607, 37037, 13, 383, 8465, 18405, 3947, 284, 1663, 422, 7209, 14005, 4168, 2029, 607, 19353, 35095, 11, 23665, 416, 3223, 4190, 2005, 287, 257, 5210, 427, 363, 13, 464, 9353, 41108, 1088, 262, 277, 29257, 547, 36808, 11, 2330, 28395, 351, 23895, 15601, 45459, 13, 383, 23361, 3114, 11666, 13, 366, 40, 892, 345, 27527, 510, 11, 8913, 13, 314, 3751, 510, 290, 345, 655, 4197, 502, 826, 656, 534, 3950, 4286, 526, 1, 2396, 644, 466, 345, 765, 11, 10846, 1701, 679, 264, 14655, 736, 1028, 262, 25834, 13, 1, 1639, 13, 1881, 2107, 1767, 11, 14290, 991, 6454, 16572, 13, 30236, 11, 8913, 13, 2011, 1438, 338, 30236, 13, 314, 1101, 13157, 345, 329, 262, 582, 314, 670, 329, 13, 2329, 3382, 284, 1561, 11, 318, 477, 13, 15658, 3382, 284, 5938, 345, 526, 1, 2504, 338, 922, 526, 1, 705, 34, 19598, 314, 466, 5938, 661, 3360, 11, 8913, 13, 314, 4724, 340, 338, 655, 262, 835, 314, 1101, 28217, 526, 1375, 12408, 5381, 2042, 29144, 293, 1032, 21029, 290, 257, 39392, 2042, 15224, 2005, 422, 617, 36908, 9664, 326, 3947, 284, 17565, 1657, 13, 366, 1532, 314, 1234, 428, 35970, 7145, 1497, 11, 481, 345, 307, 2562, 11, 8913, 30, 921, 804, 588, 345, 588, 284, 1011, 8531, 8395, 526, 1, 10814, 11, 314, 1101, 845, 2562, 13, 314, 1101, 257, 4574, 2502, 11, 645, 1917, 526, 1, 2504, 338, 3734, 11, 582, 526, 383, 277, 29257, 23717, 656, 262, 2042, 15224, 13, 366, 8128, 345, 1949, 284, 5089, 1088, 351, 502, 11, 345, 1183, 307, 2263, 530, 286, 262, 8531, 395, 8395, 286, 534, 2187, 1204, 526, 3347, 2714, 503, 607, 2832, 11, 39513, 510, 11, 262, 2330, 9353, 4622, 4104, 11, 290, 351, 257, 8523, 35282, 3904, 11, 3478, 4274, 12, 48916, 11, 1440, 12, 1087, 16912, 16578, 30242, 20784, 27803, 422, 511, 3821, 654, 11061, 262, 15601, 45459, 23361, 13, 3347, 13541, 13, 383, 20784, 6364, 25518, 13, 14126, 362, 1925, 23718, 2254, 15261, 3260, 257, 614, 286, 19540, 1040, 11, 262, 2119, 319, 262, 8208, 12, 43556, 4314, 286, 262, 609, 23718, 31248, 3947, 9812, 13, 632, 373, 3478, 10700, 416, 3624, 11, 2063, 286, 257, 18389, 13, 317, 2330, 34492, 19540, 13761, 3110, 2876, 2434, 319, 257, 1877, 3084, 416, 262, 22292, 5405, 13043, 326, 4721, 4291, 257, 7135, 29780, 13, 1, 3855, 617, 6891, 287, 345, 13, 6803, 588, 345, 761, 340, 526, 1375, 1718, 572, 607, 2042, 15224, 26, 262, 277, 29257, 9174, 11061, 607, 3211, 287, 257, 2042, 36104, 8163, 7805, 13, 1375, 12408, 257, 14368, 626, 408, 12768, 2834, 2502, 351, 8631, 7771, 1976, 2419, 1973, 1123, 8163, 13, 18003, 13288, 11, 8913, 3066, 11, 1017, 33307, 6891, 656, 257, 6016, 2266, 25152, 13, 2399, 5101, 290, 7405, 2936, 588, 484, 547, 925, 503, 286, 4898, 13, 1, 20448, 526, 679, 3114, 510, 11, 4379, 262, 582, 329, 262, 717, 640, 13, 366, 3666, 1438, 318, 943, 2781, 496, 526, 383, 3223, 33192, 373, 1280, 284, 262, 16139, 11, 262, 3154, 7721, 4190, 1203, 290, 27014, 11, 262, 11384, 6228, 290, 1327, 13, 4518, 2951, 523, 14005, 484, 925, 8913, 892, 286, 49024, 13, 366, 16012, 338, 510, 11, 8913, 13, 770, 318, 534, 9670, 1110, 11, 2933, 526, 20448, 30712, 465, 3211, 35303, 290, 262, 582, 3538, 288, 17758, 262, 629, 1940, 278, 6891, 13, 4373, 18746, 2491, 866, 262, 40260, 11464, 20189, 3355, 13, 679, 2497, 262, 32558, 3869, 5858, 832, 262, 1364, 49918, 13, 6093, 12700, 13, 383, 582, 13541, 13, 1, 3855, 534, 6891, 11, 8913, 553, 30236, 531, 13, 366, 1639, 821, 8788, 11, 475, 345, 821, 407, 1016, 6609, 705, 47163, 943, 2781, 496, 468, 465, 910, 526, 1375, 3332, 3272, 40898, 319, 257, 23938, 13294, 261, 290, 2540, 284, 2214, 36311, 262, 277, 29257, 1231, 41656, 284, 804, 379, 340, 13, 14968, 22353, 9646, 355, 339, 12606, 284, 262, 3084, 290, 1006, 2967, 465, 6508, 13, 1, 23307, 1862, 284, 3505, 262, 1175, 11, 3588, 470, 345, 11, 8913]}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group texts into blocks\n",
    "# WARNING: Be sure to subtract the size of the soft prompt!\n",
    "block_size = 1024 - n_tokens\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from mkultra.trainers import SoftPromptTrainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "\n",
    "optimizer = Adafactor([model.get_soft_params()], scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)\n",
    "scheduler = transformers.get_constant_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=1000)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "trainer = SoftPromptTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    optimizers = (optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/348 [00:00<?, ?it/s]C:\\Users\\STARSTRUCK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:562: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
      " 33%|███▎      | 116/348 [00:30<01:00,  3.81it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 8.00 GiB total capacity; 4.43 GiB already allocated; 1.14 GiB free; 5.03 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Don't support larger batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mper_device_train_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtpu_metrics_debug\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   1849\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m             \u001b[1;31m# Prediction step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2000\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2002\u001b[0m             \u001b[1;31m# Update containers on host\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   2200\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2201\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2202\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2203\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\models\\tuning.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Drop most of the args for now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         return super().forward(\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[1;31m# Flatten the tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 8.00 GiB total capacity; 4.43 GiB already allocated; 1.14 GiB free; 5.03 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}