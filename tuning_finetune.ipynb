{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "source": [
    "# Tuning on Datasets\n",
    "This sheet is adapted from the language modeling example at\n",
    "https://github.com/huggingface/transformers/tree/master/examples/pytorch\n",
    "\n",
    "The process is similar to finetuning, but only a set of input embeddings (given by model.get_soft_params()) are used as optimizer parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Decide the length of your soft prompt in tokens.\n",
    "n_tokens = 100\n",
    "model.initialize_soft_prompt(n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load an existing soft prompt\n",
    "#sp = SoftPrompt.from_file(\"existing_soft_prompt.json\")\n",
    "#model.set_soft_prompt(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-812ce7cd67147d37\n",
      "Reusing dataset text (C:\\Users\\STARSTRUCK\\.cache\\huggingface\\datasets\\text\\default-812ce7cd67147d37\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"datasets/neuromancer_reformatted.txt\"\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": path, \"validation\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.92ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 28.57ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1], 'input_ids': [1525, 3977, 20400]}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "62, 37368, 4025, 11, 1165, 3223, 13, 34686, 41976, 290, 262, 25035, 1203, 28668, 1991, 704, 1978, 319, 262, 3996, 11, 45668, 625, 416, 262, 2832, 351, 511, 6016, 23361, 13, 383, 3996, 373, 6546, 783, 351, 38744, 286, 7872, 276, 11, 44494, 35938, 326, 1067, 11137, 379, 257, 3638, 13, 337, 6421, 286, 8977, 29939, 1088, 34686, 41976, 290, 262, 665, 19811, 21755, 11, 262, 629, 333, 14992, 11, 6757, 10813, 11, 1275, 11697, 2832, 13, 20448, 27846, 379, 30236, 13, 2332, 1986, 373, 9178, 26, 262, 7577, 286, 34686, 41976, 338, 20128, 6002, 276, 290, 2900, 287, 607, 22353, 13, 943, 2781, 496, 373, 21804, 2651, 11, 465, 2832, 2835, 262, 10717, 286, 257, 8237, 20721, 11, 465, 14005, 2951, 5969, 319, 262, 3800, 11, 262, 21377, 2119, 13, 3844, 21755, 290, 28668, 550, 23791, 11, 290, 34686, 41976, 427, 4185, 1068, 13, 383, 1182, 373, 612, 11, 262, 2939, 1844, 13, 30236, 338, 1986, 11, 351, 7209, 627, 3378, 46978, 32249, 262, 2951, 13, 34686, 41976, 290, 262, 30236, 12, 9060, 2540, 284, 3155, 351, 257, 16434, 12245, 13, 3244, 262, 2939, 6364, 7083, 257, 26573, 276, 1021, 290, 22820, 19289, 663, 1936, 20784, 13, 2080, 257, 2786, 9610, 11, 4320, 2339, 8958, 341, 11, 340, 374, 4335, 34686, 41976, 338, 6247, 736, 13, 8913, 4978, 257, 19350, 286, 7362, 19656, 11, 475, 339, 373, 1541, 510, 290, 37829, 329, 262, 3420, 13, 1544, 20918, 863, 625, 257, 8278, 3822, 47590, 656, 262, 5897, 10150, 286, 262, 13546, 13, 13742, 326, 550, 3947, 284, 1969, 1088, 465, 1182, 588, 257, 410, 786, 550, 2716, 683, 783, 13, 42, 710, 10809, 11, 465, 19353, 1028, 262, 3608, 4898, 11, 339, 18484, 1973, 262, 19337, 13546, 379, 262, 6016, 22491, 286, 262, 45363, 449, 5028, 4643, 710, 13, 20448, 550, 1775, 262, 7090, 878, 26, 618, 339, 1549, 587, 257, 15287, 287, 262, 1338, 13132, 11, 484, 1549, 1444, 340, 11, 366, 25966, 278, 1103, 526, 679, 12086, 7888, 15295, 15868, 504, 739, 3687, 12075, 4675, 8091, 11, 28368, 1103, 284, 262, 2068, 4405, 286, 257, 49529, 11, 4320, 36960, 50130, 278, 290, 6225, 11, 262, 47747, 364, 537, 5912, 287, 640], 'labels': [6247, 13, 520, 1328, 40212, 351, 24887, 7586, 28806, 13, 317, 2060, 28287, 288, 22393, 2029, 262, 3996, 319, 257, 19074, 4129, 286, 2042, 6503, 13, 8913, 714, 766, 262, 6546, 26749, 286, 8977, 319, 262, 28287, 338, 6727, 12133, 13, 34686, 41976, 4721, 465, 2951, 13, 1, 40, 1549, 587, 3436, 287, 262, 2119, 11, 1464, 526, 679, 3332, 319, 262, 5118, 11, 6476, 262, 3996, 13, 383, 4171, 763, 874, 991, 11544, 287, 262, 2042, 15061, 319, 465, 14779, 417, 13, 366, 40, 836, 470, 760, 618, 314, 717, 2540, 284, 4320, 286, 607, 553, 339, 531, 11, 366, 4360, 314, 466, 3505, 326, 379, 717, 673, 373, 691, 257, 42772, 11, 257, 9082, 526, 1858, 373, 1223, 319, 262, 3996, 13, 8913, 40360, 13, 26893, 13, 1, 40, 3521, 470, 2407, 1745, 607, 11, 1745, 607, 287, 616, 2000, 13, 887, 314, 2227, 284, 1745, 607, 11, 1745, 607, 290, 517, 35713, 2399, 3809, 5281, 7138, 287, 262, 289, 1530, 286, 262, 7072, 13, 6663, 28384, 1028, 262, 1735, 286, 257, 5405, 13, 17877, 30442, 992, 13, 17877, 2073, 1965, 257, 25029, 1808, 287, 4960, 13, 366, 40, 3066, 326, 611, 314, 714, 38350, 617, 636, 286, 607, 11, 691, 257, 1402, 636, 11, 611, 314, 714, 766, 326, 636, 7138, 11, 287, 262, 749, 2818, 3703, 13, 764, 764, 22135, 32, 2415, 338, 1021, 3830, 319, 262, 33388, 783, 11, 18057, 510, 11, 262, 2330, 9353, 14005, 13, 49, 452, 41976, 23831, 2651, 11, 6497, 510, 262, 1021, 11, 290, 2540, 284, 14000, 340, 15165, 13, 383, 9353, 3888, 13, 34686, 41976, 4376, 262, 1021, 284, 465, 5422, 290, 2540, 284, 36728, 262, 9040, 286, 262, 9353, 13, 383, 23361, 547, 30267, 351, 257, 15601, 45459, 31123, 10819, 13, 32, 1021, 11, 8913, 2497, 11, 475, 407, 257, 32160, 1021, 26, 262, 4168, 17676, 736, 21461, 11, 555, 25826, 290, 555, 13034, 445, 13, 679, 12086, 257, 18785, 276, 2376, 89, 3540, 286, 410, 265, 22377, 11222, 287, 262, 4324, 286, 257, 10516, 36455, 21998, 45753, 13, 34686, 41976, 373, 4769, 262, 1021, 284, 465, 11914, 11, 43390, 663, 18057, 13, 383, 9353, 11105, 9404, 1275, 2790, 465, 1986, 13, 887, 783, 257, 1218, 1021, 3830, 319, 262, 3996, 13, 1649, 34686, 41976, 4251, 329, 340, 11, 262, 9353, 286, 262, 717, 547, 8970, 1088, 465, 15980, 11, 257, 42893, 286, 11222, 290, 9970, 13, 464, 719, 28775, 351, 257, 28201, 5387, 9156, 286, 663, 898, 13, 383, 5101, 547, 1306, 13, 43391, 13, 49379, 13, 383, 7405, 547, 845, 4950, 13, 8913, 338, 1182, 48836, 3077, 13, 2399, 13589, 373, 5894, 13, 679, 24070, 262, 938, 286, 262, 8237, 13, 49, 452, 41976, 373, 287, 262, 3996, 783, 11, 12105, 13, 2399, 9528, 550, 587, 257, 636, 286, 262, 20128, 11, 475, 8913, 3521, 470, 3505, 4379, 340, 22100, 1497, 13, 383, 2042, 15061, 3830, 379, 262, 2366, 286, 262, 3996, 11, 991, 384, 316, 722, 351, 663, 4171, 8434, 16506, 13, 3244, 262, 28668, 7042, 11, 355, 34686, 41976, 1275, 2790, 340, 656, 852, 11, 2330, 11, 1182, 1203, 11, 290, 2818, 11, 673, 2945, 351, 262, 18107, 395, 21194, 286, 15488, 13, 44, 5098, 338, 1767, 13, 8913, 18484, 11, 465, 5422, 1280, 13, 887, 340, 2492, 470, 30236, 26, 340, 373, 30236, 355, 34686, 41976, 15758, 607, 13, 383, 17515, 547, 2642, 11, 262, 37368, 4025, 11, 1165, 3223, 13, 34686, 41976, 290, 262, 25035, 1203, 28668, 1991, 704, 1978, 319, 262, 3996, 11, 45668, 625, 416, 262, 2832, 351, 511, 6016, 23361, 13, 383, 3996, 373, 6546, 783, 351, 38744, 286, 7872, 276, 11, 44494, 35938, 326, 1067, 11137, 379, 257, 3638, 13, 337, 6421, 286, 8977, 29939, 1088, 34686, 41976, 290, 262, 665, 19811, 21755, 11, 262, 629, 333, 14992, 11, 6757, 10813, 11, 1275, 11697, 2832, 13, 20448, 27846, 379, 30236, 13, 2332, 1986, 373, 9178, 26, 262, 7577, 286, 34686, 41976, 338, 20128, 6002, 276, 290, 2900, 287, 607, 22353, 13, 943, 2781, 496, 373, 21804, 2651, 11, 465, 2832, 2835, 262, 10717, 286, 257, 8237, 20721, 11, 465, 14005, 2951, 5969, 319, 262, 3800, 11, 262, 21377, 2119, 13, 3844, 21755, 290, 28668, 550, 23791, 11, 290, 34686, 41976, 427, 4185, 1068, 13, 383, 1182, 373, 612, 11, 262, 2939, 1844, 13, 30236, 338, 1986, 11, 351, 7209, 627, 3378, 46978, 32249, 262, 2951, 13, 34686, 41976, 290, 262, 30236, 12, 9060, 2540, 284, 3155, 351, 257, 16434, 12245, 13, 3244, 262, 2939, 6364, 7083, 257, 26573, 276, 1021, 290, 22820, 19289, 663, 1936, 20784, 13, 2080, 257, 2786, 9610, 11, 4320, 2339, 8958, 341, 11, 340, 374, 4335, 34686, 41976, 338, 6247, 736, 13, 8913, 4978, 257, 19350, 286, 7362, 19656, 11, 475, 339, 373, 1541, 510, 290, 37829, 329, 262, 3420, 13, 1544, 20918, 863, 625, 257, 8278, 3822, 47590, 656, 262, 5897, 10150, 286, 262, 13546, 13, 13742, 326, 550, 3947, 284, 1969, 1088, 465, 1182, 588, 257, 410, 786, 550, 2716, 683, 783, 13, 42, 710, 10809, 11, 465, 19353, 1028, 262, 3608, 4898, 11, 339, 18484, 1973, 262, 19337, 13546, 379, 262, 6016, 22491, 286, 262, 45363, 449, 5028, 4643, 710, 13, 20448, 550, 1775, 262, 7090, 878, 26, 618, 339, 1549, 587, 257, 15287, 287, 262, 1338, 13132, 11, 484, 1549, 1444, 340, 11, 366, 25966, 278, 1103, 526, 679, 12086, 7888, 15295, 15868, 504, 739, 3687, 12075, 4675, 8091, 11, 28368, 1103, 284, 262, 2068, 4405, 286, 257, 49529, 11, 4320, 36960, 50130, 278, 290, 6225, 11, 262, 47747, 364, 537, 5912, 287, 640]}.\n",
      "Sample 108 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [11, 1223, 11061, 262, 5852, 3501, 510, 351, 257, 14643, 286, 2330, 38306, 11, 262, 13774, 5025, 13, 464, 1517, 7051, 276, 284, 257, 2245, 1115, 10700, 422, 262, 923, 286, 513, 41083, 338, 25868, 11527, 13, 1, 2437, 1290, 11, 937, 1701, 337, 3010, 36340, 4193, 683, 422, 262, 599, 33598, 6383, 355, 281, 19287, 24995, 4828, 18750, 287, 262, 1517, 338, 3113, 26247, 11, 308, 5269, 286, 7872, 11913, 2809, 35355, 422, 21081, 690, 290, 2139, 2173, 13, 383, 34492, 256, 11137, 422, 262, 736, 286, 262, 5852, 290, 32724, 9342, 572, 1973, 262, 40260, 6450, 11, 25046, 530, 13894, 25035, 2157, 340, 13, 366, 1639, 1928, 6, 2513, 11, 937, 526, 337, 3010, 36340, 1718, 262, 6203, 290, 5678, 11, 1017, 14146, 262, 6380, 45173, 625, 465, 8163, 13, 464, 4161, 8906, 48588, 1088, 8913, 338, 7393, 355, 339, 3940, 262, 16899, 578, 13, 34686, 41976, 338, 6039, 418, 13488, 329, 606, 11, 262, 11543, 8188, 290, 262, 39904, 1751, 13, 30236, 550, 5445, 262, 1333, 5835, 354, 13, 337, 3010, 36340, 9514, 606, 13, 1, 28406, 553, 8913, 531, 11, 10833, 2241, 284, 4929, 510, 351, 262, 965, 2530, 3785, 13, 366, 38, 12375, 466, 428, 826, 526, 44, 3010, 36340, 27771, 11, 2900, 11, 1278, 789, 278, 379, 683, 11, 262, 3982, 9557, 287, 465, 2832, 13, 366, 11028, 11, 937, 30, 1374, 338, 826, 1701, 1, 30074, 30236, 287, 612, 11, 475, 673, 338, 503, 286, 340, 13, 34686, 41976, 11, 339, 460, 3714, 6039, 418, 13, 6674, 339, 338, 1392, 30236, 338, 277, 29257, 526, 337, 3010, 36340, 14464, 13, 366, 1870, 612, 338, 257, 37049, 11, 257, 1641, 1767, 14864, 526, 44, 3010, 36340, 338, 22002, 2769, 2945, 13, 366, 1639, 6004, 11, 28028, 937, 553, 339, 531, 13, 366, 40, 257, 16491, 13, 887, 428, 645, 285, 6, 1907, 11, 645, 16899, 1907, 13, 28028, 1907, 259, 6, 28028, 11, 4483, 259, 6, 1312, 338, 7046, 11, 21349, 760, 30, 887, 48984, 384, 71, 314, 281, 6, 314, 256, 6, 2222, 2441, 381, 259, 6, 36065, 503, 64, 428, 526, 20448, 40360, 13, 1, 3347, 257, 16491, 553, 337, 3010, 36340, 531, 11, 355, 611, 340, 4893, 2279, 13, 366, 3844, 345, 1560, 502, 11, 937, 11, 508, 314, 407, 220, 256, 6, 1494, 526, 1, 18, 41083, 553, 339, 531, 11, 706, 257, 14985, 13, 366, 32, 2576, 612, 13, 7875, 257, 17855, 2330, 33192, 1517, 319, 11, 351, 257, 14263, 13, 775, 761, 607, 526, 2215, 484, 4251, 262, 10384, 11, 337, 3010, 36340, 6807, 3892, 287, 11, 290, 8913, 550, 645, 3572, 475, 284, 1061, 683, 13, 18, 41083, 338, 1499, 373, 36043, 11, 262, 5933, 6565, 13, 337, 3010, 36340, 10158, 683, 262, 6203, 290, 262, 5678, 290, 6807, 284, 262, 5743, 286, 262, 5933, 13, 12197, 262, 2330, 5933, 13091, 11, 612, 373, 11854, 11, 16187, 286, 262, 374, 14655, 11, 16139, 12, 8929, 31237, 286, 12387, 33359, 7714, 13, 464, 1660, 300, 6320, 37220, 1028, 262, 1735, 286, 262, 5933, 13, 1, 2990, 821, 994, 553, 8913, 531, 13, 366, 2990, 17753, 307, 526, 44, 3010, 36340, 14464, 13, 464, 717, 15452, 41159, 465, 6727, 3211, 13, 383, 3982, 9557, 45615, 11, 663, 16430, 286, 31746, 12, 34167, 4171, 287, 262, 1657, 422, 262, 5933, 13, 383, 1218, 15452, 7425, 262, 18607, 2346, 11, 7216, 340, 19493, 1973, 262, 2330, 19867, 13, 337, 3010, 36340, 3332, 866, 1327, 290, 277, 11137, 379, 262, 2042, 1517, 326, 39701, 19289, 422, 465, 3211, 13, 679, 331, 14076, 379, 340, 13, 39, 1651, 10764, 503, 286, 262, 16187, 11, 257, 2368, 15452, 3492, 287, 257, 36808, 37252, 9563, 13, 679, 35737, 13, 44, 3010, 36340, 18484, 11, 465, 1021, 991, 319, 262, 7771, 18619, 13, 1, 464, 37646, 318, 16572, 553, 262, 37049, 531, 13, 8913, 12086, 30236, 338, 6764, 286, 262, 582, 508, 1549, 2923, 607, 18854, 13, 367, 1651, 373, 1194, 13, 2449, 5321, 11, 339, 2511, 12931, 257, 2565, 286, 5897, 11, 281, 10517, 9480, 13, 679, 12408, 3424, 11, 1216, 16548, 44081, 8182, 670, 38895, 290, 2705, 3223, 10012, 326, 4197, 465, 3625, 588, 18051, 11, 6626, 379, 262, 23932, 588, 7400, 72, 24359, 13, 383, 37252, 9563, 373, 257, 13257, 3704, 11, 475, 262, 2042, 36186, 627, 1428, 326, 39701, 19289, 2029, 465, 1364, 8163, 550, 262, 804, 286, 262, 1266, 609, 23718, 3777, 12437, 13, 2399, 7586, 7721, 373, 6247, 290, 7209, 13, 1, 1639, 2005, 616, 15683, 11, 937, 11, 45967, 6, 792, 261, 6, 530, 553, 337, 3010, 36340, 531, 13, 1, 10606, 1669, 271, 2700, 553, 262, 37049, 531, 11, 275, 7855, 757, 13, 366, 6943, 2408, 11, 3105, 12, 31462, 26489, 287, 5724, 864, 13522, 13, 632, 373, 407, 5292, 526, 1, 8496, 338, 513, 41083, 1701, 8913, 12606, 284, 1302, 13970, 337, 3010, 36340, 13, 679, 2497, 326, 262, 8171, 286, 262, 15452, 287, 262, 37049, 338, 9563, 373, 588, 257, 4274, 12, 48916, 26811, 13, 366, 8496, 338, 30236, 1701, 1, 15496, 11, 8913, 526, 34686, 41976, 1625, 336, 18886, 503, 286, 262, 3223, 2157, 367, 1651, 11, 30236, 338, 277, 29257, 287, 465, 1021, 13, 366, 40, 561, 423, 2938, 943, 2781, 496, 11, 7599, 13, 4231, 356, 12965, 1037, 503, 286, 326, 371, 40197, 13946, 783, 1701, 1, 3163, 2781, 496, 318, 2636, 526, 1, 3163, 2781, 496, 1239, 11196, 11, 517, 284, 262, 966, 11, 475, 262, 1705, 8941, 2058, 355, 257, 6380, 526, 1, 35376, 76, 1133, 2923, 683, 13, 679, 338, 287, 13066, 1088, 262], 'labels': [11, 1223, 11061, 262, 5852, 3501, 510, 351, 257, 14643, 286, 2330, 38306, 11, 262, 13774, 5025, 13, 464, 1517, 7051, 276, 284, 257, 2245, 1115, 10700, 422, 262, 923, 286, 513, 41083, 338, 25868, 11527, 13, 1, 2437, 1290, 11, 937, 1701, 337, 3010, 36340, 4193, 683, 422, 262, 599, 33598, 6383, 355, 281, 19287, 24995, 4828, 18750, 287, 262, 1517, 338, 3113, 26247, 11, 308, 5269, 286, 7872, 11913, 2809, 35355, 422, 21081, 690, 290, 2139, 2173, 13, 383, 34492, 256, 11137, 422, 262, 736, 286, 262, 5852, 290, 32724, 9342, 572, 1973, 262, 40260, 6450, 11, 25046, 530, 13894, 25035, 2157, 340, 13, 366, 1639, 1928, 6, 2513, 11, 937, 526, 337, 3010, 36340, 1718, 262, 6203, 290, 5678, 11, 1017, 14146, 262, 6380, 45173, 625, 465, 8163, 13, 464, 4161, 8906, 48588, 1088, 8913, 338, 7393, 355, 339, 3940, 262, 16899, 578, 13, 34686, 41976, 338, 6039, 418, 13488, 329, 606, 11, 262, 11543, 8188, 290, 262, 39904, 1751, 13, 30236, 550, 5445, 262, 1333, 5835, 354, 13, 337, 3010, 36340, 9514, 606, 13, 1, 28406, 553, 8913, 531, 11, 10833, 2241, 284, 4929, 510, 351, 262, 965, 2530, 3785, 13, 366, 38, 12375, 466, 428, 826, 526, 44, 3010, 36340, 27771, 11, 2900, 11, 1278, 789, 278, 379, 683, 11, 262, 3982, 9557, 287, 465, 2832, 13, 366, 11028, 11, 937, 30, 1374, 338, 826, 1701, 1, 30074, 30236, 287, 612, 11, 475, 673, 338, 503, 286, 340, 13, 34686, 41976, 11, 339, 460, 3714, 6039, 418, 13, 6674, 339, 338, 1392, 30236, 338, 277, 29257, 526, 337, 3010, 36340, 14464, 13, 366, 1870, 612, 338, 257, 37049, 11, 257, 1641, 1767, 14864, 526, 44, 3010, 36340, 338, 22002, 2769, 2945, 13, 366, 1639, 6004, 11, 28028, 937, 553, 339, 531, 13, 366, 40, 257, 16491, 13, 887, 428, 645, 285, 6, 1907, 11, 645, 16899, 1907, 13, 28028, 1907, 259, 6, 28028, 11, 4483, 259, 6, 1312, 338, 7046, 11, 21349, 760, 30, 887, 48984, 384, 71, 314, 281, 6, 314, 256, 6, 2222, 2441, 381, 259, 6, 36065, 503, 64, 428, 526, 20448, 40360, 13, 1, 3347, 257, 16491, 553, 337, 3010, 36340, 531, 11, 355, 611, 340, 4893, 2279, 13, 366, 3844, 345, 1560, 502, 11, 937, 11, 508, 314, 407, 220, 256, 6, 1494, 526, 1, 18, 41083, 553, 339, 531, 11, 706, 257, 14985, 13, 366, 32, 2576, 612, 13, 7875, 257, 17855, 2330, 33192, 1517, 319, 11, 351, 257, 14263, 13, 775, 761, 607, 526, 2215, 484, 4251, 262, 10384, 11, 337, 3010, 36340, 6807, 3892, 287, 11, 290, 8913, 550, 645, 3572, 475, 284, 1061, 683, 13, 18, 41083, 338, 1499, 373, 36043, 11, 262, 5933, 6565, 13, 337, 3010, 36340, 10158, 683, 262, 6203, 290, 262, 5678, 290, 6807, 284, 262, 5743, 286, 262, 5933, 13, 12197, 262, 2330, 5933, 13091, 11, 612, 373, 11854, 11, 16187, 286, 262, 374, 14655, 11, 16139, 12, 8929, 31237, 286, 12387, 33359, 7714, 13, 464, 1660, 300, 6320, 37220, 1028, 262, 1735, 286, 262, 5933, 13, 1, 2990, 821, 994, 553, 8913, 531, 13, 366, 2990, 17753, 307, 526, 44, 3010, 36340, 14464, 13, 464, 717, 15452, 41159, 465, 6727, 3211, 13, 383, 3982, 9557, 45615, 11, 663, 16430, 286, 31746, 12, 34167, 4171, 287, 262, 1657, 422, 262, 5933, 13, 383, 1218, 15452, 7425, 262, 18607, 2346, 11, 7216, 340, 19493, 1973, 262, 2330, 19867, 13, 337, 3010, 36340, 3332, 866, 1327, 290, 277, 11137, 379, 262, 2042, 1517, 326, 39701, 19289, 422, 465, 3211, 13, 679, 331, 14076, 379, 340, 13, 39, 1651, 10764, 503, 286, 262, 16187, 11, 257, 2368, 15452, 3492, 287, 257, 36808, 37252, 9563, 13, 679, 35737, 13, 44, 3010, 36340, 18484, 11, 465, 1021, 991, 319, 262, 7771, 18619, 13, 1, 464, 37646, 318, 16572, 553, 262, 37049, 531, 13, 8913, 12086, 30236, 338, 6764, 286, 262, 582, 508, 1549, 2923, 607, 18854, 13, 367, 1651, 373, 1194, 13, 2449, 5321, 11, 339, 2511, 12931, 257, 2565, 286, 5897, 11, 281, 10517, 9480, 13, 679, 12408, 3424, 11, 1216, 16548, 44081, 8182, 670, 38895, 290, 2705, 3223, 10012, 326, 4197, 465, 3625, 588, 18051, 11, 6626, 379, 262, 23932, 588, 7400, 72, 24359, 13, 383, 37252, 9563, 373, 257, 13257, 3704, 11, 475, 262, 2042, 36186, 627, 1428, 326, 39701, 19289, 2029, 465, 1364, 8163, 550, 262, 804, 286, 262, 1266, 609, 23718, 3777, 12437, 13, 2399, 7586, 7721, 373, 6247, 290, 7209, 13, 1, 1639, 2005, 616, 15683, 11, 937, 11, 45967, 6, 792, 261, 6, 530, 553, 337, 3010, 36340, 531, 13, 1, 10606, 1669, 271, 2700, 553, 262, 37049, 531, 11, 275, 7855, 757, 13, 366, 6943, 2408, 11, 3105, 12, 31462, 26489, 287, 5724, 864, 13522, 13, 632, 373, 407, 5292, 526, 1, 8496, 338, 513, 41083, 1701, 8913, 12606, 284, 1302, 13970, 337, 3010, 36340, 13, 679, 2497, 326, 262, 8171, 286, 262, 15452, 287, 262, 37049, 338, 9563, 373, 588, 257, 4274, 12, 48916, 26811, 13, 366, 8496, 338, 30236, 1701, 1, 15496, 11, 8913, 526, 34686, 41976, 1625, 336, 18886, 503, 286, 262, 3223, 2157, 367, 1651, 11, 30236, 338, 277, 29257, 287, 465, 1021, 13, 366, 40, 561, 423, 2938, 943, 2781, 496, 11, 7599, 13, 4231, 356, 12965, 1037, 503, 286, 326, 371, 40197, 13946, 783, 1701, 1, 3163, 2781, 496, 318, 2636, 526, 1, 3163, 2781, 496, 1239, 11196, 11, 517, 284, 262, 966, 11, 475, 262, 1705, 8941, 2058, 355, 257, 6380, 526, 1, 35376, 76, 1133, 2923, 683, 13, 679, 338, 287, 13066, 1088, 262]}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group texts into blocks\n",
    "# WARNING: Be sure to subtract the size of the soft prompt!\n",
    "block_size = 1024 - n_tokens\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import default_data_collator\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=1\n",
    ")\n",
    "steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to set the optimizer to tune the soft prompt (given by model.get_soft_params()).\n",
    "optimizer = Adafactor([model.get_soft_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\STARSTRUCK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:562: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
      "1: Loss: 3.9363033771514893\n",
      "2: Loss: 4.437915802001953\n",
      "3: Loss: 4.698455810546875\n",
      "4: Loss: 4.394512176513672\n",
      "5: Loss: 4.525238037109375\n",
      "6: Loss: 4.355782508850098\n",
      "7: Loss: 4.301104545593262\n",
      "8: Loss: 4.29131555557251\n",
      "9: Loss: 4.497940540313721\n",
      "10: Loss: 4.546276092529297\n",
      "11: Loss: 4.205498695373535\n",
      "12: Loss: 4.343509674072266\n",
      "13: Loss: 4.519213676452637\n",
      "14: Loss: 4.268179893493652\n",
      "15: Loss: 3.900041341781616\n",
      "16: Loss: 3.981795310974121\n",
      "17: Loss: 4.636607646942139\n",
      "18: Loss: 4.0246453285217285\n",
      "19: Loss: 3.9811670780181885\n",
      "20: Loss: 4.103039264678955\n",
      "21: Loss: 4.189911842346191\n",
      "22: Loss: 3.935837507247925\n",
      "23: Loss: 4.241316795349121\n",
      "24: Loss: 4.428104400634766\n",
      "25: Loss: 4.221592903137207\n",
      "26: Loss: 3.9282944202423096\n",
      "27: Loss: 4.077696800231934\n",
      "28: Loss: 4.163854598999023\n",
      "29: Loss: 4.226170539855957\n",
      "30: Loss: 4.5144453048706055\n",
      "31: Loss: 4.419915676116943\n",
      "32: Loss: 4.2527008056640625\n",
      "33: Loss: 4.129134178161621\n",
      "34: Loss: 4.46848726272583\n",
      "35: Loss: 4.09928560256958\n",
      "36: Loss: 4.362670421600342\n",
      "37: Loss: 4.091490268707275\n",
      "38: Loss: 4.286121368408203\n",
      "39: Loss: 3.9651994705200195\n",
      "40: Loss: 4.266005039215088\n",
      "41: Loss: 4.257222652435303\n",
      "42: Loss: 3.9361088275909424\n",
      "43: Loss: 4.181943416595459\n",
      "44: Loss: 4.363131999969482\n",
      "45: Loss: 4.4808197021484375\n",
      "46: Loss: 4.027883529663086\n",
      "47: Loss: 4.171851634979248\n",
      "48: Loss: 4.14778470993042\n",
      "49: Loss: 4.493773460388184\n",
      "50: Loss: 4.222681522369385\n",
      "51: Loss: 4.150875568389893\n",
      "52: Loss: 4.264112949371338\n",
      "53: Loss: 4.453824043273926\n",
      "54: Loss: 4.202238082885742\n",
      "55: Loss: 4.216929912567139\n",
      "56: Loss: 4.117290496826172\n",
      "57: Loss: 4.109422206878662\n",
      "58: Loss: 4.17289924621582\n",
      "59: Loss: 4.687710285186768\n",
      "60: Loss: 4.065034866333008\n",
      "61: Loss: 4.451500415802002\n",
      "62: Loss: 4.612093925476074\n",
      "63: Loss: 4.372113227844238\n",
      "64: Loss: 3.6271677017211914\n",
      "65: Loss: 4.418488502502441\n",
      "66: Loss: 4.151826858520508\n",
      "67: Loss: 4.340658187866211\n",
      "68: Loss: 3.8359057903289795\n",
      "69: Loss: 4.220722198486328\n",
      "70: Loss: 4.014372825622559\n",
      "71: Loss: 4.136779308319092\n",
      "72: Loss: 4.109051704406738\n",
      "73: Loss: 4.681361675262451\n",
      "74: Loss: 4.4669694900512695\n",
      "75: Loss: 4.24405574798584\n",
      "76: Loss: 4.475664138793945\n",
      "77: Loss: 4.0479865074157715\n",
      "78: Loss: 4.376590251922607\n",
      "79: Loss: 4.211118698120117\n",
      "80: Loss: 4.551955223083496\n",
      "81: Loss: 4.450276851654053\n",
      "82: Loss: 4.287016868591309\n",
      "83: Loss: 4.03087854385376\n",
      "84: Loss: 4.212594032287598\n",
      "85: Loss: 3.7298734188079834\n",
      "86: Loss: 4.195497512817383\n",
      "87: Loss: 4.207446575164795\n",
      "88: Loss: 4.419020175933838\n",
      "89: Loss: 4.174505710601807\n",
      "90: Loss: 4.533751964569092\n",
      "91: Loss: 4.21281099319458\n",
      "92: Loss: 4.062732696533203\n",
      "93: Loss: 4.297962188720703\n",
      "94: Loss: 4.39919376373291\n",
      "95: Loss: 3.9197494983673096\n",
      "96: Loss: 4.428443431854248\n",
      "97: Loss: 4.174369812011719\n",
      "98: Loss: 4.136077880859375\n",
      "99: Loss: 3.888153553009033\n",
      "100: Loss: 4.484286308288574\n",
      "101: Loss: 4.240795612335205\n",
      "102: Loss: 4.30432653427124\n",
      "103: Loss: 4.2741923332214355\n",
      "104: Loss: 4.123308181762695\n",
      "105: Loss: 4.438809394836426\n",
      "106: Loss: 3.9685378074645996\n",
      "107: Loss: 4.347555160522461\n",
      "108: Loss: 4.524048805236816\n",
      "109: Loss: 4.0297770500183105\n",
      "110: Loss: 4.253750801086426\n",
      "111: Loss: 4.319068431854248\n",
      "112: Loss: 4.508455753326416\n",
      "113: Loss: 4.3922929763793945\n",
      "114: Loss: 4.427478313446045\n",
      "115: Loss: 4.37664794921875\n",
      "116: Loss: 4.464567184448242\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs = 1\n",
    "total_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k:v.type(torch.long).to(\"cuda\") for k,v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_step += 1\n",
    "        print(f\"{total_step}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your soft prompt\n",
    "metadata = { \"name\" : \"My soft prompt\",\n",
    "             \"description\" : \"What I trained it on and for how long, final loss, model, etcetera\" }\n",
    "\n",
    "sp = SoftPrompt.from_tuning_model(model, metadata)\n",
    "sp.to_file(\"soft_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Armitage, the man who'd been waiting for him to come out of his room. He was wearing a black suit and tie with white trim on it; he had an old-fashioned hat that looked like something from The Wizard Of Oz—a sorter version than what you might find in any other place: grayish brown hair tied back into ponytail over long sleeves (his eyes were wide open), blue jeans tucked under tight pants or dark boots underneath them as if they weren't there at all but\n"
     ]
    }
   ],
   "source": [
    "# Try generating with your model\n",
    "model.eval()\n",
    "\n",
    "call = tokenizer(\"Armitage\", return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    temperature=0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  }
 ]
}