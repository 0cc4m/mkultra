{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "source": [
    "# Tuning on Datasets\n",
    "This sheet is adapted from the language modeling example at\n",
    "https://github.com/huggingface/transformers/tree/master/examples/pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "n_tokens = 100\n",
    "\n",
    "model.initialize_soft_prompt(n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-812ce7cd67147d37\n",
      "Reusing dataset text (C:\\Users\\STARSTRUCK\\.cache\\huggingface\\datasets\\text\\default-812ce7cd67147d37\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"datasets/neuromancer_reformatted.txt\"\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": path, \"validation\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 20.40ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 29.41ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1], 'input_ids': [1525, 3977, 20400]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "6, 607, 314, 1101, 287, 290, 4769, 553, 8913, 531, 13, 44, 5098, 2540, 284, 42102, 866, 262, 20749, 13, 1649, 673, 27846, 736, 11, 1752, 11, 8913, 2497, 262, 1067, 388, 10137, 5920, 286, 1115, 24956, 14, 7934, 2324, 10942, 13, 1881, 286, 606, 3947, 284, 423, 645, 2951, 13, 1, 45803, 20155, 290, 27466, 423, 15283, 262, 2323, 4314, 11, 5181, 10584, 13, 19434, 321, 31853, 2367, 13, 35068, 338, 1972, 35075, 526, 1, 35700, 35075, 866, 994, 553, 673, 531, 11, 25635, 5223, 832, 257, 5166, 286, 12768, 7771, 8215, 13, 366, 23379, 612, 11, 49160, 526, 20448, 26157, 656, 262, 17593, 290, 5954, 262, 4161, 8906, 422, 465, 22645, 13, 679, 373, 288, 23437, 351, 15488, 13, 679, 21122, 465, 22645, 351, 257, 24808, 11, 1718, 257, 2068, 31145, 286, 1660, 422, 262, 17026, 9294, 13970, 262, 30789, 8130, 11, 290, 10667, 262, 3975, 286, 262, 5888, 9066, 319, 262, 3159, 13, 317, 22271, 278, 2266, 23493, 47092, 832, 262, 19001, 286, 257, 32439, 13, 5514, 3939, 31551, 422, 262, 4077, 16605, 326, 8203, 262, 4067, 286, 262, 360, 39291, 21939, 1370, 338, 5678, 13, 679, 14028, 644, 340, 373, 1804, 284, 607, 1232, 11, 284, 2513, 319, 340, 326, 835, 13, 2080, 1576, 886, 13425, 259, 15075, 11, 673, 714, 2513, 319, 257, 5166, 286, 15222, 336, 8142, 13, 679, 37257, 262, 36104, 19356, 326, 2714, 683, 287, 262, 5118, 290, 6928, 262, 4161, 8906, 13, 49, 28399, 783, 25, 4161, 8906, 11, 14509, 11, 290, 14283, 13, 464, 24956, 14, 7934, 2267, 5888, 373, 257, 2636, 6143, 1989, 26, 262, 5696, 8574, 994, 550, 284, 307, 10170, 4615, 878, 484, 714, 307, 9556, 2286, 13, 30236, 32724, 9342, 1022, 15274, 286, 10411, 12768, 5793, 364, 13, 1, 24446, 607, 1936, 517, 290, 3478, 284, 607, 1364, 11, 2806, 375, 553, 8913, 531, 13, 1, 20029, 517, 290, 3478, 1364, 11, 5181, 10584, 553, 262, 2792, 582, 531, 13, 3347, 1718, 262, 1364, 13, 317, 2330, 12, 24903, 300, 35808, 9875, 1068, 1022, 734, 5793, 364, 11, 607, 25839, 9583, 11, 2951, 9178, 13, 30236, 9514, 607, 13, 8913, 14028, 644, 262, 12495, 82, 550, 1760, 284, 29791, 326, 1241], 'labels': [656, 262, 16558, 11, 20493, 4171, 25988, 22563, 2029, 683, 3491, 1203, 290, 7209, 355, 21682, 276, 5405, 11, 339, 13973, 257, 850, 23065, 326, 43495, 1728, 29858, 287, 262, 4755, 33702, 498, 9729, 13, 7975, 783, 13, 797, 690, 278, 21461, 11, 262, 9471, 302, 15418, 2535, 262, 9664, 286, 262, 4324, 13, 45677, 13, 818, 262, 24956, 14, 7934, 10866, 11, 734, 30364, 12495, 82, 3332, 7995, 306, 2157, 257, 1877, 36954, 1410, 353, 11, 256, 9269, 262, 16352, 351, 257, 2008, 4676, 13, 1119, 1111, 12408, 442, 480, 38970, 14803, 13, 366, 45803, 20155, 389, 40170, 19828, 31853, 2367, 783, 553, 530, 4367, 11, 5486, 329, 262, 4414, 286, 465, 13589, 285, 522, 13, 366, 35230, 2340, 389, 991, 2111, 284, 1956, 511, 2243, 353, 526, 20448, 2277, 262, 985, 42003, 5078, 13, 843, 26157, 656, 262, 35358, 286, 5445, 9970, 13, 30236, 373, 865, 2286, 1028, 262, 9178, 12768, 3355, 286, 257, 890, 20749, 11, 607, 8033, 2406, 374, 14655, 290, 30690, 13, 8913, 373, 736, 287, 262, 17593, 11101, 11, 257, 2330, 12, 8940, 1627, 286, 2356, 31465, 287, 465, 1364, 19341, 13, 1, 2061, 338, 5836, 11, 2806, 375, 1701, 339, 1965, 262, 2792, 582, 13, 1, 40, 50109, 11, 49160, 13, 10584, 338, 407, 3375, 13, 16314, 526, 20448, 338, 1430, 373, 16259, 13, 317, 2060, 4190, 12, 38125, 4704, 286, 46153, 25988, 7083, 422, 262, 3641, 286, 262, 15032, 4324, 284, 262, 15852, 19001, 286, 465, 4771, 25766, 13, 679, 1422, 470, 423, 640, 284, 4043, 13, 20879, 257, 2769, 8033, 11, 339, 26157, 757, 13, 44, 5098, 1718, 257, 2060, 2239, 11, 2111, 284, 1104, 607, 3463, 319, 262, 20749, 3355, 13, 554, 262, 42186, 11, 8913, 7128, 22739, 13, 383, 1218, 2239, 1718, 607, 625, 281, 503, 49729, 3211, 13, 35712, 23179, 6016, 351, 4713, 2910, 13, 402, 2475, 7752, 286, 257, 23273, 13608, 20721, 6380, 301, 1015, 13, 2332, 5761, 3947, 284, 423, 33214, 284, 257, 13275, 13, 2080, 262, 2368, 2239, 11, 8913, 25421, 290, 1043, 2241, 736, 287, 262, 17593, 13, 1, 15783, 375, 30, 6182, 11, 5156, 13, 764, 22135, 2332, 3809, 5381, 351, 2356, 13, 1375, 2284, 70, 704, 13, 366, 22253, 1917, 351, 262, 30962, 13, 11382, 530, 286, 606, 6265, 616, 1232, 526, 1, 2061, 345, 761, 783, 11, 5181, 10584, 1701, 383, 2792, 582, 338, 3809, 373, 37665, 4612, 11, 3016, 2626, 2157, 9037, 13, 20448, 4137, 2241, 284, 14283, 736, 13, 1375, 373, 21804, 1028, 262, 3355, 11, 2263, 477, 286, 607, 3463, 319, 607, 826, 1232, 13, 1375, 277, 11137, 832, 262, 10154, 286, 262, 6050, 338, 479, 648, 38049, 10000, 290, 25518, 257, 9629, 286, 7309, 941, 9395, 351, 257, 27223, 286, 4587, 9937, 36730, 13, 1375, 6163, 1115, 290, 15683, 276, 606, 1327, 1028, 607, 1364, 15980, 11, 625, 262, 32375, 13, 9699, 7319, 4580, 4546, 82, 286, 886, 13425, 259, 15075, 1625, 866, 319, 262, 2356, 588, 257, 15554, 11, 48883, 340, 13, 2332, 736, 610, 1740, 3063, 5753, 2280, 13, 14657, 9813, 286, 23125, 300, 6320, 510, 607, 30389, 13, 1375, 21893, 290, 6364, 18397, 13, 1, 16454, 11, 2806, 375, 13, 16805, 783, 13, 887, 314, 1183, 761, 257, 3315, 1074, 618, 314, 1282, 503, 13, 14026, 616, 661, 13, 49160, 11, 314, 1101, 734, 2431, 422, 2496, 13, 1680, 345, 1745, 1701, 1, 24446, 607, 314, 1101, 287, 290, 4769, 553, 8913, 531, 13, 44, 5098, 2540, 284, 42102, 866, 262, 20749, 13, 1649, 673, 27846, 736, 11, 1752, 11, 8913, 2497, 262, 1067, 388, 10137, 5920, 286, 1115, 24956, 14, 7934, 2324, 10942, 13, 1881, 286, 606, 3947, 284, 423, 645, 2951, 13, 1, 45803, 20155, 290, 27466, 423, 15283, 262, 2323, 4314, 11, 5181, 10584, 13, 19434, 321, 31853, 2367, 13, 35068, 338, 1972, 35075, 526, 1, 35700, 35075, 866, 994, 553, 673, 531, 11, 25635, 5223, 832, 257, 5166, 286, 12768, 7771, 8215, 13, 366, 23379, 612, 11, 49160, 526, 20448, 26157, 656, 262, 17593, 290, 5954, 262, 4161, 8906, 422, 465, 22645, 13, 679, 373, 288, 23437, 351, 15488, 13, 679, 21122, 465, 22645, 351, 257, 24808, 11, 1718, 257, 2068, 31145, 286, 1660, 422, 262, 17026, 9294, 13970, 262, 30789, 8130, 11, 290, 10667, 262, 3975, 286, 262, 5888, 9066, 319, 262, 3159, 13, 317, 22271, 278, 2266, 23493, 47092, 832, 262, 19001, 286, 257, 32439, 13, 5514, 3939, 31551, 422, 262, 4077, 16605, 326, 8203, 262, 4067, 286, 262, 360, 39291, 21939, 1370, 338, 5678, 13, 679, 14028, 644, 340, 373, 1804, 284, 607, 1232, 11, 284, 2513, 319, 340, 326, 835, 13, 2080, 1576, 886, 13425, 259, 15075, 11, 673, 714, 2513, 319, 257, 5166, 286, 15222, 336, 8142, 13, 679, 37257, 262, 36104, 19356, 326, 2714, 683, 287, 262, 5118, 290, 6928, 262, 4161, 8906, 13, 49, 28399, 783, 25, 4161, 8906, 11, 14509, 11, 290, 14283, 13, 464, 24956, 14, 7934, 2267, 5888, 373, 257, 2636, 6143, 1989, 26, 262, 5696, 8574, 994, 550, 284, 307, 10170, 4615, 878, 484, 714, 307, 9556, 2286, 13, 30236, 32724, 9342, 1022, 15274, 286, 10411, 12768, 5793, 364, 13, 1, 24446, 607, 1936, 517, 290, 3478, 284, 607, 1364, 11, 2806, 375, 553, 8913, 531, 13, 1, 20029, 517, 290, 3478, 1364, 11, 5181, 10584, 553, 262, 2792, 582, 531, 13, 3347, 1718, 262, 1364, 13, 317, 2330, 12, 24903, 300, 35808, 9875, 1068, 1022, 734, 5793, 364, 11, 607, 25839, 9583, 11, 2951, 9178, 13, 30236, 9514, 607, 13, 8913, 14028, 644, 262, 12495, 82, 550, 1760, 284, 29791, 326, 1241]}.\n",
      "Sample 61 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [13, 887, 326, 550, 2622, 257, 5719, 1336, 286, 7733, 290, 257, 39120, 4161, 2934, 14335, 13, 2061, 34686, 41976, 27947, 11, 345, 1392, 13, 8913, 14682, 465, 257, 10813, 1182, 290, 15246, 656, 262, 13546, 13, 1544, 714, 4724, 262, 886, 11, 262, 19523, 13, 1318, 373, 281, 37204, 40686, 25, 34686, 41976, 7584, 262, 4320, 15219, 1978, 11, 262, 4320, 15219, 2753, 683, 5475, 13, 2080, 883, 2832, 13, 7610, 18041, 41714, 262, 36371, 35938, 13, 7376, 364, 422, 262, 7072, 11, 28445, 13, 8913, 6204, 290, 4966, 465, 2832, 625, 465, 8242, 13, 679, 2900, 290, 6807, 736, 656, 262, 569, 278, 20259, 30, 1326, 15638, 30, 2375, 13, 44, 5098, 338, 5118, 373, 6565, 13, 383, 3800, 373, 36043, 13, 943, 2781, 496, 3332, 3436, 11, 991, 16143, 379, 262, 3800, 11, 262, 10717, 286, 262, 8237, 20721, 1022, 465, 9353, 13, 1, 8496, 318, 673, 1701, 8913, 1965, 13, 1, 38, 505, 553, 943, 2781, 496, 531, 13, 1, 3347, 467, 706, 683, 1701, 1, 2949, 526, 1318, 373, 257, 2705, 44569, 764, 220, 943, 2781, 496, 3114, 866, 379, 262, 5405, 13, 2399, 1364, 1021, 1625, 510, 4769, 262, 28287, 286, 5405, 351, 663, 3953, 286, 2266, 8237, 13, 383, 5445, 10717, 39701, 19289, 588, 257, 1017, 1428, 286, 4771, 13, 8913, 1718, 340, 422, 683, 290, 900, 340, 287, 257, 1660, 5405, 13, 1, 24446, 502, 810, 673, 1816, 11, 943, 2781, 496, 526, 464, 7588, 1625, 510, 13, 8913, 3114, 656, 262, 14005, 2951, 13, 10528, 612, 379, 477, 13, 366, 3347, 338, 3750, 284, 8335, 5223, 13, 921, 1839, 470, 766, 607, 757, 13, 921, 1183, 307, 1978, 1141, 262, 1057, 526, 1, 5195, 750, 34686, 41976, 466, 326, 284, 607, 1701, 3163, 2781, 496, 6204, 11, 22000, 262, 14779, 1424, 286, 465, 15224, 13, 366, 3855, 617, 3993, 11, 8913, 526, 1, 1135, 1057, 11, 9439, 1701, 3163, 2781, 496, 13541, 465, 24513, 8212, 290, 6807, 1497, 11, 3812, 262, 8420, 13, 20448, 31862, 465, 22645, 290, 3114, 1088, 262, 2119, 13, 383, 16278, 364, 547, 7396, 11, 1466, 16755, 355, 1450, 925, 14532, 13, 679, 6810, 262, 29780, 329, 262, 717, 640, 11, 32268, 991, 50104, 612, 287, 2839, 11854, 13, 679, 2982, 262, 537, 676, 286, 8465, 1574, 11, 38952, 5273, 13, 383, 32268, 9617, 15360, 16187, 319, 262, 13387, 13, 464, 2576, 338, 1986, 4120, 355, 25891, 355, 530, 286, 34686, 41976, 338, 19887, 11, 607, 1402, 2832, 319, 262, 23895, 4898, 286, 262, 3652, 436, 27585, 26, 673, 23831, 2651, 11, 1986, 38404, 11, 340, 3947, 284, 683, 11, 607, 3223, 2951, 6824, 319, 1223, 3675, 13, 383, 3800, 13, 632, 373, 257, 8871, 1986, 11, 475, 407, 4950, 13, 7563, 21413, 11, 262, 19353, 35095, 1029, 1865, 29381, 21049, 12, 11534, 11, 5422, 3094, 290, 4081, 11, 12974, 31414, 416, 257, 7135, 11, 1196, 666, 9686, 351, 781, 1723, 18216, 41408, 13, 843, 788, 673, 373, 3750, 11, 736, 656, 2839, 20263, 290, 262, 9280, 286, 32268, 13, 1722, 339, 1364, 262, 7072, 11, 339, 6810, 262, 734, 1862, 4141, 3653, 290, 511, 11077, 11, 508, 547, 4953, 329, 262, 8848, 284, 262, 1290, 15191, 290, 262, 16936, 21507, 13, 14574, 2119, 373, 10574, 11, 262, 4124, 6513, 321, 7209, 355, 617, 10481, 706, 257, 46163, 20013, 13, 2332, 6131, 373, 3750, 13, 679, 3114, 329, 257, 3465, 13, 1318, 373, 2147, 13, 12168, 4201, 3804, 878, 262, 3715, 3675, 262, 4324, 6823, 832, 465, 12097, 290, 14274, 42661, 13, 679, 3114, 510, 290, 2497, 257, 1570, 286, 2935, 1304, 1045, 11, 5789, 12437, 25, 1962, 35764, 11, 23459, 88, 25496, 11, 41167, 11, 14734, 13, 1544, 18484, 11, 788, 14682, 465, 1182, 290, 12606, 284, 257, 6103, 339, 8020, 470, 20466, 17247, 13, 679, 2900, 262, 31912, 859, 572, 290, 373, 20945, 351, 262, 48093, 326, 8812, 2286, 262, 1290, 22638, 13, 1544, 6497, 510, 262, 3072, 290, 5281, 340, 503, 284, 262, 3608, 29780, 13, 1, 3855, 502, 257, 1271, 329, 262, 17068, 7164, 3304, 42911, 339, 1297, 262, 6915, 13, 366, 1026, 338, 257, 27762, 11, 6823, 503, 286, 16899, 13946, 526, 464, 11594, 3809, 664, 863, 257, 3478, 12, 27003, 1271, 13, 366, 22788, 553, 340, 2087, 11, 366, 1169, 9352, 287, 1808, 318, 5961, 10546, 666, 526, 44, 3010, 36340, 9373, 319, 262, 8150, 8216, 13, 366, 38101, 1701, 1, 20448, 13, 921, 1392, 257, 38053, 11, 337, 3010, 36340, 1701, 1, 38101, 13, 1550, 294, 6, 16408, 552, 11, 21349, 760, 526, 1, 6090, 345, 651, 340, 572, 329, 502, 11, 582, 30, 5930, 340, 319, 616, 30789, 8130, 13, 3244, 1210, 616, 6203, 319, 13, 632, 338, 262, 941, 351, 262, 5755, 3212, 319, 340, 526, 1, 2437, 345, 466, 259, 6, 287, 612, 11, 937, 1701, 1, 5779, 11, 314, 761, 617, 1037, 526, 1, 44, 709, 259, 3256, 937, 13, 314, 651, 294, 6, 38053, 526, 20448, 16399, 284, 18107, 9037, 981, 337, 3010, 36340, 7223, 262, 2829, 3072, 2792, 13, 366, 23709, 428, 553, 339, 1297, 262, 30789, 8130, 11, 618, 339, 2982, 340, 307, 538, 13, 1, 1639, 389, 5486, 422, 257, 7272, 20738, 4067, 553, 262, 3644, 13030, 2684, 306, 13, 1, 34094, 340, 553, 339, 531, 13, 366, 1890, 1136, 262, 4771, 13, 1400, 4771, 13, 8798, 262, 5678, 13, 360, 39291, 1701, 1, 10814, 11, 8913, 526, 383, 21939, 1370, 5158, 832, 262, 30789, 8130, 338, 3809, 11594, 11, 262, 7773, 23371, 18702, 2626, 5000, 13, 1, 35, 844, 11, 345, 821, 546, 284, 10862], 'labels': [13, 887, 326, 550, 2622, 257, 5719, 1336, 286, 7733, 290, 257, 39120, 4161, 2934, 14335, 13, 2061, 34686, 41976, 27947, 11, 345, 1392, 13, 8913, 14682, 465, 257, 10813, 1182, 290, 15246, 656, 262, 13546, 13, 1544, 714, 4724, 262, 886, 11, 262, 19523, 13, 1318, 373, 281, 37204, 40686, 25, 34686, 41976, 7584, 262, 4320, 15219, 1978, 11, 262, 4320, 15219, 2753, 683, 5475, 13, 2080, 883, 2832, 13, 7610, 18041, 41714, 262, 36371, 35938, 13, 7376, 364, 422, 262, 7072, 11, 28445, 13, 8913, 6204, 290, 4966, 465, 2832, 625, 465, 8242, 13, 679, 2900, 290, 6807, 736, 656, 262, 569, 278, 20259, 30, 1326, 15638, 30, 2375, 13, 44, 5098, 338, 5118, 373, 6565, 13, 383, 3800, 373, 36043, 13, 943, 2781, 496, 3332, 3436, 11, 991, 16143, 379, 262, 3800, 11, 262, 10717, 286, 262, 8237, 20721, 1022, 465, 9353, 13, 1, 8496, 318, 673, 1701, 8913, 1965, 13, 1, 38, 505, 553, 943, 2781, 496, 531, 13, 1, 3347, 467, 706, 683, 1701, 1, 2949, 526, 1318, 373, 257, 2705, 44569, 764, 220, 943, 2781, 496, 3114, 866, 379, 262, 5405, 13, 2399, 1364, 1021, 1625, 510, 4769, 262, 28287, 286, 5405, 351, 663, 3953, 286, 2266, 8237, 13, 383, 5445, 10717, 39701, 19289, 588, 257, 1017, 1428, 286, 4771, 13, 8913, 1718, 340, 422, 683, 290, 900, 340, 287, 257, 1660, 5405, 13, 1, 24446, 502, 810, 673, 1816, 11, 943, 2781, 496, 526, 464, 7588, 1625, 510, 13, 8913, 3114, 656, 262, 14005, 2951, 13, 10528, 612, 379, 477, 13, 366, 3347, 338, 3750, 284, 8335, 5223, 13, 921, 1839, 470, 766, 607, 757, 13, 921, 1183, 307, 1978, 1141, 262, 1057, 526, 1, 5195, 750, 34686, 41976, 466, 326, 284, 607, 1701, 3163, 2781, 496, 6204, 11, 22000, 262, 14779, 1424, 286, 465, 15224, 13, 366, 3855, 617, 3993, 11, 8913, 526, 1, 1135, 1057, 11, 9439, 1701, 3163, 2781, 496, 13541, 465, 24513, 8212, 290, 6807, 1497, 11, 3812, 262, 8420, 13, 20448, 31862, 465, 22645, 290, 3114, 1088, 262, 2119, 13, 383, 16278, 364, 547, 7396, 11, 1466, 16755, 355, 1450, 925, 14532, 13, 679, 6810, 262, 29780, 329, 262, 717, 640, 11, 32268, 991, 50104, 612, 287, 2839, 11854, 13, 679, 2982, 262, 537, 676, 286, 8465, 1574, 11, 38952, 5273, 13, 383, 32268, 9617, 15360, 16187, 319, 262, 13387, 13, 464, 2576, 338, 1986, 4120, 355, 25891, 355, 530, 286, 34686, 41976, 338, 19887, 11, 607, 1402, 2832, 319, 262, 23895, 4898, 286, 262, 3652, 436, 27585, 26, 673, 23831, 2651, 11, 1986, 38404, 11, 340, 3947, 284, 683, 11, 607, 3223, 2951, 6824, 319, 1223, 3675, 13, 383, 3800, 13, 632, 373, 257, 8871, 1986, 11, 475, 407, 4950, 13, 7563, 21413, 11, 262, 19353, 35095, 1029, 1865, 29381, 21049, 12, 11534, 11, 5422, 3094, 290, 4081, 11, 12974, 31414, 416, 257, 7135, 11, 1196, 666, 9686, 351, 781, 1723, 18216, 41408, 13, 843, 788, 673, 373, 3750, 11, 736, 656, 2839, 20263, 290, 262, 9280, 286, 32268, 13, 1722, 339, 1364, 262, 7072, 11, 339, 6810, 262, 734, 1862, 4141, 3653, 290, 511, 11077, 11, 508, 547, 4953, 329, 262, 8848, 284, 262, 1290, 15191, 290, 262, 16936, 21507, 13, 14574, 2119, 373, 10574, 11, 262, 4124, 6513, 321, 7209, 355, 617, 10481, 706, 257, 46163, 20013, 13, 2332, 6131, 373, 3750, 13, 679, 3114, 329, 257, 3465, 13, 1318, 373, 2147, 13, 12168, 4201, 3804, 878, 262, 3715, 3675, 262, 4324, 6823, 832, 465, 12097, 290, 14274, 42661, 13, 679, 3114, 510, 290, 2497, 257, 1570, 286, 2935, 1304, 1045, 11, 5789, 12437, 25, 1962, 35764, 11, 23459, 88, 25496, 11, 41167, 11, 14734, 13, 1544, 18484, 11, 788, 14682, 465, 1182, 290, 12606, 284, 257, 6103, 339, 8020, 470, 20466, 17247, 13, 679, 2900, 262, 31912, 859, 572, 290, 373, 20945, 351, 262, 48093, 326, 8812, 2286, 262, 1290, 22638, 13, 1544, 6497, 510, 262, 3072, 290, 5281, 340, 503, 284, 262, 3608, 29780, 13, 1, 3855, 502, 257, 1271, 329, 262, 17068, 7164, 3304, 42911, 339, 1297, 262, 6915, 13, 366, 1026, 338, 257, 27762, 11, 6823, 503, 286, 16899, 13946, 526, 464, 11594, 3809, 664, 863, 257, 3478, 12, 27003, 1271, 13, 366, 22788, 553, 340, 2087, 11, 366, 1169, 9352, 287, 1808, 318, 5961, 10546, 666, 526, 44, 3010, 36340, 9373, 319, 262, 8150, 8216, 13, 366, 38101, 1701, 1, 20448, 13, 921, 1392, 257, 38053, 11, 337, 3010, 36340, 1701, 1, 38101, 13, 1550, 294, 6, 16408, 552, 11, 21349, 760, 526, 1, 6090, 345, 651, 340, 572, 329, 502, 11, 582, 30, 5930, 340, 319, 616, 30789, 8130, 13, 3244, 1210, 616, 6203, 319, 13, 632, 338, 262, 941, 351, 262, 5755, 3212, 319, 340, 526, 1, 2437, 345, 466, 259, 6, 287, 612, 11, 937, 1701, 1, 5779, 11, 314, 761, 617, 1037, 526, 1, 44, 709, 259, 3256, 937, 13, 314, 651, 294, 6, 38053, 526, 20448, 16399, 284, 18107, 9037, 981, 337, 3010, 36340, 7223, 262, 2829, 3072, 2792, 13, 366, 23709, 428, 553, 339, 1297, 262, 30789, 8130, 11, 618, 339, 2982, 340, 307, 538, 13, 1, 1639, 389, 5486, 422, 257, 7272, 20738, 4067, 553, 262, 3644, 13030, 2684, 306, 13, 1, 34094, 340, 553, 339, 531, 13, 366, 1890, 1136, 262, 4771, 13, 1400, 4771, 13, 8798, 262, 5678, 13, 360, 39291, 1701, 1, 10814, 11, 8913, 526, 383, 21939, 1370, 5158, 832, 262, 30789, 8130, 338, 3809, 11594, 11, 262, 7773, 23371, 18702, 2626, 5000, 13, 1, 35, 844, 11, 345, 821, 546, 284, 10862]}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group texts into blocks\n",
    "# WARNING: Be sure to subtract the size of the soft prompt!\n",
    "block_size = 1024 - n_tokens\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from mkultra.trainers import SoftPromptTrainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "optimizer = Adafactor([model.get_soft_params()], scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)\n",
    "scheduler = transformers.get_constant_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=1000)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    optimizers = (optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1588\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1590\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\models\\tuning.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mGPT2PromptTuningLM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGPTPromptTuningMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}