{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "source": [
    "# Tuning on Datasets\n",
    "This sheet is adapted from the language modeling example at\n",
    "https://github.com/huggingface/transformers/tree/master/examples/pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "n_tokens = 100\n",
    "\n",
    "model.initialize_soft_prompt(n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-812ce7cd67147d37\n",
      "Reusing dataset text (C:\\Users\\STARSTRUCK\\.cache\\huggingface\\datasets\\text\\default-812ce7cd67147d37\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"datasets/neuromancer_reformatted.txt\"\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": path, \"validation\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31.24ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 30.30ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1], 'input_ids': [1525, 3977, 20400]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 503, 13, 8913, 20821, 262, 2266, 311, 1092, 78, 6050, 284, 257, 6143, 25755, 735, 290, 5954, 2241, 866, 284, 262, 308, 12, 12384, 13, 1, 6214, 644, 294, 6, 10905, 910, 11, 937, 553, 337, 3010, 36340, 531, 13, 366, 34556, 7622, 1265, 259, 6, 329, 345, 526, 1, 2396, 508, 338, 510, 612, 287, 326, 1517, 1701, 1, 30556, 2869, 12, 7081, 1625, 878, 13, 1052, 6, 783, 339, 5399, 416, 345, 43438, 943, 2781, 496, 11, 1282, 503, 4848, 274, 485, 35713, 20448, 1234, 262, 4161, 8906, 319, 290, 474, 6021, 287, 13, 1, 35, 39291, 1701, 464, 17593, 3751, 683, 262, 11398, 34126, 286, 262, 7771, 12082, 287, 32615, 74, 320, 13, 1, 2061, 345, 651, 43701, 6, 510, 284, 11, 2933, 30, 314, 587, 3285, 259, 6, 20605, 312, 3923, 13, 30789, 8130, 338, 39378, 656, 257, 15203, 3331, 319, 534, 6478, 338, 8848, 783, 13, 16123, 8169, 381, 259, 4458, 921, 2834, 617, 39141, 4894, 1701, 1, 10995, 11, 475, 10633, 76, 1133, 2923, 705, 368, 526, 1, 5779, 11, 326, 1839, 470, 1745, 705, 368, 890, 13, 43257, 517, 810, 883, 1625, 422, 13, 1355, 510, 994, 287, 2700, 13, 5147, 511, 13136, 389, 477, 625, 428, 10706, 6567, 588, 17607, 319, 7510, 13, 843, 534, 6478, 11, 8913, 11, 339, 1139, 467, 13, 679, 1139, 1057, 340, 290, 1057, 340, 783, 526, 20448, 25436, 329, 262, 4848, 274, 485, 22715, 13, 1, 43, 368, 1326, 1011, 326, 257, 792, 11, 8913, 35713, 383, 17593, 38258, 290, 41716, 355, 262, 21939, 1370, 10945, 281, 28746, 2168, 286, 18045, 351, 257, 2866, 290, 9922, 326, 925, 8913, 266, 924, 351, 31651, 13, 1, 2484, 270, 11, 360, 39291, 35713, 1, 10814, 11, 2933, 11, 314, 373, 326, 922, 618, 314, 373, 6776, 13, 921, 18959, 470, 1775, 407, 20079, 4458, 1400, 2832, 2474, 1, 2504, 338, 340, 11, 24926, 30, 4403, 4077, 35991, 572, 1364, 1701, 1, 1639, 1392, 340, 13, 26040, 4755, 1366, 329, 39412, 959, 12, 26754, 7742, 311, 13, 32, 1539, 290, 326, 4771, 318, 7560, 416, 511, 734, 8030, 9552, 338, 13, 1550, 1582, 351, 1997, 287, 262, 2422, 6567, 11, 3073, 284, 502, 13, 1320, 338, 5822, 5968, 4771, 11, 8913, 11, 2042, 355, 262, 12296, 290, 29395, 355, 5405, 13, 25305, 534, 3632, 2582, 355, 804, 379, 345, 13, 775, 651, 597, 5699, 783, 11, 340, 1183, 423, 491, 49908, 510, 674, 840, 290, 503, 1111, 11368, 11, 307, 1560, 259], 'labels': [1973, 262, 2330, 19867, 13, 38, 16995, 3214, 1497, 355, 262, 599, 42343, 33214, 13, 8913, 338, 11384, 28488, 276, 13, 32, 263, 349, 373, 4953, 351, 465, 629, 25141, 13970, 262, 23423, 13, 1, 20448, 11, 937, 11, 1263, 1917, 526, 383, 2705, 3809, 18107, 287, 465, 9512, 13, 679, 22531, 2817, 262, 6115, 1630, 290, 613, 1068, 656, 262, 17210, 272, 1986, 12, 6816, 286, 15781, 349, 338, 14335, 13, 1, 38, 12375, 651, 284, 7164, 3304, 837, 15781, 349, 526, 1, 38101, 13, 520, 2416, 287, 11, 937, 13, 887, 7164, 3304, 220, 25798, 13, 575, 19725, 11, 1625, 878, 11, 673, 1625, 736, 13, 2735, 673, 5793, 259, 6, 11831, 319, 17068, 7164, 3304, 764, 366, 51, 870, 30, 366, 34, 480, 878, 1701, 8913, 19952, 656, 262, 629, 25141, 338, 5739, 290, 2540, 284, 3049, 268, 262, 29552, 13, 1, 16504, 43207, 13, 347, 2909, 345, 5301, 35713, 3163, 2781, 496, 13, 18546, 1484, 4263, 286, 373, 862, 290, 26120, 8278, 287, 8913, 338, 2000, 355, 484, 1625, 287, 6504, 286, 17068, 7164, 3304, 13, 220, 383, 1310, 27762, 373, 47032, 1028, 262, 12768, 41899, 897, 286, 257, 33143, 11, 12833, 576, 4074, 1936, 1661, 607, 4129, 13, 383, 5101, 286, 21338, 829, 6204, 503, 1028, 7164, 3304, 220, 705, 82, 39378, 23644, 351, 262, 6283, 16287, 286, 17076, 290, 8246, 19606, 13, 317, 14005, 1162, 2143, 515, 7706, 1014, 26929, 503, 286, 262, 43207, 11, 3013, 4335, 35303, 284, 3368, 262, 27762, 338, 11874, 11, 290, 5017, 262, 46088, 25834, 13, 1318, 373, 1223, 33390, 546, 262, 13888, 11, 475, 340, 550, 517, 284, 466, 351, 4213, 286, 13017, 621, 286, 1714, 13, 1, 2061, 338, 5836, 351, 337, 3010, 36340, 1701, 1, 44, 3010, 36340, 3734, 13, 15658, 1282, 866, 262, 12403, 13, 575, 19725, 8022, 1561, 284, 683, 11, 910, 8960, 526, 1722, 484, 28507, 1613, 262, 12768, 4074, 11, 8913, 2497, 262, 1438, 367, 43664, 15543, 287, 23453, 2330, 44590, 11061, 281, 909, 6511, 13946, 286, 4960, 13, 1, 40, 836, 470, 588, 428, 11, 582, 13, 314, 373, 3612, 3863, 340, 338, 640, 356, 1392, 674, 840, 503, 286, 994, 6949, 526, 1, 44, 3010, 36340, 892, 259, 6, 326, 7141, 1517, 11, 937, 11, 475, 7164, 3304, 220, 407, 307, 467, 259, 6, 1290, 588, 326, 526, 44, 3010, 36340, 373, 1308, 1806, 257, 2866, 276, 12, 929, 1458, 10924, 284, 465, 5243, 618, 8913, 1625, 832, 262, 2651, 5793, 290, 4615, 465, 14335, 13, 1, 32, 263, 349, 338, 3750, 736, 284, 262, 4631, 263, 42911, 8913, 531, 13, 44, 3010, 36340, 14464, 11, 991, 48508, 284, 262, 21822, 13, 20448, 5954, 2241, 625, 262, 8022, 338, 38193, 256, 9248, 286, 15157, 28860, 290, 2540, 284, 4781, 465, 6050, 13, 337, 3010, 36340, 338, 2951, 547, 4838, 783, 26, 339, 14464, 355, 339, 16399, 284, 617, 10971, 625, 257, 5166, 286, 9512, 351, 6016, 10912, 21226, 11, 465, 4772, 1126, 839, 351, 10368, 13, 679, 12408, 374, 14655, 21029, 290, 281, 1468, 4077, 36104, 15224, 351, 262, 27409, 19551, 503, 13, 8913, 20821, 262, 2266, 311, 1092, 78, 6050, 284, 257, 6143, 25755, 735, 290, 5954, 2241, 866, 284, 262, 308, 12, 12384, 13, 1, 6214, 644, 294, 6, 10905, 910, 11, 937, 553, 337, 3010, 36340, 531, 13, 366, 34556, 7622, 1265, 259, 6, 329, 345, 526, 1, 2396, 508, 338, 510, 612, 287, 326, 1517, 1701, 1, 30556, 2869, 12, 7081, 1625, 878, 13, 1052, 6, 783, 339, 5399, 416, 345, 43438, 943, 2781, 496, 11, 1282, 503, 4848, 274, 485, 35713, 20448, 1234, 262, 4161, 8906, 319, 290, 474, 6021, 287, 13, 1, 35, 39291, 1701, 464, 17593, 3751, 683, 262, 11398, 34126, 286, 262, 7771, 12082, 287, 32615, 74, 320, 13, 1, 2061, 345, 651, 43701, 6, 510, 284, 11, 2933, 30, 314, 587, 3285, 259, 6, 20605, 312, 3923, 13, 30789, 8130, 338, 39378, 656, 257, 15203, 3331, 319, 534, 6478, 338, 8848, 783, 13, 16123, 8169, 381, 259, 4458, 921, 2834, 617, 39141, 4894, 1701, 1, 10995, 11, 475, 10633, 76, 1133, 2923, 705, 368, 526, 1, 5779, 11, 326, 1839, 470, 1745, 705, 368, 890, 13, 43257, 517, 810, 883, 1625, 422, 13, 1355, 510, 994, 287, 2700, 13, 5147, 511, 13136, 389, 477, 625, 428, 10706, 6567, 588, 17607, 319, 7510, 13, 843, 534, 6478, 11, 8913, 11, 339, 1139, 467, 13, 679, 1139, 1057, 340, 290, 1057, 340, 783, 526, 20448, 25436, 329, 262, 4848, 274, 485, 22715, 13, 1, 43, 368, 1326, 1011, 326, 257, 792, 11, 8913, 35713, 383, 17593, 38258, 290, 41716, 355, 262, 21939, 1370, 10945, 281, 28746, 2168, 286, 18045, 351, 257, 2866, 290, 9922, 326, 925, 8913, 266, 924, 351, 31651, 13, 1, 2484, 270, 11, 360, 39291, 35713, 1, 10814, 11, 2933, 11, 314, 373, 326, 922, 618, 314, 373, 6776, 13, 921, 18959, 470, 1775, 407, 20079, 4458, 1400, 2832, 2474, 1, 2504, 338, 340, 11, 24926, 30, 4403, 4077, 35991, 572, 1364, 1701, 1, 1639, 1392, 340, 13, 26040, 4755, 1366, 329, 39412, 959, 12, 26754, 7742, 311, 13, 32, 1539, 290, 326, 4771, 318, 7560, 416, 511, 734, 8030, 9552, 338, 13, 1550, 1582, 351, 1997, 287, 262, 2422, 6567, 11, 3073, 284, 502, 13, 1320, 338, 5822, 5968, 4771, 11, 8913, 11, 2042, 355, 262, 12296, 290, 29395, 355, 5405, 13, 25305, 534, 3632, 2582, 355, 804, 379, 345, 13, 775, 651, 597, 5699, 783, 11, 340, 1183, 423, 491, 49908, 510, 674, 840, 290, 503, 1111, 11368, 11, 307, 1560, 259]}.\n",
      "Sample 38 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [11914, 9859, 736, 13, 366, 3673, 1576, 11, 6029, 41495, 526, 679, 10158, 607, 257, 14091, 1994, 351, 257, 2835, 7872, 7621, 13, 366, 1639, 821, 6823, 1541, 13, 8835, 6679, 338, 26148, 526, 679, 3114, 1088, 13, 366, 1212, 3240, 22523, 526, 1, 1639, 651, 556, 273, 6570, 20803, 11, 484, 1011, 345, 503, 422, 739, 257, 29500, 13, 2329, 16614, 340, 338, 12232, 393, 1223, 526, 1375, 665, 49376, 262, 1994, 1088, 257, 7660, 13, 366, 1639, 994, 355, 1188, 316, 393, 644, 1701, 1, 40, 17753, 2198, 503, 617, 3516, 338, 35223, 553, 262, 15368, 531, 13, 1, 2437, 546, 616, 6203, 1701, 8913, 1965, 13, 464, 15368, 1592, 771, 13, 366, 31310, 3760, 262, 8435, 13, 16981, 262, 6478, 526, 44, 5098, 338, 9353, 3888, 287, 262, 9082, 286, 607, 15224, 11, 257, 781, 15799, 286, 474, 425, 13, 383, 15368, 7342, 11, 788, 14464, 13, 1, 10995, 553, 673, 531, 11, 366, 40, 760, 508, 326, 318, 526, 1375, 13665, 9091, 607, 1182, 287, 262, 4571, 286, 262, 7662, 2024, 13, 366, 16773, 319, 11, 42966, 526, 8913, 3940, 607, 351, 1111, 11668, 13, 14574, 2119, 1244, 423, 587, 262, 530, 287, 609, 23718, 810, 339, 1549, 717, 1775, 943, 2781, 496, 13, 679, 1816, 284, 262, 4324, 11, 287, 262, 3329, 11, 2048, 12451, 284, 766, 11790, 4696, 13, 1318, 373, 1194, 7541, 1973, 262, 4675, 13, 632, 373, 991, 43079, 13, 317, 1178, 3850, 12, 34422, 550, 2077, 5473, 287, 3420, 1322, 11, 511, 1468, 3809, 1050, 20193, 12908, 287, 15747, 286, 1598, 7309, 11, 2370, 326, 262, 3194, 1573, 991, 8359, 257, 1728, 29063, 994, 13, 632, 373, 257, 40186, 1499, 13, 679, 7342, 257, 19222, 2042, 15792, 305, 268, 38988, 11, 257, 20049, 17669, 12, 3846, 11315, 11, 355, 340, 595, 7053, 2004, 1936, 424, 297, 268, 12, 11534, 9663, 3790, 287, 7440, 10137, 4077, 22551, 13, 1119, 5982, 262, 7541, 1973, 262, 4675, 13, 1544, 27846, 736, 379, 262, 3996, 11, 379, 30236, 11, 290, 607, 6340, 9449, 7425, 683, 13, 1375, 1549, 1364, 262, 12314, 1773, 382, 3350, 319, 262, 3996, 6649, 397, 287, 511, 42186, 11, 13970, 262, 1007, 1082, 7617, 9318, 2189, 13, 2332, 15232, 12548, 636, 286, 262, 2119, 338, 1657, 29220, 13, 1544, 550, 262, 3072, 287, 465, 1021, 878, 340, 550, 257, 2863, 284, 5858, 5403, 13, 366, 38, 9435, 345, 821, 510, 553, 943, 2781, 496, 531, 13, 1, 40, 1101, 655, 13, 11182, 338, 991, 739, 13, 20600, 11, 6478, 11, 314, 892, 340, 338, 3863, 640, 356, 423, 257, 1310, 1561, 13, 314, 892, 314, 670, 1365, 611, 314, 760, 257, 1310, 517, 546, 644, 314, 1101, 1804, 526, 15086, 594, 319, 262, 1627, 13, 8913, 1643, 465, 10645, 13, 1, 1639, 760, 355, 881, 355, 345, 761, 284, 13, 6674, 517, 526, 1, 1639, 892, 523, 1701, 1, 3855, 12049, 11, 8913, 13, 3497, 607, 510, 13, 921, 1183, 423, 257, 24955, 287, 546, 17280, 2431, 13, 2399, 1438, 318, 3813, 89, 571, 1077, 73, 666, 526, 383, 3072, 7245, 515, 26625, 13, 943, 2781, 496, 373, 3750, 13, 1, 54, 539, 510, 11, 5156, 553, 8913, 531, 13, 366, 33, 528, 526, 1, 40, 1053, 587, 21693, 281, 1711, 1541, 526, 383, 22353, 2900, 13, 1, 1135, 1392, 257, 8221, 17520, 295, 2406, 510, 526, 1, 1639, 1392, 281, 1027, 329, 3303, 11, 8913, 13, 5147, 345, 821, 636, 29388, 13, 1320, 338, 262, 4151, 943, 2781, 496, 468, 550, 319, 34686, 41976, 13, 10478, 502, 510, 526, 15156, 89, 571, 1077, 73, 666, 8302, 284, 307, 257, 1862, 582, 287, 257, 12768, 6050, 290, 3869, 12, 19298, 276, 11, 40070, 15232, 13, 2399, 2330, 10147, 373, 1280, 379, 262, 19908, 11, 13477, 257, 2603, 286, 3223, 4190, 523, 15715, 326, 8913, 379, 717, 4020, 566, 340, 329, 617, 1611, 286, 256, 12, 15600, 13, 679, 5284, 351, 257, 2042, 31248, 26473, 14921, 351, 1115, 7009, 11, 7956, 5250, 14180, 286, 6546, 2042, 6891, 290, 1115, 23408, 11, 14787, 12, 25717, 39713, 42402, 13, 1, 1135, 1276, 11, 355, 345, 910, 287, 17589, 346, 528, 837, 1011, 428, 530, 845, 2562, 526, 679, 3947, 284, 24170, 6235, 306, 379, 30236, 11, 475, 379, 938, 339, 4615, 262, 8465, 15232, 13, 2399, 2951, 547, 257, 3223, 7586, 326, 14451, 262, 17979, 286, 465, 845, 1790, 2422, 12, 8968, 4190, 13, 679, 13541, 13, 366, 1026, 318, 1365, 11, 428, 835, 11, 3763, 30, 25974, 356, 787, 262, 6278, 417, 220, 37174, 11, 10162, 656, 10162, 2644, 921, 3573, 553, 339, 531, 284, 607, 11, 366, 27238, 1011, 1337, 13, 554, 7137, 612, 318, 40174, 286, 1466, 508, 6332, 884, 19008, 526, 44, 5098, 1643, 530, 286, 262, 1613, 1678, 287, 2063, 13, 366, 1026, 338, 616, 905, 11, 3619, 553, 673, 531, 11, 607, 5422, 1336, 13, 1375, 1125, 19103, 11, 27961, 11, 290, 300, 9484, 607, 11914, 13, 366, 40, 760, 546, 345, 13, 520, 970, 329, 262, 2422, 11, 826, 1701, 2332, 1021, 27803, 37296, 813, 656, 262, 2166, 286, 607, 15224, 290, 1625, 503, 351, 262, 277, 29257, 13, 8913, 8020, 470, 1900, 673, 550, 340, 13, 1, 16371, 2562, 11, 3387, 553, 3813, 89, 571, 1077, 73, 666, 531, 11, 465, 2330, 442, 1437, 294, 34477, 12912, 48829, 422, 465, 11914, 13, 3347, 7083, 262, 2485, 13, 366, 13300, 345, 651, 262, 20732, 11, 6041, 286, 606, 11, 393, 3863, 345, 651, 257, 4890, 13, 1881, 35970, 11, 7510, 2550, 13, 921, 1839, 470, 1254, 340, 329, 1933, 526, 1], 'labels': [11914, 9859, 736, 13, 366, 3673, 1576, 11, 6029, 41495, 526, 679, 10158, 607, 257, 14091, 1994, 351, 257, 2835, 7872, 7621, 13, 366, 1639, 821, 6823, 1541, 13, 8835, 6679, 338, 26148, 526, 679, 3114, 1088, 13, 366, 1212, 3240, 22523, 526, 1, 1639, 651, 556, 273, 6570, 20803, 11, 484, 1011, 345, 503, 422, 739, 257, 29500, 13, 2329, 16614, 340, 338, 12232, 393, 1223, 526, 1375, 665, 49376, 262, 1994, 1088, 257, 7660, 13, 366, 1639, 994, 355, 1188, 316, 393, 644, 1701, 1, 40, 17753, 2198, 503, 617, 3516, 338, 35223, 553, 262, 15368, 531, 13, 1, 2437, 546, 616, 6203, 1701, 8913, 1965, 13, 464, 15368, 1592, 771, 13, 366, 31310, 3760, 262, 8435, 13, 16981, 262, 6478, 526, 44, 5098, 338, 9353, 3888, 287, 262, 9082, 286, 607, 15224, 11, 257, 781, 15799, 286, 474, 425, 13, 383, 15368, 7342, 11, 788, 14464, 13, 1, 10995, 553, 673, 531, 11, 366, 40, 760, 508, 326, 318, 526, 1375, 13665, 9091, 607, 1182, 287, 262, 4571, 286, 262, 7662, 2024, 13, 366, 16773, 319, 11, 42966, 526, 8913, 3940, 607, 351, 1111, 11668, 13, 14574, 2119, 1244, 423, 587, 262, 530, 287, 609, 23718, 810, 339, 1549, 717, 1775, 943, 2781, 496, 13, 679, 1816, 284, 262, 4324, 11, 287, 262, 3329, 11, 2048, 12451, 284, 766, 11790, 4696, 13, 1318, 373, 1194, 7541, 1973, 262, 4675, 13, 632, 373, 991, 43079, 13, 317, 1178, 3850, 12, 34422, 550, 2077, 5473, 287, 3420, 1322, 11, 511, 1468, 3809, 1050, 20193, 12908, 287, 15747, 286, 1598, 7309, 11, 2370, 326, 262, 3194, 1573, 991, 8359, 257, 1728, 29063, 994, 13, 632, 373, 257, 40186, 1499, 13, 679, 7342, 257, 19222, 2042, 15792, 305, 268, 38988, 11, 257, 20049, 17669, 12, 3846, 11315, 11, 355, 340, 595, 7053, 2004, 1936, 424, 297, 268, 12, 11534, 9663, 3790, 287, 7440, 10137, 4077, 22551, 13, 1119, 5982, 262, 7541, 1973, 262, 4675, 13, 1544, 27846, 736, 379, 262, 3996, 11, 379, 30236, 11, 290, 607, 6340, 9449, 7425, 683, 13, 1375, 1549, 1364, 262, 12314, 1773, 382, 3350, 319, 262, 3996, 6649, 397, 287, 511, 42186, 11, 13970, 262, 1007, 1082, 7617, 9318, 2189, 13, 2332, 15232, 12548, 636, 286, 262, 2119, 338, 1657, 29220, 13, 1544, 550, 262, 3072, 287, 465, 1021, 878, 340, 550, 257, 2863, 284, 5858, 5403, 13, 366, 38, 9435, 345, 821, 510, 553, 943, 2781, 496, 531, 13, 1, 40, 1101, 655, 13, 11182, 338, 991, 739, 13, 20600, 11, 6478, 11, 314, 892, 340, 338, 3863, 640, 356, 423, 257, 1310, 1561, 13, 314, 892, 314, 670, 1365, 611, 314, 760, 257, 1310, 517, 546, 644, 314, 1101, 1804, 526, 15086, 594, 319, 262, 1627, 13, 8913, 1643, 465, 10645, 13, 1, 1639, 760, 355, 881, 355, 345, 761, 284, 13, 6674, 517, 526, 1, 1639, 892, 523, 1701, 1, 3855, 12049, 11, 8913, 13, 3497, 607, 510, 13, 921, 1183, 423, 257, 24955, 287, 546, 17280, 2431, 13, 2399, 1438, 318, 3813, 89, 571, 1077, 73, 666, 526, 383, 3072, 7245, 515, 26625, 13, 943, 2781, 496, 373, 3750, 13, 1, 54, 539, 510, 11, 5156, 553, 8913, 531, 13, 366, 33, 528, 526, 1, 40, 1053, 587, 21693, 281, 1711, 1541, 526, 383, 22353, 2900, 13, 1, 1135, 1392, 257, 8221, 17520, 295, 2406, 510, 526, 1, 1639, 1392, 281, 1027, 329, 3303, 11, 8913, 13, 5147, 345, 821, 636, 29388, 13, 1320, 338, 262, 4151, 943, 2781, 496, 468, 550, 319, 34686, 41976, 13, 10478, 502, 510, 526, 15156, 89, 571, 1077, 73, 666, 8302, 284, 307, 257, 1862, 582, 287, 257, 12768, 6050, 290, 3869, 12, 19298, 276, 11, 40070, 15232, 13, 2399, 2330, 10147, 373, 1280, 379, 262, 19908, 11, 13477, 257, 2603, 286, 3223, 4190, 523, 15715, 326, 8913, 379, 717, 4020, 566, 340, 329, 617, 1611, 286, 256, 12, 15600, 13, 679, 5284, 351, 257, 2042, 31248, 26473, 14921, 351, 1115, 7009, 11, 7956, 5250, 14180, 286, 6546, 2042, 6891, 290, 1115, 23408, 11, 14787, 12, 25717, 39713, 42402, 13, 1, 1135, 1276, 11, 355, 345, 910, 287, 17589, 346, 528, 837, 1011, 428, 530, 845, 2562, 526, 679, 3947, 284, 24170, 6235, 306, 379, 30236, 11, 475, 379, 938, 339, 4615, 262, 8465, 15232, 13, 2399, 2951, 547, 257, 3223, 7586, 326, 14451, 262, 17979, 286, 465, 845, 1790, 2422, 12, 8968, 4190, 13, 679, 13541, 13, 366, 1026, 318, 1365, 11, 428, 835, 11, 3763, 30, 25974, 356, 787, 262, 6278, 417, 220, 37174, 11, 10162, 656, 10162, 2644, 921, 3573, 553, 339, 531, 284, 607, 11, 366, 27238, 1011, 1337, 13, 554, 7137, 612, 318, 40174, 286, 1466, 508, 6332, 884, 19008, 526, 44, 5098, 1643, 530, 286, 262, 1613, 1678, 287, 2063, 13, 366, 1026, 338, 616, 905, 11, 3619, 553, 673, 531, 11, 607, 5422, 1336, 13, 1375, 1125, 19103, 11, 27961, 11, 290, 300, 9484, 607, 11914, 13, 366, 40, 760, 546, 345, 13, 520, 970, 329, 262, 2422, 11, 826, 1701, 2332, 1021, 27803, 37296, 813, 656, 262, 2166, 286, 607, 15224, 290, 1625, 503, 351, 262, 277, 29257, 13, 8913, 8020, 470, 1900, 673, 550, 340, 13, 1, 16371, 2562, 11, 3387, 553, 3813, 89, 571, 1077, 73, 666, 531, 11, 465, 2330, 442, 1437, 294, 34477, 12912, 48829, 422, 465, 11914, 13, 3347, 7083, 262, 2485, 13, 366, 13300, 345, 651, 262, 20732, 11, 6041, 286, 606, 11, 393, 3863, 345, 651, 257, 4890, 13, 1881, 35970, 11, 7510, 2550, 13, 921, 1839, 470, 1254, 340, 329, 1933, 526, 1]}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group texts into blocks\n",
    "# WARNING: Be sure to subtract the size of the soft prompt!\n",
    "block_size = 1024 - n_tokens\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import default_data_collator\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=2\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from mkultra.trainers import SoftPromptTrainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "\n",
    "optimizer = Adafactor([model.get_soft_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "attention_mask.shape torch.Size([2, 924])\n",
      "1: Loss: 4.683137893676758\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "2: Loss: 4.347650527954102\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "3: Loss: 4.402397632598877\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "4: Loss: 4.403602123260498\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "5: Loss: 4.4982991218566895\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "6: Loss: 4.507497310638428\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "7: Loss: 4.427779674530029\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "8: Loss: 4.495551586151123\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "9: Loss: 4.666794776916504\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "10: Loss: 4.386064529418945\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "11: Loss: 4.577767848968506\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "12: Loss: 4.400213241577148\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "13: Loss: 4.582705974578857\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "14: Loss: 4.4947357177734375\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "15: Loss: 4.424174785614014\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "16: Loss: 4.468663692474365\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "17: Loss: 4.398224830627441\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "18: Loss: 4.2408342361450195\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "19: Loss: 4.453339099884033\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "20: Loss: 4.377774715423584\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "21: Loss: 4.329304218292236\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "22: Loss: 4.4332990646362305\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "23: Loss: 4.163923740386963\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "24: Loss: 4.575640678405762\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "25: Loss: 4.120519161224365\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "26: Loss: 4.5116987228393555\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "27: Loss: 4.456766128540039\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "28: Loss: 4.534897327423096\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "29: Loss: 4.378227233886719\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "30: Loss: 4.545950412750244\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "31: Loss: 4.092928886413574\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "32: Loss: 4.5377678871154785\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "33: Loss: 4.159762859344482\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "34: Loss: 4.289729595184326\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "35: Loss: 4.353936195373535\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "36: Loss: 4.43389368057251\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "37: Loss: 4.418415546417236\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "38: Loss: 4.4666242599487305\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "39: Loss: 4.616391658782959\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "40: Loss: 4.5748419761657715\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "41: Loss: 4.403321266174316\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "42: Loss: 4.416225910186768\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "43: Loss: 4.432640552520752\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "44: Loss: 4.651280879974365\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "45: Loss: 4.533641338348389\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "46: Loss: 4.39278507232666\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "47: Loss: 4.390194416046143\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "48: Loss: 4.411249160766602\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "49: Loss: 4.522272109985352\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "50: Loss: 4.217926979064941\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "51: Loss: 4.5536932945251465\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "52: Loss: 4.313566207885742\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "53: Loss: 4.183739185333252\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "54: Loss: 4.448067665100098\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "55: Loss: 4.185108661651611\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "56: Loss: 4.399654388427734\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "57: Loss: 4.589838027954102\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "58: Loss: 4.313483715057373\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "59: Loss: 4.382741451263428\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "60: Loss: 4.1965718269348145\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "61: Loss: 4.146615028381348\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "62: Loss: 4.530211925506592\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "63: Loss: 4.511685848236084\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "64: Loss: 4.327962398529053\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "65: Loss: 4.542709827423096\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "66: Loss: 4.434135913848877\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "67: Loss: 4.419013977050781\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "68: Loss: 4.428621768951416\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "69: Loss: 4.24506950378418\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "70: Loss: 4.377384185791016\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "71: Loss: 4.5489583015441895\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "72: Loss: 4.280648708343506\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "73: Loss: 4.375644207000732\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "74: Loss: 4.483155727386475\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "75: Loss: 4.48102331161499\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "76: Loss: 4.342395782470703\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "77: Loss: 4.323346138000488\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "78: Loss: 4.595541000366211\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "79: Loss: 4.448040962219238\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "80: Loss: 4.448185443878174\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "81: Loss: 4.573436737060547\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "82: Loss: 4.219350337982178\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "83: Loss: 4.608246326446533\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "84: Loss: 4.4420318603515625\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "85: Loss: 4.355764389038086\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "86: Loss: 4.489466667175293\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "87: Loss: 4.545799255371094\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "88: Loss: 4.1988372802734375\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "89: Loss: 4.524331569671631\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "90: Loss: 4.494537353515625\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "91: Loss: 4.372265338897705\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "92: Loss: 4.366288185119629\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "93: Loss: 4.389959335327148\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "94: Loss: 4.221919536590576\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "95: Loss: 4.525860786437988\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "96: Loss: 4.591976165771484\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "97: Loss: 4.435881614685059\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "98: Loss: 4.29598331451416\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "99: Loss: 4.3331427574157715\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "100: Loss: 4.278336048126221\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "101: Loss: 4.226154804229736\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "102: Loss: 4.480208396911621\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "103: Loss: 4.364259719848633\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "104: Loss: 4.535038471221924\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "105: Loss: 4.545931339263916\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "106: Loss: 4.412410736083984\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "107: Loss: 4.147848129272461\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "108: Loss: 4.401791572570801\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "109: Loss: 4.496715545654297\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "110: Loss: 4.157626152038574\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "111: Loss: 4.327887535095215\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "112: Loss: 4.398446083068848\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "113: Loss: 4.3636651039123535\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "114: Loss: 4.6071553230285645\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "115: Loss: 4.290399074554443\n",
      "attention_mask.shape torch.Size([2, 924])\n",
      "116: Loss: 4.662249565124512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_train_epochs = 2\n",
    "\n",
    "total_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k:v.type(torch.long).to(\"cuda\") for k,v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_step += 1\n",
    "        print(f\"{total_step}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp = SoftPrompt.from_tuning_model(model)\n",
    "#sp.to_file(\"neuromancer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SoftPrompt.from_file(\"sample_sps/finetune/neuromancer.json\")\n",
    "model.set_soft_prompt(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2116478d7a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Armitage\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m basic_output = model.generate(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdo_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, tfs, repetition_penalty, repetition_penalty_range, repetition_penalty_slope, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[1;31m# sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m             return self.sample(\n\u001b[0m\u001b[0;32m   1040\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             \u001b[1;31m# forward pass to get next token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1550\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   1551\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\models\\tuning.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inputs_embeds'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cat_learned_embedding_to_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'position_ids'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Git Repos\\mkultra\\mkultra\\models\\tuning.py\u001b[0m in \u001b[0;36m_extend_labels\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# Add '-100's (prevent loss calculation where the learned embed would be)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mn_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "call = tokenizer(\"Armitage\", return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    temperature=0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}