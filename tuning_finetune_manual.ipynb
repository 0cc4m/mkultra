{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd03a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "source": [
    "# Tuning on Datasets\n",
    "This sheet is adapted from the language modeling example at\n",
    "https://github.com/huggingface/transformers/tree/master/examples/pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.tuning import GPT2PromptTuningLM\n",
    "from mkultra.tokenizers import GPT2TokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an mkultra prompt tuning LM and a standard tokenizer.\n",
    "model = GPT2PromptTuningLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "n_tokens = 100\n",
    "\n",
    "model.initialize_soft_prompt(n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-812ce7cd67147d37\n",
      "Reusing dataset text (C:\\Users\\STARSTRUCK\\.cache\\huggingface\\datasets\\text\\default-812ce7cd67147d37\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"datasets/neuromancer_reformatted.txt\"\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": path, \"validation\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 26.31ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 28.84ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1], 'input_ids': [1525, 3977, 20400]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 1561, 259, 6, 45967, 6, 16899, 11, 45812, 384, 71, 2005, 281, 6, 1057, 526, 679, 4966, 262, 736, 286, 257, 1588, 7586, 1021, 1973, 465, 5422, 13, 1, 3163, 2781, 496, 1701, 8913, 1592, 771, 355, 262, 731, 499, 831, 33077, 2543, 500, 8181, 2502, 2277, 683, 351, 663, 1336, 12245, 11, 555, 9612, 276, 416, 262, 17593, 393, 985, 42003, 13, 14842, 338, 1392, 645, 25377, 287, 340, 11, 339, 1297, 2241, 11, 340, 460, 470, 1107, 1254, 428, 2089, 13, 366, 2061, 466, 345, 1612, 11, 582, 30, 679, 338, 3501, 345, 6266, 30, 1867, 1701, 1, 9069, 11, 943, 2781, 496, 11, 339, 1560, 259, 6, 502, 900, 1781, 329, 17837, 11, 21349, 760, 30, 679, 1560, 259, 6, 502, 612, 307, 2911, 11, 21349, 760, 30, 7911, 319, 616, 3159, 45967, 6, 465, 10147, 477, 2910, 11, 937, 11, 281, 6, 307, 7165, 355, 617, 3290, 11, 1561, 259, 6, 8196, 259, 6, 33053, 281, 6, 3394, 281, 6, 294, 6, 2910, 286, 294, 6, 13935, 364, 2236, 307, 319, 674, 2832, 526, 679, 14682, 465, 1182, 757, 11, 262, 15157, 11128, 20009, 278, 290, 29202, 4623, 287, 6632, 12, 70, 11, 465, 11914, 33214, 13, 366, 21077, 364, 384, 71, 262, 337, 1133, 3809, 307, 3991, 22298, 10403, 11, 281, 6, 15781, 349, 281, 6, 314, 1928, 6, 705, 3903, 261, 17068, 7164, 3304, 220, 290, 1441, 526, 1, 3163, 2781, 496, 11, 339, 373, 10657, 30, 7235, 1701, 1, 6090, 470, 384, 71, 11, 21349, 760, 30, 887, 2910, 11, 281, 6, 7815, 7165, 11, 8913, 526, 1, 16454, 553, 8913, 531, 11, 366, 2396, 644, 546, 502, 30, 921, 821, 1016, 1363, 13, 1867, 546, 502, 11, 337, 3010, 36340, 1701, 1, 9069, 553, 337, 3010, 36340, 531, 11, 366, 5832, 401, 259, 6, 45967, 6, 502, 13, 314, 281, 6, 314, 1282, 16899, 45967, 6, 15781, 349, 11, 28028, 4631, 263, 13, 220, 17446, 1770, 13, 943, 2781, 496, 256, 6, 1561, 45967, 6, 10905, 42812, 11, 530, 10905, 256, 6, 705, 77, 847, 35713, 20448, 27846, 625, 465, 8163, 25, 465, 26399, 6050, 28507, 1028, 262, 25755, 735, 810, 339, 1549, 20821, 340, 11, 20009, 278, 287, 262, 1633, 1459, 422, 262, 1468, 3394, 27268, 527, 13, 679, 4838, 465, 2951, 13, 679, 2497, 262, 5360, 82, 286, 43026, 6249, 10890, 287, 465, 45894, 13, 679, 2497, 30236, 387, 16619, 5223, 510, 262, 13079, 7771, 1057, 14542, 13, 679, 4721, 465, 2951, 13, 1, 40, 50109, 11, 582, 553, 339, 531, 11, 257, 6283, 6938, 287, 465, 5422, 13, 679, 3114, 866, 379, 465, 6915, 11, 379, 465, 2832, 13, 366, 40, 836, 470, 760, 526, 679, 3114, 736, 510], 'labels': [338, 329, 345, 553, 673, 531, 11, 14281, 47878, 13, 383, 1976, 27498, 9660, 3077, 757, 290, 257, 3275, 336, 46322, 612, 11, 287, 262, 5228, 286, 607, 5761, 11, 20720, 510, 416, 262, 3359, 10349, 13, 38, 412, 399, 412, 371, 317, 406, 220, 220, 402, 40, 371, 406, 314, 399, 402, 1058, 1058, 1058, 51, 371, 317, 314, 399, 412, 360, 34, 440, 371, 309, 440, 220, 220, 376, 440, 371, 50, 327, 371, 412, 317, 337, 314, 399, 402, 37, 314, 311, 309, 220, 220, 317, 399, 360, 50, 440, 406, 360, 220, 367, 314, 311, 32, 311, 311, 220, 309, 440, 51, 367, 412, 220, 350, 412, 399, 309, 32, 402, 440, 399, 1058, 1058, 1058, 1058, 54, 1220, 337, 471, 309, 412, 705, 311, 47, 371, 314, 337, 317, 371, 575, 38, 371, 314, 350, 220, 220, 440, 399, 32, 371, 337, 314, 309, 317, 402, 36, 220, 220, 314, 311, 220, 220, 317, 34, 440, 399, 311, 309, 371, 471, 34, 309, 220, 220, 440, 376, 220, 220, 402, 40, 371, 406, 314, 399, 402, 1058, 54, 1220, 337, 471, 309, 412, 50, 412, 1168, 220, 220, 317, 705, 311, 44, 412, 399, 309, 314, 440, 399, 46, 376, 220, 220, 402, 44, 412, 317, 399, 311, 39, 412, 705, 311, 34, 371, 317, 327, 509, 40, 399, 402, 25, 1058, 1058, 1058, 54, 317, 309, 327, 367, 56, 440, 471, 371, 32, 311, 311, 25, 1058, 1058, 7904, 1058, 35, 314, 1395, 314, 412, 1, 5779, 553, 673, 531, 11, 14187, 3500, 11, 2263, 477, 286, 607, 3463, 319, 607, 826, 1232, 11, 366, 5162, 408, 345, 1392, 2761, 1165, 526, 1375, 3114, 866, 13, 1318, 373, 257, 18107, 9197, 286, 1657, 11, 645, 4025, 621, 262, 20422, 2835, 286, 262, 609, 33670, 1994, 326, 288, 22393, 1022, 607, 17515, 13, 1375, 3114, 510, 13, 10528, 379, 477, 13, 1375, 10479, 1739, 607, 45796, 290, 262, 12403, 8278, 656, 49047, 6650, 11, 262, 34492, 10868, 663, 835, 510, 262, 1057, 14542, 13, 366, 24795, 1297, 502, 546, 428, 636, 553, 673, 531, 13, 20448, 474, 6021, 503, 13, 1, 44, 3010, 36340, 764, 764, 22135, 1, 9069, 11, 345, 6478, 805, 3750, 3326, 6, 6283, 526, 383, 16899, 578, 373, 5762, 257, 4171, 311, 1092, 78, 17076, 6050, 8208, 812, 4697, 621, 262, 530, 8913, 550, 26399, 287, 4848, 274, 485, 11, 663, 14335, 739, 465, 3211, 290, 465, 15157, 28860, 6131, 2004, 287, 257, 2010, 1451, 6763, 2395, 1513, 422, 14032, 15985, 21181, 13, 2399, 2951, 547, 1017, 2175, 351, 308, 272, 6592, 290, 12097, 13, 366, 15597, 869, 259, 6, 866, 994, 45967, 6, 6266, 837, 937, 11, 475, 307, 617, 28028, 1175, 35713, 337, 3010, 36340, 14682, 465, 1182, 13, 366, 32, 263, 349, 281, 6, 314, 1561, 259, 3256, 281, 6, 15781, 349, 1561, 259, 6, 45967, 6, 16899, 11, 45812, 384, 71, 2005, 281, 6, 1057, 526, 679, 4966, 262, 736, 286, 257, 1588, 7586, 1021, 1973, 465, 5422, 13, 1, 3163, 2781, 496, 1701, 8913, 1592, 771, 355, 262, 731, 499, 831, 33077, 2543, 500, 8181, 2502, 2277, 683, 351, 663, 1336, 12245, 11, 555, 9612, 276, 416, 262, 17593, 393, 985, 42003, 13, 14842, 338, 1392, 645, 25377, 287, 340, 11, 339, 1297, 2241, 11, 340, 460, 470, 1107, 1254, 428, 2089, 13, 366, 2061, 466, 345, 1612, 11, 582, 30, 679, 338, 3501, 345, 6266, 30, 1867, 1701, 1, 9069, 11, 943, 2781, 496, 11, 339, 1560, 259, 6, 502, 900, 1781, 329, 17837, 11, 21349, 760, 30, 679, 1560, 259, 6, 502, 612, 307, 2911, 11, 21349, 760, 30, 7911, 319, 616, 3159, 45967, 6, 465, 10147, 477, 2910, 11, 937, 11, 281, 6, 307, 7165, 355, 617, 3290, 11, 1561, 259, 6, 8196, 259, 6, 33053, 281, 6, 3394, 281, 6, 294, 6, 2910, 286, 294, 6, 13935, 364, 2236, 307, 319, 674, 2832, 526, 679, 14682, 465, 1182, 757, 11, 262, 15157, 11128, 20009, 278, 290, 29202, 4623, 287, 6632, 12, 70, 11, 465, 11914, 33214, 13, 366, 21077, 364, 384, 71, 262, 337, 1133, 3809, 307, 3991, 22298, 10403, 11, 281, 6, 15781, 349, 281, 6, 314, 1928, 6, 705, 3903, 261, 17068, 7164, 3304, 220, 290, 1441, 526, 1, 3163, 2781, 496, 11, 339, 373, 10657, 30, 7235, 1701, 1, 6090, 470, 384, 71, 11, 21349, 760, 30, 887, 2910, 11, 281, 6, 7815, 7165, 11, 8913, 526, 1, 16454, 553, 8913, 531, 11, 366, 2396, 644, 546, 502, 30, 921, 821, 1016, 1363, 13, 1867, 546, 502, 11, 337, 3010, 36340, 1701, 1, 9069, 553, 337, 3010, 36340, 531, 11, 366, 5832, 401, 259, 6, 45967, 6, 502, 13, 314, 281, 6, 314, 1282, 16899, 45967, 6, 15781, 349, 11, 28028, 4631, 263, 13, 220, 17446, 1770, 13, 943, 2781, 496, 256, 6, 1561, 45967, 6, 10905, 42812, 11, 530, 10905, 256, 6, 705, 77, 847, 35713, 20448, 27846, 625, 465, 8163, 25, 465, 26399, 6050, 28507, 1028, 262, 25755, 735, 810, 339, 1549, 20821, 340, 11, 20009, 278, 287, 262, 1633, 1459, 422, 262, 1468, 3394, 27268, 527, 13, 679, 4838, 465, 2951, 13, 679, 2497, 262, 5360, 82, 286, 43026, 6249, 10890, 287, 465, 45894, 13, 679, 2497, 30236, 387, 16619, 5223, 510, 262, 13079, 7771, 1057, 14542, 13, 679, 4721, 465, 2951, 13, 1, 40, 50109, 11, 582, 553, 339, 531, 11, 257, 6283, 6938, 287, 465, 5422, 13, 679, 3114, 866, 379, 465, 6915, 11, 379, 465, 2832, 13, 366, 40, 836, 470, 760, 526, 679, 3114, 736, 510]}.\n",
      "Sample 85 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [18418, 78, 373, 1262, 262, 367, 3216, 10247, 705, 82, 16408, 6203, 284, 2792, 351, 262, 30789, 8130, 287, 17068, 7164, 3304, 13, 1, 20448, 11, 314, 761, 262, 2465, 3136, 319, 37053, 9850, 526, 1, 25515, 11, 314, 764, 764, 764, 21636, 1701, 1, 39, 648, 287, 612, 11, 2933, 13, 11436, 534, 3047, 526, 1537, 810, 423, 345, 587, 837, 582, 30, 220, 339, 24595, 1965, 262, 3550, 84, 1348, 2951, 13, 10633, 76, 1133, 550, 3170, 1223, 1444, 943, 2781, 496, 656, 257, 3797, 265, 9229, 28757, 3706, 18418, 78, 13, 11161, 9431, 18418, 78, 326, 943, 2781, 496, 373, 262, 1103, 1517, 11, 290, 943, 2781, 496, 550, 6807, 11, 6619, 11, 3897, 1150, 11, 30539, 1068, 1366, 329, 3139, 11, 2166, 276, 329, 10633, 76, 1133, 287, 326, 2119, 287, 262, 609, 23718, 31248, 2644, 843, 783, 943, 2781, 496, 373, 3750, 11, 16318, 1497, 416, 262, 13520, 286, 18418, 78, 338, 23208, 13, 887, 810, 550, 18418, 78, 587, 837, 883, 812, 30, 37, 9221, 11, 11544, 290, 38497, 11, 503, 286, 257, 47965, 6766, 13, 1, 20448, 11, 428, 481, 307, 2408, 329, 345, 284, 2453, 11, 314, 760, 326, 13, 921, 821, 281, 3818, 13, 383, 3047, 13, 314, 1833, 13, 887, 11, 8913, 11, 355, 1793, 318, 616, 4973, 11, 356, 423, 587, 26281, 526, 51, 4127, 2067, 422, 262, 4171, 2951, 13, 1, 5216, 26261, 11, 29042, 11, 508, 30, 5338, 338, 26281, 514, 1701, 1, 12218, 23837, 1359, 11, 8913, 13, 921, 743, 760, 683, 416, 257, 2438, 1438, 13, 921, 466, 760, 262, 582, 286, 4150, 314, 2740, 526, 1, 10995, 553, 8913, 531, 11, 355, 262, 10953, 3767, 284, 5202, 11, 366, 40, 4724, 314, 466, 13, 7361, 553, 339, 2087, 11, 319, 25278, 13, 366, 1537, 11, 15967, 11, 21636, 11, 644, 3446, 815, 356, 466, 30, 2735, 11, 314, 1612, 526, 1, 5122, 7077, 379, 428, 966, 11, 8913, 11, 7363, 287, 5474, 13, 14473, 13, 4319, 4247, 13, 775, 460, 787, 262, 26838, 4865, 11, 1755, 7207, 9439, 13, 309, 2871, 404, 7348, 319, 10107, 13, 26682, 286, 262, 12581, 11, 2933, 13, 887, 326, 481, 691, 307, 262, 3726, 526, 383, 4171, 2951, 1017, 2175, 2029, 256, 3577, 19353, 35095, 29395, 351, 10953, 13, 366, 10049, 262, 3726, 13, 5147, 2433, 282, 422, 2029, 13, 3574, 2029, 764, 764, 22135, 679, 10764, 736, 422, 262, 4676, 11, 3223, 43329, 319, 465, 12445, 665, 359, 10147, 13, 943, 2781, 496, 338, 1986, 550, 587, 9335, 2339, 11, 848, 562, 425, 11, 475, 18418, 78, 338, 373, 262, 2081, 5513, 528, 1868, 9335, 11, 8526, 48755, 2769, 287, 37099, 8280, 11, 1233, 24707, 262, 5789, 8185, 13, 1, 5216, 26261, 11, 314, 3285, 345, 11, 582, 13, 20600, 11, 21636, 11, 8788, 30, 314, 765, 345, 284, 1280, 262, 11, 29042, 764, 764, 764, 7510, 11, 644, 338, 340, 1444, 11, 360, 844, 1701, 1, 464, 3095, 24406, 5793, 553, 262, 21939, 1370, 531, 13, 1, 11505, 262, 3095, 24406, 5793, 13, 2329, 1560, 534, 4318, 8624, 612, 284, 1280, 340, 11, 826, 30, 775, 1183, 307, 510, 612, 351, 345, 3049, 11, 21636, 13, 3244, 356, 460, 1561, 546, 1972, 503, 286, 994, 526, 464, 2376, 89, 3540, 23717, 13, 1, 26554, 11, 314, 892, 345, 655, 2626, 502, 11, 612, 553, 262, 21939, 1370, 531, 13, 1, 464, 31930, 553, 8913, 531, 11, 366, 1169, 9372, 31930, 553, 290, 474, 6021, 503, 13, 1, 18833, 1653, 1701, 337, 3010, 36340, 7342, 625, 262, 37468, 4171, 8163, 286, 465, 1468, 311, 1092, 78, 355, 8913, 11615, 503, 286, 262, 308, 12, 12384, 13, 1, 1870, 651, 428, 17495, 321, 1517, 572, 502, 35713, 309, 1018, 2667, 379, 262, 3936, 3797, 43332, 13, 366, 7594, 257, 3105, 8764, 11, 290, 326, 29836, 26148, 4206, 703, 284, 3753, 340, 11, 290, 783, 339, 338, 27627, 959, 621, 257, 427, 342, 1076, 4227, 526, 679, 277, 11137, 351, 262, 2166, 286, 262, 2266, 311, 1092, 78, 11, 32012, 703, 284, 670, 262, 29718, 13, 1, 37310, 805, 11, 339, 8764, 220, 345, 1701, 337, 3010, 36340, 37468, 465, 19353, 13, 366, 30074, 257, 3315, 6220, 11, 21349, 760, 526, 1, 44, 3010, 36340, 11, 1951, 11, 1037, 502, 351, 428, 17495, 321, 6050, 526, 464, 16899, 578, 12165, 572, 422, 262, 11398, 8022, 8265, 13, 366, 28406, 11, 937, 13, 24291, 5403, 11, 2005, 1752, 11, 10787, 582, 1234, 340, 13, 775, 651, 510, 612, 35713, 1858, 373, 1633, 287, 262, 1162, 2143, 515, 7706, 1014, 326, 2957, 422, 17068, 7164, 3304, 220, 705, 82, 46088, 5793, 284, 262, 3095, 24406, 5793, 286, 262, 43207, 1444, 367, 3216, 10247, 837, 475, 484, 4030, 511, 14803, 15283, 13, 337, 3010, 36340, 10945, 262, 10066, 351, 47735, 291, 11542, 11, 691, 14187, 3500, 284, 1037, 8913, 11, 508, 1549, 3750, 656, 281, 13006, 47978, 355, 339, 1549, 10764, 503, 286, 7164, 3304, 13, 383, 2330, 7309, 5389, 286, 262, 12403, 29083, 262, 8246, 19606, 26, 612, 547, 645, 16187, 13, 27676, 3304, 220, 705, 82, 1633, 5354, 25834, 373, 39378, 290, 46852, 11, 24789, 351, 257, 12855, 12, 7718, 1079, 15218, 286, 16899, 13, 367, 3216, 10247, 705, 82, 3095, 24406, 25834, 373, 27892, 12768, 11, 9178, 290, 37293, 13, 337, 3010, 36340, 18846, 465, 1278, 2668, 1021, 287, 257, 7135, 28836, 13, 8913, 2497, 465, 9353, 1445, 13, 2297, 33697, 1625, 284, 1204, 287, 262, 28836, 11, 14143, 866, 422, 15334, 13, 337, 3010, 36340, 25518, 465, 1021, 13, 8913, 11, 351], 'labels': [18418, 78, 373, 1262, 262, 367, 3216, 10247, 705, 82, 16408, 6203, 284, 2792, 351, 262, 30789, 8130, 287, 17068, 7164, 3304, 13, 1, 20448, 11, 314, 761, 262, 2465, 3136, 319, 37053, 9850, 526, 1, 25515, 11, 314, 764, 764, 764, 21636, 1701, 1, 39, 648, 287, 612, 11, 2933, 13, 11436, 534, 3047, 526, 1537, 810, 423, 345, 587, 837, 582, 30, 220, 339, 24595, 1965, 262, 3550, 84, 1348, 2951, 13, 10633, 76, 1133, 550, 3170, 1223, 1444, 943, 2781, 496, 656, 257, 3797, 265, 9229, 28757, 3706, 18418, 78, 13, 11161, 9431, 18418, 78, 326, 943, 2781, 496, 373, 262, 1103, 1517, 11, 290, 943, 2781, 496, 550, 6807, 11, 6619, 11, 3897, 1150, 11, 30539, 1068, 1366, 329, 3139, 11, 2166, 276, 329, 10633, 76, 1133, 287, 326, 2119, 287, 262, 609, 23718, 31248, 2644, 843, 783, 943, 2781, 496, 373, 3750, 11, 16318, 1497, 416, 262, 13520, 286, 18418, 78, 338, 23208, 13, 887, 810, 550, 18418, 78, 587, 837, 883, 812, 30, 37, 9221, 11, 11544, 290, 38497, 11, 503, 286, 257, 47965, 6766, 13, 1, 20448, 11, 428, 481, 307, 2408, 329, 345, 284, 2453, 11, 314, 760, 326, 13, 921, 821, 281, 3818, 13, 383, 3047, 13, 314, 1833, 13, 887, 11, 8913, 11, 355, 1793, 318, 616, 4973, 11, 356, 423, 587, 26281, 526, 51, 4127, 2067, 422, 262, 4171, 2951, 13, 1, 5216, 26261, 11, 29042, 11, 508, 30, 5338, 338, 26281, 514, 1701, 1, 12218, 23837, 1359, 11, 8913, 13, 921, 743, 760, 683, 416, 257, 2438, 1438, 13, 921, 466, 760, 262, 582, 286, 4150, 314, 2740, 526, 1, 10995, 553, 8913, 531, 11, 355, 262, 10953, 3767, 284, 5202, 11, 366, 40, 4724, 314, 466, 13, 7361, 553, 339, 2087, 11, 319, 25278, 13, 366, 1537, 11, 15967, 11, 21636, 11, 644, 3446, 815, 356, 466, 30, 2735, 11, 314, 1612, 526, 1, 5122, 7077, 379, 428, 966, 11, 8913, 11, 7363, 287, 5474, 13, 14473, 13, 4319, 4247, 13, 775, 460, 787, 262, 26838, 4865, 11, 1755, 7207, 9439, 13, 309, 2871, 404, 7348, 319, 10107, 13, 26682, 286, 262, 12581, 11, 2933, 13, 887, 326, 481, 691, 307, 262, 3726, 526, 383, 4171, 2951, 1017, 2175, 2029, 256, 3577, 19353, 35095, 29395, 351, 10953, 13, 366, 10049, 262, 3726, 13, 5147, 2433, 282, 422, 2029, 13, 3574, 2029, 764, 764, 22135, 679, 10764, 736, 422, 262, 4676, 11, 3223, 43329, 319, 465, 12445, 665, 359, 10147, 13, 943, 2781, 496, 338, 1986, 550, 587, 9335, 2339, 11, 848, 562, 425, 11, 475, 18418, 78, 338, 373, 262, 2081, 5513, 528, 1868, 9335, 11, 8526, 48755, 2769, 287, 37099, 8280, 11, 1233, 24707, 262, 5789, 8185, 13, 1, 5216, 26261, 11, 314, 3285, 345, 11, 582, 13, 20600, 11, 21636, 11, 8788, 30, 314, 765, 345, 284, 1280, 262, 11, 29042, 764, 764, 764, 7510, 11, 644, 338, 340, 1444, 11, 360, 844, 1701, 1, 464, 3095, 24406, 5793, 553, 262, 21939, 1370, 531, 13, 1, 11505, 262, 3095, 24406, 5793, 13, 2329, 1560, 534, 4318, 8624, 612, 284, 1280, 340, 11, 826, 30, 775, 1183, 307, 510, 612, 351, 345, 3049, 11, 21636, 13, 3244, 356, 460, 1561, 546, 1972, 503, 286, 994, 526, 464, 2376, 89, 3540, 23717, 13, 1, 26554, 11, 314, 892, 345, 655, 2626, 502, 11, 612, 553, 262, 21939, 1370, 531, 13, 1, 464, 31930, 553, 8913, 531, 11, 366, 1169, 9372, 31930, 553, 290, 474, 6021, 503, 13, 1, 18833, 1653, 1701, 337, 3010, 36340, 7342, 625, 262, 37468, 4171, 8163, 286, 465, 1468, 311, 1092, 78, 355, 8913, 11615, 503, 286, 262, 308, 12, 12384, 13, 1, 1870, 651, 428, 17495, 321, 1517, 572, 502, 35713, 309, 1018, 2667, 379, 262, 3936, 3797, 43332, 13, 366, 7594, 257, 3105, 8764, 11, 290, 326, 29836, 26148, 4206, 703, 284, 3753, 340, 11, 290, 783, 339, 338, 27627, 959, 621, 257, 427, 342, 1076, 4227, 526, 679, 277, 11137, 351, 262, 2166, 286, 262, 2266, 311, 1092, 78, 11, 32012, 703, 284, 670, 262, 29718, 13, 1, 37310, 805, 11, 339, 8764, 220, 345, 1701, 337, 3010, 36340, 37468, 465, 19353, 13, 366, 30074, 257, 3315, 6220, 11, 21349, 760, 526, 1, 44, 3010, 36340, 11, 1951, 11, 1037, 502, 351, 428, 17495, 321, 6050, 526, 464, 16899, 578, 12165, 572, 422, 262, 11398, 8022, 8265, 13, 366, 28406, 11, 937, 13, 24291, 5403, 11, 2005, 1752, 11, 10787, 582, 1234, 340, 13, 775, 651, 510, 612, 35713, 1858, 373, 1633, 287, 262, 1162, 2143, 515, 7706, 1014, 326, 2957, 422, 17068, 7164, 3304, 220, 705, 82, 46088, 5793, 284, 262, 3095, 24406, 5793, 286, 262, 43207, 1444, 367, 3216, 10247, 837, 475, 484, 4030, 511, 14803, 15283, 13, 337, 3010, 36340, 10945, 262, 10066, 351, 47735, 291, 11542, 11, 691, 14187, 3500, 284, 1037, 8913, 11, 508, 1549, 3750, 656, 281, 13006, 47978, 355, 339, 1549, 10764, 503, 286, 7164, 3304, 13, 383, 2330, 7309, 5389, 286, 262, 12403, 29083, 262, 8246, 19606, 26, 612, 547, 645, 16187, 13, 27676, 3304, 220, 705, 82, 1633, 5354, 25834, 373, 39378, 290, 46852, 11, 24789, 351, 257, 12855, 12, 7718, 1079, 15218, 286, 16899, 13, 367, 3216, 10247, 705, 82, 3095, 24406, 25834, 373, 27892, 12768, 11, 9178, 290, 37293, 13, 337, 3010, 36340, 18846, 465, 1278, 2668, 1021, 287, 257, 7135, 28836, 13, 8913, 2497, 465, 9353, 1445, 13, 2297, 33697, 1625, 284, 1204, 287, 262, 28836, 11, 14143, 866, 422, 15334, 13, 337, 3010, 36340, 25518, 465, 1021, 13, 8913, 11, 351]}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group texts into blocks\n",
    "# WARNING: Be sure to subtract the size of the soft prompt!\n",
    "block_size = 1024 - n_tokens\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import default_data_collator\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=1\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from mkultra.trainers import SoftPromptTrainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "\n",
    "optimizer = Adafactor([model.get_soft_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "s: 4.416869640350342\n",
      "2248: Loss: 4.338390350341797\n",
      "2249: Loss: 3.7304539680480957\n",
      "2250: Loss: 4.077001571655273\n",
      "2251: Loss: 3.929363250732422\n",
      "2252: Loss: 3.9027459621429443\n",
      "2253: Loss: 4.196310520172119\n",
      "2254: Loss: 3.983847141265869\n",
      "2255: Loss: 4.212437152862549\n",
      "2256: Loss: 3.8088133335113525\n",
      "2257: Loss: 3.9873321056365967\n",
      "2258: Loss: 3.8761932849884033\n",
      "2259: Loss: 4.325216293334961\n",
      "2260: Loss: 4.3976850509643555\n",
      "2261: Loss: 4.4717888832092285\n",
      "2262: Loss: 3.9001622200012207\n",
      "2263: Loss: 4.1136579513549805\n",
      "2264: Loss: 4.366222858428955\n",
      "2265: Loss: 4.522253036499023\n",
      "2266: Loss: 4.0323028564453125\n",
      "2267: Loss: 4.241368770599365\n",
      "2268: Loss: 4.2641143798828125\n",
      "2269: Loss: 3.9137303829193115\n",
      "2270: Loss: 4.089511394500732\n",
      "2271: Loss: 3.880185127258301\n",
      "2272: Loss: 4.275704383850098\n",
      "2273: Loss: 4.123953819274902\n",
      "2274: Loss: 4.099294185638428\n",
      "2275: Loss: 3.8179166316986084\n",
      "2276: Loss: 3.8860814571380615\n",
      "2277: Loss: 4.3091840744018555\n",
      "2278: Loss: 3.926036834716797\n",
      "2279: Loss: 3.870868682861328\n",
      "2280: Loss: 4.34092378616333\n",
      "2281: Loss: 3.784480333328247\n",
      "2282: Loss: 4.343077182769775\n",
      "2283: Loss: 4.002196788787842\n",
      "2284: Loss: 4.263792037963867\n",
      "2285: Loss: 4.44088888168335\n",
      "2286: Loss: 4.026649475097656\n",
      "2287: Loss: 3.968066930770874\n",
      "2288: Loss: 3.7887887954711914\n",
      "2289: Loss: 3.6170711517333984\n",
      "2290: Loss: 4.3055291175842285\n",
      "2291: Loss: 4.354931354522705\n",
      "2292: Loss: 3.5470025539398193\n",
      "2293: Loss: 3.9788081645965576\n",
      "2294: Loss: 4.059606552124023\n",
      "2295: Loss: 4.179569721221924\n",
      "2296: Loss: 4.083169460296631\n",
      "2297: Loss: 4.3629350662231445\n",
      "2298: Loss: 4.0658040046691895\n",
      "2299: Loss: 4.003189563751221\n",
      "2300: Loss: 4.144251346588135\n",
      "2301: Loss: 4.282849311828613\n",
      "2302: Loss: 4.483170986175537\n",
      "2303: Loss: 4.134543418884277\n",
      "2304: Loss: 4.03051233291626\n",
      "2305: Loss: 4.016500949859619\n",
      "2306: Loss: 3.8381943702697754\n",
      "2307: Loss: 4.0524797439575195\n",
      "2308: Loss: 4.211631774902344\n",
      "2309: Loss: 3.9250826835632324\n",
      "2310: Loss: 4.4822564125061035\n",
      "2311: Loss: 4.1092705726623535\n",
      "2312: Loss: 4.081592082977295\n",
      "2313: Loss: 4.012631416320801\n",
      "2314: Loss: 4.150125503540039\n",
      "2315: Loss: 4.001873970031738\n",
      "2316: Loss: 4.296724319458008\n",
      "2317: Loss: 4.30046272277832\n",
      "2318: Loss: 4.180288314819336\n",
      "2319: Loss: 4.279417991638184\n",
      "2320: Loss: 4.104739189147949\n",
      "2321: Loss: 3.8323135375976562\n",
      "2322: Loss: 4.329283237457275\n",
      "2323: Loss: 3.993990659713745\n",
      "2324: Loss: 4.185217380523682\n",
      "2325: Loss: 4.083304405212402\n",
      "2326: Loss: 4.300755500793457\n",
      "2327: Loss: 4.073713302612305\n",
      "2328: Loss: 3.9376864433288574\n",
      "2329: Loss: 4.048763751983643\n",
      "2330: Loss: 3.9030063152313232\n",
      "2331: Loss: 4.181944370269775\n",
      "2332: Loss: 4.205230712890625\n",
      "2333: Loss: 3.93876314163208\n",
      "2334: Loss: 4.505702018737793\n",
      "2335: Loss: 4.402918338775635\n",
      "2336: Loss: 4.017194747924805\n",
      "2337: Loss: 4.070115566253662\n",
      "2338: Loss: 3.727264165878296\n",
      "2339: Loss: 4.086564540863037\n",
      "2340: Loss: 3.929285764694214\n",
      "2341: Loss: 4.324413776397705\n",
      "2342: Loss: 4.301859378814697\n",
      "2343: Loss: 4.203340530395508\n",
      "2344: Loss: 4.335300922393799\n",
      "2345: Loss: 3.9006495475769043\n",
      "2346: Loss: 4.258556365966797\n",
      "2347: Loss: 3.973468780517578\n",
      "2348: Loss: 3.5274062156677246\n",
      "2349: Loss: 4.360637664794922\n",
      "2350: Loss: 3.9853157997131348\n",
      "2351: Loss: 3.783926010131836\n",
      "2352: Loss: 3.9954254627227783\n",
      "2353: Loss: 3.924457550048828\n",
      "2354: Loss: 4.1988372802734375\n",
      "2355: Loss: 3.8933186531066895\n",
      "2356: Loss: 3.9170279502868652\n",
      "2357: Loss: 4.031123638153076\n",
      "2358: Loss: 3.723386764526367\n",
      "2359: Loss: 4.358371257781982\n",
      "2360: Loss: 4.0504374504089355\n",
      "2361: Loss: 3.9674830436706543\n",
      "2362: Loss: 4.009954452514648\n",
      "2363: Loss: 3.786125659942627\n",
      "2364: Loss: 4.266141414642334\n",
      "2365: Loss: 4.2960205078125\n",
      "2366: Loss: 4.0376105308532715\n",
      "2367: Loss: 4.4628400802612305\n",
      "2368: Loss: 4.34806489944458\n",
      "2369: Loss: 4.274275779724121\n",
      "2370: Loss: 4.28745698928833\n",
      "2371: Loss: 3.588054656982422\n",
      "2372: Loss: 4.004303455352783\n",
      "2373: Loss: 4.102092266082764\n",
      "2374: Loss: 4.265005588531494\n",
      "2375: Loss: 3.805086851119995\n",
      "2376: Loss: 4.005093097686768\n",
      "2377: Loss: 3.88747239112854\n",
      "2378: Loss: 4.29149866104126\n",
      "2379: Loss: 4.18910026550293\n",
      "2380: Loss: 4.078084468841553\n",
      "2381: Loss: 4.426312446594238\n",
      "2382: Loss: 4.077535152435303\n",
      "2383: Loss: 4.058803081512451\n",
      "2384: Loss: 4.11355447769165\n",
      "2385: Loss: 4.222512245178223\n",
      "2386: Loss: 3.9561235904693604\n",
      "2387: Loss: 3.79512619972229\n",
      "2388: Loss: 3.8467628955841064\n",
      "2389: Loss: 4.087252616882324\n",
      "2390: Loss: 4.491085529327393\n",
      "2391: Loss: 4.455944061279297\n",
      "2392: Loss: 4.324147701263428\n",
      "2393: Loss: 4.051028728485107\n",
      "2394: Loss: 4.011473178863525\n",
      "2395: Loss: 3.9855732917785645\n",
      "2396: Loss: 4.314761638641357\n",
      "2397: Loss: 4.381008148193359\n",
      "2398: Loss: 4.210944652557373\n",
      "2399: Loss: 4.05722188949585\n",
      "2400: Loss: 4.253739833831787\n",
      "2401: Loss: 4.138329029083252\n",
      "2402: Loss: 4.152293682098389\n",
      "2403: Loss: 3.9409098625183105\n",
      "2404: Loss: 4.510257720947266\n",
      "2405: Loss: 4.0992255210876465\n",
      "2406: Loss: 4.186351776123047\n",
      "2407: Loss: 3.8019208908081055\n",
      "2408: Loss: 4.325689315795898\n",
      "2409: Loss: 4.1633381843566895\n",
      "2410: Loss: 3.8368170261383057\n",
      "2411: Loss: 4.095095634460449\n",
      "2412: Loss: 4.290858745574951\n",
      "2413: Loss: 3.900953769683838\n",
      "2414: Loss: 4.0451226234436035\n",
      "2415: Loss: 4.096569538116455\n",
      "2416: Loss: 4.105259895324707\n",
      "2417: Loss: 4.160951614379883\n",
      "2418: Loss: 4.114941596984863\n",
      "2419: Loss: 4.31412410736084\n",
      "2420: Loss: 3.877502679824829\n",
      "2421: Loss: 3.8967576026916504\n",
      "2422: Loss: 3.7642874717712402\n",
      "2423: Loss: 3.794940710067749\n",
      "2424: Loss: 3.953946113586426\n",
      "2425: Loss: 3.7850372791290283\n",
      "2426: Loss: 4.184232234954834\n",
      "2427: Loss: 3.7085909843444824\n",
      "2428: Loss: 4.450197219848633\n",
      "2429: Loss: 4.318947792053223\n",
      "2430: Loss: 4.254538059234619\n",
      "2431: Loss: 4.0767388343811035\n",
      "2432: Loss: 4.174002647399902\n",
      "2433: Loss: 4.298785209655762\n",
      "2434: Loss: 4.046700954437256\n",
      "2435: Loss: 3.844146251678467\n",
      "2436: Loss: 3.948857069015503\n",
      "2437: Loss: 4.021507263183594\n",
      "2438: Loss: 4.249009132385254\n",
      "2439: Loss: 4.120722770690918\n",
      "2440: Loss: 4.1943678855896\n",
      "2441: Loss: 4.1720290184021\n",
      "2442: Loss: 4.15701961517334\n",
      "2443: Loss: 3.882878303527832\n",
      "2444: Loss: 3.9526588916778564\n",
      "2445: Loss: 4.21329402923584\n",
      "2446: Loss: 4.319237232208252\n",
      "2447: Loss: 3.932567596435547\n",
      "2448: Loss: 4.149825096130371\n",
      "2449: Loss: 4.3261518478393555\n",
      "2450: Loss: 3.800029993057251\n",
      "2451: Loss: 3.9995293617248535\n",
      "2452: Loss: 4.1384758949279785\n",
      "2453: Loss: 4.324100017547607\n",
      "2454: Loss: 4.270421504974365\n",
      "2455: Loss: 4.288588523864746\n",
      "2456: Loss: 3.8998477458953857\n",
      "2457: Loss: 4.341231346130371\n",
      "2458: Loss: 4.220896244049072\n",
      "2459: Loss: 4.328161716461182\n",
      "2460: Loss: 3.884873867034912\n",
      "2461: Loss: 4.294702529907227\n",
      "2462: Loss: 3.9642794132232666\n",
      "2463: Loss: 4.04755163192749\n",
      "2464: Loss: 4.163364887237549\n",
      "2465: Loss: 3.883002519607544\n",
      "2466: Loss: 4.226936340332031\n",
      "2467: Loss: 4.309756278991699\n",
      "2468: Loss: 3.9602386951446533\n",
      "2469: Loss: 3.9410812854766846\n",
      "2470: Loss: 4.183885097503662\n",
      "2471: Loss: 4.108035087585449\n",
      "2472: Loss: 3.7832765579223633\n",
      "2473: Loss: 4.096222877502441\n",
      "2474: Loss: 4.03632926940918\n",
      "2475: Loss: 4.357846736907959\n",
      "2476: Loss: 4.067893981933594\n",
      "2477: Loss: 4.192117214202881\n",
      "2478: Loss: 3.794997453689575\n",
      "2479: Loss: 3.7904396057128906\n",
      "2480: Loss: 4.268094062805176\n",
      "2481: Loss: 3.9095940589904785\n",
      "2482: Loss: 4.121005535125732\n",
      "2483: Loss: 4.028059959411621\n",
      "2484: Loss: 3.6818432807922363\n",
      "2485: Loss: 4.136655330657959\n",
      "2486: Loss: 4.189937114715576\n",
      "2487: Loss: 3.632707118988037\n",
      "2488: Loss: 3.726207733154297\n",
      "2489: Loss: 3.996141195297241\n",
      "2490: Loss: 3.7678866386413574\n",
      "2491: Loss: 3.94513201713562\n",
      "2492: Loss: 4.02440071105957\n",
      "2493: Loss: 4.045772075653076\n",
      "2494: Loss: 4.032689571380615\n",
      "2495: Loss: 4.0262451171875\n",
      "2496: Loss: 3.813457489013672\n",
      "2497: Loss: 4.284626483917236\n",
      "2498: Loss: 4.051265239715576\n",
      "2499: Loss: 4.495650291442871\n",
      "2500: Loss: 4.066532135009766\n",
      "2501: Loss: 4.073578834533691\n",
      "2502: Loss: 4.324058532714844\n",
      "2503: Loss: 4.320605754852295\n",
      "2504: Loss: 3.974888563156128\n",
      "2505: Loss: 3.966235399246216\n",
      "2506: Loss: 4.262981414794922\n",
      "2507: Loss: 4.064745903015137\n",
      "2508: Loss: 3.8162801265716553\n",
      "2509: Loss: 3.9496853351593018\n",
      "2510: Loss: 4.258546352386475\n",
      "2511: Loss: 4.196699619293213\n",
      "2512: Loss: 4.312678813934326\n",
      "2513: Loss: 3.9474189281463623\n",
      "2514: Loss: 3.971619129180908\n",
      "2515: Loss: 4.436453342437744\n",
      "2516: Loss: 4.1456990242004395\n",
      "2517: Loss: 3.8270328044891357\n",
      "2518: Loss: 3.926034688949585\n",
      "2519: Loss: 4.139368534088135\n",
      "2520: Loss: 3.9581453800201416\n",
      "2521: Loss: 4.238675117492676\n",
      "2522: Loss: 3.8825087547302246\n",
      "2523: Loss: 4.1564226150512695\n",
      "2524: Loss: 4.452977657318115\n",
      "2525: Loss: 3.9454915523529053\n",
      "2526: Loss: 3.875354051589966\n",
      "2527: Loss: 4.442050457000732\n",
      "2528: Loss: 4.4431257247924805\n",
      "2529: Loss: 3.9525723457336426\n",
      "2530: Loss: 4.403438091278076\n",
      "2531: Loss: 4.456882953643799\n",
      "2532: Loss: 3.5408172607421875\n",
      "2533: Loss: 4.174230575561523\n",
      "2534: Loss: 3.9265334606170654\n",
      "2535: Loss: 4.069464683532715\n",
      "2536: Loss: 4.348458290100098\n",
      "2537: Loss: 3.946079730987549\n",
      "2538: Loss: 3.9843780994415283\n",
      "2539: Loss: 4.2855610847473145\n",
      "2540: Loss: 4.33213996887207\n",
      "2541: Loss: 3.7804863452911377\n",
      "2542: Loss: 4.164918422698975\n",
      "2543: Loss: 4.418397426605225\n",
      "2544: Loss: 4.468632221221924\n",
      "2545: Loss: 3.942992687225342\n",
      "2546: Loss: 4.089990615844727\n",
      "2547: Loss: 4.017270088195801\n",
      "2548: Loss: 3.9911811351776123\n",
      "2549: Loss: 3.666123867034912\n",
      "2550: Loss: 4.330493927001953\n",
      "2551: Loss: 4.2875471115112305\n",
      "2552: Loss: 4.041264533996582\n",
      "2553: Loss: 4.19563627243042\n",
      "2554: Loss: 4.128597259521484\n",
      "2555: Loss: 3.6350655555725098\n",
      "2556: Loss: 3.804327964782715\n",
      "2557: Loss: 4.2719621658325195\n",
      "2558: Loss: 4.108559608459473\n",
      "2559: Loss: 3.920654296875\n",
      "2560: Loss: 4.251492500305176\n",
      "2561: Loss: 4.2208662033081055\n",
      "2562: Loss: 4.34689998626709\n",
      "2563: Loss: 4.30582857131958\n",
      "2564: Loss: 3.7887508869171143\n",
      "2565: Loss: 4.276650428771973\n",
      "2566: Loss: 4.335582256317139\n",
      "2567: Loss: 3.8313653469085693\n",
      "2568: Loss: 3.4964230060577393\n",
      "2569: Loss: 3.8462510108947754\n",
      "2570: Loss: 4.281414031982422\n",
      "2571: Loss: 4.084835529327393\n",
      "2572: Loss: 3.9083502292633057\n",
      "2573: Loss: 4.085312366485596\n",
      "2574: Loss: 4.0440239906311035\n",
      "2575: Loss: 4.3866777420043945\n",
      "2576: Loss: 4.291429042816162\n",
      "2577: Loss: 3.914623975753784\n",
      "2578: Loss: 4.02584171295166\n",
      "2579: Loss: 3.927182674407959\n",
      "2580: Loss: 4.320280075073242\n",
      "2581: Loss: 4.080649375915527\n",
      "2582: Loss: 3.948335886001587\n",
      "2583: Loss: 4.295089244842529\n",
      "2584: Loss: 3.9196274280548096\n",
      "2585: Loss: 4.510616779327393\n",
      "2586: Loss: 3.982112407684326\n",
      "2587: Loss: 3.816516160964966\n",
      "2588: Loss: 3.7169156074523926\n",
      "2589: Loss: 4.18508243560791\n",
      "2590: Loss: 4.437350749969482\n",
      "2591: Loss: 4.179515361785889\n",
      "2592: Loss: 3.946747064590454\n",
      "2593: Loss: 4.344734191894531\n",
      "2594: Loss: 3.9650449752807617\n",
      "2595: Loss: 3.674684524536133\n",
      "2596: Loss: 4.137439727783203\n",
      "2597: Loss: 4.06891393661499\n",
      "2598: Loss: 4.150694847106934\n",
      "2599: Loss: 3.7931854724884033\n",
      "2600: Loss: 4.021958827972412\n",
      "2601: Loss: 4.356265068054199\n",
      "2602: Loss: 3.900766372680664\n",
      "2603: Loss: 4.194302082061768\n",
      "2604: Loss: 4.048712253570557\n",
      "2605: Loss: 4.124416828155518\n",
      "2606: Loss: 4.1608757972717285\n",
      "2607: Loss: 4.348915100097656\n",
      "2608: Loss: 3.934849500656128\n",
      "2609: Loss: 3.8776092529296875\n",
      "2610: Loss: 4.061952590942383\n",
      "2611: Loss: 4.05985689163208\n",
      "2612: Loss: 4.306265354156494\n",
      "2613: Loss: 4.309534549713135\n",
      "2614: Loss: 3.6514639854431152\n",
      "2615: Loss: 4.4376606941223145\n",
      "2616: Loss: 3.7432758808135986\n",
      "2617: Loss: 4.39939546585083\n",
      "2618: Loss: 3.9815800189971924\n",
      "2619: Loss: 3.9458072185516357\n",
      "2620: Loss: 4.411978721618652\n",
      "2621: Loss: 3.973634958267212\n",
      "2622: Loss: 3.935370683670044\n",
      "2623: Loss: 4.112537860870361\n",
      "2624: Loss: 4.193042755126953\n",
      "2625: Loss: 4.196298122406006\n",
      "2626: Loss: 4.084221839904785\n",
      "2627: Loss: 4.055871963500977\n",
      "2628: Loss: 4.168087005615234\n",
      "2629: Loss: 4.0907793045043945\n",
      "2630: Loss: 3.8666269779205322\n",
      "2631: Loss: 4.022319793701172\n",
      "2632: Loss: 4.31083345413208\n",
      "2633: Loss: 3.935525417327881\n",
      "2634: Loss: 4.068702220916748\n",
      "2635: Loss: 4.26290225982666\n",
      "2636: Loss: 4.268965721130371\n",
      "2637: Loss: 4.41993522644043\n",
      "2638: Loss: 3.992311716079712\n",
      "2639: Loss: 4.116755485534668\n",
      "2640: Loss: 4.036072254180908\n",
      "2641: Loss: 4.517007827758789\n",
      "2642: Loss: 4.031017303466797\n",
      "2643: Loss: 4.379879474639893\n",
      "2644: Loss: 4.096177577972412\n",
      "2645: Loss: 3.965393543243408\n",
      "2646: Loss: 4.159021377563477\n",
      "2647: Loss: 3.7508132457733154\n",
      "2648: Loss: 3.769270658493042\n",
      "2649: Loss: 3.9743096828460693\n",
      "2650: Loss: 3.963895559310913\n",
      "2651: Loss: 4.076876163482666\n",
      "2652: Loss: 3.9618241786956787\n",
      "2653: Loss: 3.843740701675415\n",
      "2654: Loss: 3.9417436122894287\n",
      "2655: Loss: 4.200317859649658\n",
      "2656: Loss: 4.030147552490234\n",
      "2657: Loss: 3.8303751945495605\n",
      "2658: Loss: 4.425425052642822\n",
      "2659: Loss: 4.174056053161621\n",
      "2660: Loss: 4.022371292114258\n",
      "2661: Loss: 3.863551616668701\n",
      "2662: Loss: 4.264858722686768\n",
      "2663: Loss: 3.7886576652526855\n",
      "2664: Loss: 4.236822605133057\n",
      "2665: Loss: 4.321450710296631\n",
      "2666: Loss: 3.9140405654907227\n",
      "2667: Loss: 4.504921913146973\n",
      "2668: Loss: 4.206201553344727\n",
      "2669: Loss: 3.797140598297119\n",
      "2670: Loss: 3.8708364963531494\n",
      "2671: Loss: 4.153685092926025\n",
      "2672: Loss: 4.235158920288086\n",
      "2673: Loss: 4.159190654754639\n",
      "2674: Loss: 4.3555707931518555\n",
      "2675: Loss: 4.031749248504639\n",
      "2676: Loss: 4.0452375411987305\n",
      "2677: Loss: 4.016582012176514\n",
      "2678: Loss: 4.121365070343018\n",
      "2679: Loss: 3.8669509887695312\n",
      "2680: Loss: 3.6915338039398193\n",
      "2681: Loss: 4.052116394042969\n",
      "2682: Loss: 4.0741705894470215\n",
      "2683: Loss: 3.9522955417633057\n",
      "2684: Loss: 3.7554662227630615\n",
      "2685: Loss: 3.9720406532287598\n",
      "2686: Loss: 4.169778347015381\n",
      "2687: Loss: 4.037741184234619\n",
      "2688: Loss: 4.360867977142334\n",
      "2689: Loss: 4.3902130126953125\n",
      "2690: Loss: 4.405789375305176\n",
      "2691: Loss: 4.098483562469482\n",
      "2692: Loss: 3.7993292808532715\n",
      "2693: Loss: 4.303319931030273\n",
      "2694: Loss: 3.8651411533355713\n",
      "2695: Loss: 4.362677097320557\n",
      "2696: Loss: 3.9889450073242188\n",
      "2697: Loss: 4.454047203063965\n",
      "2698: Loss: 4.039160251617432\n",
      "2699: Loss: 4.055177211761475\n",
      "2700: Loss: 4.010413646697998\n",
      "2701: Loss: 3.512279748916626\n",
      "2702: Loss: 3.7064390182495117\n",
      "2703: Loss: 4.265746593475342\n",
      "2704: Loss: 3.7509937286376953\n",
      "2705: Loss: 4.0035271644592285\n",
      "2706: Loss: 3.895242214202881\n",
      "2707: Loss: 4.229692459106445\n",
      "2708: Loss: 4.333646774291992\n",
      "2709: Loss: 4.347461700439453\n",
      "2710: Loss: 4.283603191375732\n",
      "2711: Loss: 3.9481406211853027\n",
      "2712: Loss: 4.3094987869262695\n",
      "2713: Loss: 4.2265729904174805\n",
      "2714: Loss: 4.184451103210449\n",
      "2715: Loss: 4.2179131507873535\n",
      "2716: Loss: 3.8009958267211914\n",
      "2717: Loss: 3.931898355484009\n",
      "2718: Loss: 3.914367198944092\n",
      "2719: Loss: 4.0487236976623535\n",
      "2720: Loss: 4.227233409881592\n",
      "2721: Loss: 3.7593703269958496\n",
      "2722: Loss: 4.0957794189453125\n",
      "2723: Loss: 3.9483447074890137\n",
      "2724: Loss: 3.6106462478637695\n",
      "2725: Loss: 4.245670318603516\n",
      "2726: Loss: 3.9432318210601807\n",
      "2727: Loss: 3.9420998096466064\n",
      "2728: Loss: 3.909648895263672\n",
      "2729: Loss: 3.9630942344665527\n",
      "2730: Loss: 3.7824833393096924\n",
      "2731: Loss: 4.132948398590088\n",
      "2732: Loss: 4.148754119873047\n",
      "2733: Loss: 4.024160861968994\n",
      "2734: Loss: 4.322464466094971\n",
      "2735: Loss: 4.3095502853393555\n",
      "2736: Loss: 4.166599750518799\n",
      "2737: Loss: 4.094618797302246\n",
      "2738: Loss: 4.407848358154297\n",
      "2739: Loss: 3.970900535583496\n",
      "2740: Loss: 4.262073040008545\n",
      "2741: Loss: 4.094967842102051\n",
      "2742: Loss: 3.885819435119629\n",
      "2743: Loss: 3.7861649990081787\n",
      "2744: Loss: 3.8918380737304688\n",
      "2745: Loss: 4.326405048370361\n",
      "2746: Loss: 4.043062686920166\n",
      "2747: Loss: 4.1616692543029785\n",
      "2748: Loss: 4.300838470458984\n",
      "2749: Loss: 4.351314067840576\n",
      "2750: Loss: 4.2870001792907715\n",
      "2751: Loss: 4.218541622161865\n",
      "2752: Loss: 3.9357872009277344\n",
      "2753: Loss: 4.434834957122803\n",
      "2754: Loss: 4.4210615158081055\n",
      "2755: Loss: 3.9755301475524902\n",
      "2756: Loss: 3.9547412395477295\n",
      "2757: Loss: 4.499927997589111\n",
      "2758: Loss: 3.9951741695404053\n",
      "2759: Loss: 4.03469181060791\n",
      "2760: Loss: 4.441742420196533\n",
      "2761: Loss: 4.237998962402344\n",
      "2762: Loss: 4.173826217651367\n",
      "2763: Loss: 4.007294178009033\n",
      "2764: Loss: 4.057328701019287\n",
      "2765: Loss: 4.142268657684326\n",
      "2766: Loss: 4.31351375579834\n",
      "2767: Loss: 4.054119110107422\n",
      "2768: Loss: 4.332631587982178\n",
      "2769: Loss: 4.235948085784912\n",
      "2770: Loss: 3.95782470703125\n",
      "2771: Loss: 4.123558044433594\n",
      "2772: Loss: 4.255115032196045\n",
      "2773: Loss: 4.113163471221924\n",
      "2774: Loss: 3.900441884994507\n",
      "2775: Loss: 3.7942066192626953\n",
      "2776: Loss: 4.052639484405518\n",
      "2777: Loss: 3.804306745529175\n",
      "2778: Loss: 4.055841445922852\n",
      "2779: Loss: 4.4678754806518555\n",
      "2780: Loss: 3.968533754348755\n",
      "2781: Loss: 4.0536723136901855\n",
      "2782: Loss: 3.9640848636627197\n",
      "2783: Loss: 4.198405742645264\n",
      "2784: Loss: 3.7810580730438232\n",
      "2785: Loss: 4.118260860443115\n",
      "2786: Loss: 3.872622489929199\n",
      "2787: Loss: 4.2592339515686035\n",
      "2788: Loss: 4.4805521965026855\n",
      "2789: Loss: 3.8710126876831055\n",
      "2790: Loss: 4.0117950439453125\n",
      "2791: Loss: 4.319723606109619\n",
      "2792: Loss: 4.0823469161987305\n",
      "2793: Loss: 4.425261974334717\n",
      "2794: Loss: 4.023988723754883\n",
      "2795: Loss: 4.488215923309326\n",
      "2796: Loss: 3.806316614151001\n",
      "2797: Loss: 4.3866705894470215\n",
      "2798: Loss: 3.885930299758911\n",
      "2799: Loss: 4.129870891571045\n",
      "2800: Loss: 3.6899023056030273\n",
      "2801: Loss: 4.206483364105225\n",
      "2802: Loss: 3.7874505519866943\n",
      "2803: Loss: 4.33389949798584\n",
      "2804: Loss: 3.9678072929382324\n",
      "2805: Loss: 4.03315544128418\n",
      "2806: Loss: 3.4891366958618164\n",
      "2807: Loss: 4.32907247543335\n",
      "2808: Loss: 3.6942214965820312\n",
      "2809: Loss: 4.399917125701904\n",
      "2810: Loss: 4.313773155212402\n",
      "2811: Loss: 4.1597161293029785\n",
      "2812: Loss: 4.165252685546875\n",
      "2813: Loss: 4.238753795623779\n",
      "2814: Loss: 3.9662983417510986\n",
      "2815: Loss: 4.110318660736084\n",
      "2816: Loss: 4.271470069885254\n",
      "2817: Loss: 3.7727365493774414\n",
      "2818: Loss: 4.2925848960876465\n",
      "2819: Loss: 3.9383487701416016\n",
      "2820: Loss: 4.041215419769287\n",
      "2821: Loss: 4.228514194488525\n",
      "2822: Loss: 4.081226825714111\n",
      "2823: Loss: 4.1472697257995605\n",
      "2824: Loss: 3.5943524837493896\n",
      "2825: Loss: 4.088422775268555\n",
      "2826: Loss: 4.322963237762451\n",
      "2827: Loss: 4.2461161613464355\n",
      "2828: Loss: 4.4235920906066895\n",
      "2829: Loss: 4.159049987792969\n",
      "2830: Loss: 4.044074058532715\n",
      "2831: Loss: 4.008694648742676\n",
      "2832: Loss: 4.314769744873047\n",
      "2833: Loss: 3.938643455505371\n",
      "2834: Loss: 3.8654119968414307\n",
      "2835: Loss: 3.7714452743530273\n",
      "2836: Loss: 4.345732688903809\n",
      "2837: Loss: 4.169384479522705\n",
      "2838: Loss: 3.9405922889709473\n",
      "2839: Loss: 4.058414459228516\n",
      "2840: Loss: 3.8054280281066895\n",
      "2841: Loss: 3.907996416091919\n",
      "2842: Loss: 4.043167591094971\n",
      "2843: Loss: 3.7719106674194336\n",
      "2844: Loss: 4.1166157722473145\n",
      "2845: Loss: 4.05431604385376\n",
      "2846: Loss: 4.345558166503906\n",
      "2847: Loss: 3.9823312759399414\n",
      "2848: Loss: 4.000216960906982\n",
      "2849: Loss: 3.9339473247528076\n",
      "2850: Loss: 3.8176767826080322\n",
      "2851: Loss: 3.785851240158081\n",
      "2852: Loss: 3.9243295192718506\n",
      "2853: Loss: 4.3213324546813965\n",
      "2854: Loss: 4.058414459228516\n",
      "2855: Loss: 3.7124452590942383\n",
      "2856: Loss: 3.985429286956787\n",
      "2857: Loss: 3.9859375953674316\n",
      "2858: Loss: 4.101715564727783\n",
      "2859: Loss: 3.9223616123199463\n",
      "2860: Loss: 4.3362884521484375\n",
      "2861: Loss: 4.278949737548828\n",
      "2862: Loss: 4.172979354858398\n",
      "2863: Loss: 3.958235502243042\n",
      "2864: Loss: 4.160064697265625\n",
      "2865: Loss: 4.039752960205078\n",
      "2866: Loss: 4.293767929077148\n",
      "2867: Loss: 4.377476215362549\n",
      "2868: Loss: 3.88287353515625\n",
      "2869: Loss: 3.9498119354248047\n",
      "2870: Loss: 4.064637184143066\n",
      "2871: Loss: 4.188957214355469\n",
      "2872: Loss: 3.896015167236328\n",
      "2873: Loss: 4.347140789031982\n",
      "2874: Loss: 4.2260518074035645\n",
      "2875: Loss: 4.265125751495361\n",
      "2876: Loss: 4.1996636390686035\n",
      "2877: Loss: 4.457657337188721\n",
      "2878: Loss: 4.168368816375732\n",
      "2879: Loss: 3.946349620819092\n",
      "2880: Loss: 4.0052809715271\n",
      "2881: Loss: 4.15723991394043\n",
      "2882: Loss: 4.036787509918213\n",
      "2883: Loss: 4.094120025634766\n",
      "2884: Loss: 4.268949508666992\n",
      "2885: Loss: 4.029987335205078\n",
      "2886: Loss: 3.913367509841919\n",
      "2887: Loss: 3.884397506713867\n",
      "2888: Loss: 3.9163498878479004\n",
      "2889: Loss: 3.9783096313476562\n",
      "2890: Loss: 4.085838317871094\n",
      "2891: Loss: 3.7416391372680664\n",
      "2892: Loss: 3.950378894805908\n",
      "2893: Loss: 4.280056476593018\n",
      "2894: Loss: 3.854979991912842\n",
      "2895: Loss: 4.2401275634765625\n",
      "2896: Loss: 4.133851528167725\n",
      "2897: Loss: 4.327009201049805\n",
      "2898: Loss: 4.428060531616211\n",
      "2899: Loss: 4.304665565490723\n",
      "2900: Loss: 4.016112804412842\n",
      "2901: Loss: 4.337560653686523\n",
      "2902: Loss: 3.8752121925354004\n",
      "2903: Loss: 4.253287315368652\n",
      "2904: Loss: 4.07293176651001\n",
      "2905: Loss: 4.276174068450928\n",
      "2906: Loss: 4.247784614562988\n",
      "2907: Loss: 3.823707103729248\n",
      "2908: Loss: 4.309209823608398\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-56c11d37e515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"step\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RMS\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_data_fp32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m                 \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[0mbeta2t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"step\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"decay_rate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36m_get_lr\u001b[1;34m(param_group, param_state)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mparam_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scale_parameter\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[0mparam_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RMS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparam_scale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrel_step_sz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_train_epochs = 100\n",
    "\n",
    "total_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k:v.type(torch.long).to(\"cuda\") for k,v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_step += 1\n",
    "        print(f\"{total_step}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp = SoftPrompt.from_tuning_model(model)\n",
    "#sp.to_file(\"neuromancer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SoftPrompt.from_file(\"neuromancer.json\")\n",
    "model.set_soft_prompt(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Armitage, the man who had been his best friend since he was a boy. He'd always wanted to be with him.\"\"I'm sorry,\" she said softly. \"You're not going anywhere now?\"He looked at her in surprise. She smiled and nodded slowly toward Case's shoulder. The girl sat down next door on an empty floor of one-story apartment building that housed two large offices for Maelcum; it wasn't long before they were all gone except their windows shattered by firecr\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "call = tokenizer(\"Armitage\", return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 100,\n",
    "    max_length=call.shape[-1] + 100,\n",
    "    temperature=0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}